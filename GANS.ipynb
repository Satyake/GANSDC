{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "#print torch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyperparameters\n",
    "batchSize = 10\n",
    "imageSize = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load custom dataset\n",
    "#transform=transforms.Compose([transforms.Scale(imageSize),transforms.ToTensor()])\n",
    "dataset = dset.ImageFolder(root = 'C:/Users/satya/Downloads/faces/', transform=transform)\n",
    "len(dataset)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load custom dataset\n",
    "#transform=transforms.Compose([transforms.Scale(imageSize),transforms.ToTensor()])\n",
    "dataset = dset.ImageFolder(root = 'C:/Users/satya/Downloads/animegirlsext', transform=transform)\n",
    "len(dataset)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, num_workers=2)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the transformations\n",
    "transform=transforms.Compose([transforms.Scale(imageSize),transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.datasets' has no attribute 'FashionMNIST'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-2a60ae2fbb06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#dataset=dset.CIFAR10(root='./data',download=True,transform=transform)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#dataset=dset.MNIST(root='./data',download=True,transform=transform)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFashionMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision.datasets' has no attribute 'FashionMNIST'"
     ]
    }
   ],
   "source": [
    "#loading the dataset:\n",
    "#dataset=dset.CIFAR10(root='./data',download=True,transform=transform)\n",
    "#dataset=dset.MNIST(root='./data',download=True,transform=transform)\n",
    "dataset=dset.FashionMNIST(root='./data',download=True,transform=transform)\n",
    "dataloader=torch.utils.data.DataLoader(dataset,batch_size=batchSize,shuffle=True, num_workers=2)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining weights init function that takes an input a neural network m and will initialize all its weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu=1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/110] Loss_D: 1.1048 Loss_G: 22.4026\n",
      "[0/25][1/110] Loss_D: 0.2796 Loss_G: 7.6188\n",
      "[0/25][2/110] Loss_D: 2.4683 Loss_G: 21.7364\n",
      "[0/25][3/110] Loss_D: 0.0152 Loss_G: 14.4943\n",
      "[0/25][4/110] Loss_D: 0.1556 Loss_G: 8.2295\n",
      "[0/25][5/110] Loss_D: 2.4590 Loss_G: 25.6691\n",
      "[0/25][6/110] Loss_D: 0.0492 Loss_G: 23.8426\n",
      "[0/25][7/110] Loss_D: 0.0027 Loss_G: 17.9340\n",
      "[0/25][8/110] Loss_D: 0.3359 Loss_G: 16.0547\n",
      "[0/25][9/110] Loss_D: 0.3944 Loss_G: 24.0682\n",
      "[0/25][10/110] Loss_D: 0.4027 Loss_G: 20.7553\n",
      "[0/25][11/110] Loss_D: 0.0058 Loss_G: 10.4692\n",
      "[0/25][12/110] Loss_D: 3.0828 Loss_G: 37.0623\n",
      "[0/25][13/110] Loss_D: 0.6796 Loss_G: 39.9172\n",
      "[0/25][14/110] Loss_D: 0.6795 Loss_G: 40.4774\n",
      "[0/25][15/110] Loss_D: 0.0007 Loss_G: 40.5093\n",
      "[0/25][16/110] Loss_D: 0.0000 Loss_G: 40.9439\n",
      "[0/25][17/110] Loss_D: 0.5957 Loss_G: 39.6050\n",
      "[0/25][18/110] Loss_D: 0.0000 Loss_G: 39.5200\n",
      "[0/25][19/110] Loss_D: 0.0000 Loss_G: 40.1034\n",
      "[0/25][20/110] Loss_D: 0.0001 Loss_G: 38.9078\n",
      "[0/25][21/110] Loss_D: 0.0000 Loss_G: 39.0722\n",
      "[0/25][22/110] Loss_D: 0.0001 Loss_G: 38.8627\n",
      "[0/25][23/110] Loss_D: 0.0001 Loss_G: 39.6891\n",
      "[0/25][24/110] Loss_D: 0.0000 Loss_G: 39.4571\n",
      "[0/25][25/110] Loss_D: 0.0000 Loss_G: 39.2359\n",
      "[0/25][26/110] Loss_D: 0.0000 Loss_G: 39.3829\n",
      "[0/25][27/110] Loss_D: 0.0000 Loss_G: 39.3105\n",
      "[0/25][28/110] Loss_D: 0.0000 Loss_G: 38.9908\n",
      "[0/25][29/110] Loss_D: 0.0002 Loss_G: 39.9622\n",
      "[0/25][30/110] Loss_D: 0.0000 Loss_G: 39.9318\n",
      "[0/25][31/110] Loss_D: 0.0000 Loss_G: 38.6424\n",
      "[0/25][32/110] Loss_D: 0.0000 Loss_G: 38.9387\n",
      "[0/25][33/110] Loss_D: 0.0000 Loss_G: 38.8131\n",
      "[0/25][34/110] Loss_D: 0.0000 Loss_G: 38.8114\n",
      "[0/25][35/110] Loss_D: 0.0000 Loss_G: 37.7602\n",
      "[0/25][36/110] Loss_D: 0.0000 Loss_G: 38.1679\n",
      "[0/25][37/110] Loss_D: 0.0000 Loss_G: 37.7352\n",
      "[0/25][38/110] Loss_D: 0.0000 Loss_G: 38.9789\n",
      "[0/25][39/110] Loss_D: 0.0003 Loss_G: 38.3466\n",
      "[0/25][40/110] Loss_D: 0.0000 Loss_G: 38.3111\n",
      "[0/25][41/110] Loss_D: 0.0000 Loss_G: 38.2815\n",
      "[0/25][42/110] Loss_D: 0.0000 Loss_G: 38.7889\n",
      "[0/25][43/110] Loss_D: 0.0000 Loss_G: 38.1551\n",
      "[0/25][44/110] Loss_D: 0.0000 Loss_G: 38.3684\n",
      "[0/25][45/110] Loss_D: 0.0000 Loss_G: 39.0414\n",
      "[0/25][46/110] Loss_D: 0.0000 Loss_G: 37.6610\n",
      "[0/25][47/110] Loss_D: 0.0000 Loss_G: 39.8027\n",
      "[0/25][48/110] Loss_D: 0.0000 Loss_G: 38.9857\n",
      "[0/25][49/110] Loss_D: 0.0000 Loss_G: 38.2187\n",
      "[0/25][50/110] Loss_D: 0.0004 Loss_G: 37.5146\n",
      "[0/25][51/110] Loss_D: 0.0000 Loss_G: 37.2188\n",
      "[0/25][52/110] Loss_D: 0.0003 Loss_G: 38.2944\n",
      "[0/25][53/110] Loss_D: 0.0001 Loss_G: 37.7506\n",
      "[0/25][54/110] Loss_D: 0.0000 Loss_G: 39.1602\n",
      "[0/25][55/110] Loss_D: 0.0008 Loss_G: 38.4832\n",
      "[0/25][56/110] Loss_D: 0.0000 Loss_G: 38.2855\n",
      "[0/25][57/110] Loss_D: 0.0001 Loss_G: 37.8937\n",
      "[0/25][58/110] Loss_D: 0.0000 Loss_G: 37.5055\n",
      "[0/25][59/110] Loss_D: 0.0000 Loss_G: 37.2584\n",
      "[0/25][60/110] Loss_D: 0.0000 Loss_G: 38.9236\n",
      "[0/25][61/110] Loss_D: 0.0001 Loss_G: 37.0302\n",
      "[0/25][62/110] Loss_D: 0.0000 Loss_G: 37.1974\n",
      "[0/25][63/110] Loss_D: 0.0005 Loss_G: 37.7605\n",
      "[0/25][64/110] Loss_D: 0.0000 Loss_G: 37.8718\n",
      "[0/25][65/110] Loss_D: 0.0000 Loss_G: 37.9490\n",
      "[0/25][66/110] Loss_D: 0.0000 Loss_G: 36.9520\n",
      "[0/25][67/110] Loss_D: 0.0001 Loss_G: 36.5341\n",
      "[0/25][68/110] Loss_D: 0.0001 Loss_G: 36.5724\n",
      "[0/25][69/110] Loss_D: 0.0000 Loss_G: 36.1421\n",
      "[0/25][70/110] Loss_D: 0.0001 Loss_G: 36.4069\n",
      "[0/25][71/110] Loss_D: 0.0000 Loss_G: 35.5547\n",
      "[0/25][72/110] Loss_D: 0.0006 Loss_G: 35.4578\n",
      "[0/25][73/110] Loss_D: 0.0000 Loss_G: 34.2845\n",
      "[0/25][74/110] Loss_D: 0.0000 Loss_G: 32.9246\n",
      "[0/25][75/110] Loss_D: 0.0000 Loss_G: 28.2256\n",
      "[0/25][76/110] Loss_D: 0.0000 Loss_G: 18.8235\n",
      "[0/25][77/110] Loss_D: 5.2845 Loss_G: 41.7108\n",
      "[0/25][78/110] Loss_D: 5.4627 Loss_G: 36.9552\n",
      "[0/25][79/110] Loss_D: 0.0000 Loss_G: 36.0646\n",
      "[0/25][80/110] Loss_D: 0.0000 Loss_G: 34.4220\n",
      "[0/25][81/110] Loss_D: 0.0007 Loss_G: 34.1651\n",
      "[0/25][82/110] Loss_D: 0.1421 Loss_G: 31.7457\n",
      "[0/25][83/110] Loss_D: 0.0000 Loss_G: 23.0227\n",
      "[0/25][84/110] Loss_D: 12.6573 Loss_G: 22.8396\n",
      "[0/25][85/110] Loss_D: 7.8841 Loss_G: 41.7403\n",
      "[0/25][86/110] Loss_D: 16.9711 Loss_G: 30.6698\n",
      "[0/25][87/110] Loss_D: 0.6050 Loss_G: 21.1283\n",
      "[0/25][88/110] Loss_D: 9.2277 Loss_G: 12.1367\n",
      "[0/25][89/110] Loss_D: 4.2562 Loss_G: 15.9273\n",
      "[0/25][90/110] Loss_D: 3.6010 Loss_G: 19.8464\n",
      "[0/25][91/110] Loss_D: 1.7716 Loss_G: 12.2083\n",
      "[0/25][92/110] Loss_D: 10.0034 Loss_G: 21.1808\n",
      "[0/25][93/110] Loss_D: 0.9457 Loss_G: 23.4720\n",
      "[0/25][94/110] Loss_D: 0.6165 Loss_G: 13.5288\n",
      "[0/25][95/110] Loss_D: 2.6725 Loss_G: 12.2739\n",
      "[0/25][96/110] Loss_D: 0.1884 Loss_G: 14.5856\n",
      "[0/25][97/110] Loss_D: 0.4522 Loss_G: 10.9306\n",
      "[0/25][98/110] Loss_D: 3.7490 Loss_G: 16.7370\n",
      "[0/25][99/110] Loss_D: 0.2466 Loss_G: 14.8262\n",
      "[0/25][100/110] Loss_D: 3.8044 Loss_G: 12.7754\n",
      "[0/25][101/110] Loss_D: 2.8250 Loss_G: 19.0401\n",
      "[0/25][102/110] Loss_D: 3.5464 Loss_G: 15.0243\n",
      "[0/25][103/110] Loss_D: 0.4203 Loss_G: 11.8283\n",
      "[0/25][104/110] Loss_D: 1.9089 Loss_G: 9.9542\n",
      "[0/25][105/110] Loss_D: 1.0245 Loss_G: 16.4296\n",
      "[0/25][106/110] Loss_D: 0.1364 Loss_G: 13.4564\n",
      "[0/25][107/110] Loss_D: 1.1256 Loss_G: 12.8551\n",
      "[0/25][108/110] Loss_D: 1.6844 Loss_G: 10.7352\n",
      "[0/25][109/110] Loss_D: 2.4646 Loss_G: 10.8403\n",
      "[1/25][0/110] Loss_D: 2.5648 Loss_G: 8.2667\n",
      "[1/25][1/110] Loss_D: 0.7705 Loss_G: 8.2701\n",
      "[1/25][2/110] Loss_D: 1.0255 Loss_G: 11.8647\n",
      "[1/25][3/110] Loss_D: 0.8308 Loss_G: 12.9172\n",
      "[1/25][4/110] Loss_D: 1.2174 Loss_G: 7.2749\n",
      "[1/25][5/110] Loss_D: 5.3573 Loss_G: 15.9566\n",
      "[1/25][6/110] Loss_D: 4.0829 Loss_G: 12.1101\n",
      "[1/25][7/110] Loss_D: 1.1283 Loss_G: 4.3109\n",
      "[1/25][8/110] Loss_D: 1.9402 Loss_G: 4.8169\n",
      "[1/25][9/110] Loss_D: 1.6413 Loss_G: 5.8838\n",
      "[1/25][10/110] Loss_D: 0.6146 Loss_G: 7.5057\n",
      "[1/25][11/110] Loss_D: 0.9986 Loss_G: 8.3326\n",
      "[1/25][12/110] Loss_D: 1.9942 Loss_G: 4.9866\n",
      "[1/25][13/110] Loss_D: 6.0019 Loss_G: 24.2867\n",
      "[1/25][14/110] Loss_D: 17.7862 Loss_G: 13.6972\n",
      "[1/25][15/110] Loss_D: 9.1229 Loss_G: 3.1926\n",
      "[1/25][16/110] Loss_D: 2.0838 Loss_G: 2.6664\n",
      "[1/25][17/110] Loss_D: 1.9121 Loss_G: 4.6597\n",
      "[1/25][18/110] Loss_D: 1.1500 Loss_G: 3.4238\n",
      "[1/25][19/110] Loss_D: 1.1694 Loss_G: 1.7596\n",
      "[1/25][20/110] Loss_D: 0.8399 Loss_G: 3.9412\n",
      "[1/25][21/110] Loss_D: 0.8018 Loss_G: 3.2371\n",
      "[1/25][22/110] Loss_D: 1.2775 Loss_G: 1.6121\n",
      "[1/25][23/110] Loss_D: 1.1898 Loss_G: 6.7991\n",
      "[1/25][24/110] Loss_D: 1.4205 Loss_G: 3.8274\n",
      "[1/25][25/110] Loss_D: 0.4195 Loss_G: 2.7237\n",
      "[1/25][26/110] Loss_D: 1.3657 Loss_G: 6.2250\n",
      "[1/25][27/110] Loss_D: 1.0080 Loss_G: 2.7150\n",
      "[1/25][28/110] Loss_D: 0.9120 Loss_G: 3.5397\n",
      "[1/25][29/110] Loss_D: 0.8977 Loss_G: 3.7318\n",
      "[1/25][30/110] Loss_D: 0.7782 Loss_G: 4.0541\n",
      "[1/25][31/110] Loss_D: 1.4051 Loss_G: 4.1982\n",
      "[1/25][32/110] Loss_D: 0.6903 Loss_G: 5.8755\n",
      "[1/25][33/110] Loss_D: 0.7508 Loss_G: 4.5896\n",
      "[1/25][34/110] Loss_D: 0.6078 Loss_G: 2.8983\n",
      "[1/25][35/110] Loss_D: 1.7945 Loss_G: 11.3713\n",
      "[1/25][36/110] Loss_D: 3.3720 Loss_G: 5.3186\n",
      "[1/25][37/110] Loss_D: 0.5192 Loss_G: 2.3433\n",
      "[1/25][38/110] Loss_D: 1.4377 Loss_G: 4.6232\n",
      "[1/25][39/110] Loss_D: 1.7098 Loss_G: 3.1387\n",
      "[1/25][40/110] Loss_D: 1.3340 Loss_G: 1.6551\n",
      "[1/25][41/110] Loss_D: 1.9161 Loss_G: 5.7483\n",
      "[1/25][42/110] Loss_D: 1.2470 Loss_G: 3.7397\n",
      "[1/25][43/110] Loss_D: 0.4399 Loss_G: 4.1421\n",
      "[1/25][44/110] Loss_D: 0.9875 Loss_G: 5.3855\n",
      "[1/25][45/110] Loss_D: 0.5135 Loss_G: 3.8872\n",
      "[1/25][46/110] Loss_D: 0.6485 Loss_G: 3.3129\n",
      "[1/25][47/110] Loss_D: 0.6102 Loss_G: 4.1581\n",
      "[1/25][48/110] Loss_D: 0.8602 Loss_G: 3.3225\n",
      "[1/25][49/110] Loss_D: 0.6048 Loss_G: 4.3249\n",
      "[1/25][50/110] Loss_D: 1.9167 Loss_G: 3.8694\n",
      "[1/25][51/110] Loss_D: 1.0729 Loss_G: 5.7470\n",
      "[1/25][52/110] Loss_D: 0.9684 Loss_G: 4.4511\n",
      "[1/25][53/110] Loss_D: 1.5409 Loss_G: 5.4080\n",
      "[1/25][54/110] Loss_D: 0.5277 Loss_G: 3.8701\n",
      "[1/25][55/110] Loss_D: 0.5884 Loss_G: 2.5442\n",
      "[1/25][56/110] Loss_D: 0.7131 Loss_G: 2.6327\n",
      "[1/25][57/110] Loss_D: 0.8706 Loss_G: 3.3452\n",
      "[1/25][58/110] Loss_D: 0.2598 Loss_G: 4.6835\n",
      "[1/25][59/110] Loss_D: 1.2888 Loss_G: 1.9041\n",
      "[1/25][60/110] Loss_D: 0.3952 Loss_G: 3.1922\n",
      "[1/25][61/110] Loss_D: 0.6073 Loss_G: 4.2503\n",
      "[1/25][62/110] Loss_D: 0.2647 Loss_G: 3.7701\n",
      "[1/25][63/110] Loss_D: 0.6304 Loss_G: 2.9959\n",
      "[1/25][64/110] Loss_D: 0.4111 Loss_G: 4.9766\n",
      "[1/25][65/110] Loss_D: 0.4768 Loss_G: 4.9342\n",
      "[1/25][66/110] Loss_D: 0.8802 Loss_G: 2.7523\n",
      "[1/25][67/110] Loss_D: 1.4152 Loss_G: 5.8159\n",
      "[1/25][68/110] Loss_D: 1.6198 Loss_G: 4.1980\n",
      "[1/25][69/110] Loss_D: 1.6711 Loss_G: 5.5817\n",
      "[1/25][70/110] Loss_D: 0.7121 Loss_G: 4.2096\n",
      "[1/25][71/110] Loss_D: 0.6000 Loss_G: 4.4913\n",
      "[1/25][72/110] Loss_D: 1.2358 Loss_G: 3.9604\n",
      "[1/25][73/110] Loss_D: 1.0238 Loss_G: 5.2998\n",
      "[1/25][74/110] Loss_D: 1.3183 Loss_G: 2.7872\n",
      "[1/25][75/110] Loss_D: 0.9763 Loss_G: 5.6749\n",
      "[1/25][76/110] Loss_D: 0.5409 Loss_G: 4.0983\n",
      "[1/25][77/110] Loss_D: 0.9817 Loss_G: 4.1309\n",
      "[1/25][78/110] Loss_D: 1.2433 Loss_G: 4.6294\n",
      "[1/25][79/110] Loss_D: 1.4586 Loss_G: 10.3960\n",
      "[1/25][80/110] Loss_D: 3.2857 Loss_G: 3.1341\n",
      "[1/25][81/110] Loss_D: 1.3457 Loss_G: 4.3756\n",
      "[1/25][82/110] Loss_D: 1.1168 Loss_G: 4.3838\n",
      "[1/25][83/110] Loss_D: 0.4123 Loss_G: 3.8958\n",
      "[1/25][84/110] Loss_D: 1.2311 Loss_G: 1.4969\n",
      "[1/25][85/110] Loss_D: 0.4361 Loss_G: 4.3450\n",
      "[1/25][86/110] Loss_D: 0.5038 Loss_G: 6.0026\n",
      "[1/25][87/110] Loss_D: 0.8799 Loss_G: 4.0583\n",
      "[1/25][88/110] Loss_D: 0.7937 Loss_G: 4.4718\n",
      "[1/25][89/110] Loss_D: 0.4560 Loss_G: 3.0857\n",
      "[1/25][90/110] Loss_D: 0.8335 Loss_G: 4.7456\n",
      "[1/25][91/110] Loss_D: 0.7193 Loss_G: 8.8149\n",
      "[1/25][92/110] Loss_D: 1.5614 Loss_G: 3.8886\n",
      "[1/25][93/110] Loss_D: 1.1337 Loss_G: 6.1862\n",
      "[1/25][94/110] Loss_D: 0.6529 Loss_G: 3.3105\n",
      "[1/25][95/110] Loss_D: 0.5857 Loss_G: 6.2848\n",
      "[1/25][96/110] Loss_D: 0.6312 Loss_G: 2.9006\n",
      "[1/25][97/110] Loss_D: 1.0253 Loss_G: 7.5022\n",
      "[1/25][98/110] Loss_D: 1.6458 Loss_G: 2.3794\n",
      "[1/25][99/110] Loss_D: 1.4020 Loss_G: 6.7816\n",
      "[1/25][100/110] Loss_D: 0.7938 Loss_G: 4.8216\n",
      "[1/25][101/110] Loss_D: 0.7467 Loss_G: 3.6032\n",
      "[1/25][102/110] Loss_D: 0.8873 Loss_G: 5.0569\n",
      "[1/25][103/110] Loss_D: 2.6040 Loss_G: 2.7282\n",
      "[1/25][104/110] Loss_D: 1.3911 Loss_G: 8.3887\n",
      "[1/25][105/110] Loss_D: 2.3545 Loss_G: 2.9675\n",
      "[1/25][106/110] Loss_D: 1.5133 Loss_G: 4.7680\n",
      "[1/25][107/110] Loss_D: 1.0390 Loss_G: 5.4285\n",
      "[1/25][108/110] Loss_D: 0.5459 Loss_G: 4.5031\n",
      "[1/25][109/110] Loss_D: 0.9339 Loss_G: 6.1131\n",
      "[2/25][0/110] Loss_D: 1.8725 Loss_G: 2.1799\n",
      "[2/25][1/110] Loss_D: 2.0371 Loss_G: 7.0626\n",
      "[2/25][2/110] Loss_D: 0.8726 Loss_G: 4.7797\n",
      "[2/25][3/110] Loss_D: 0.8353 Loss_G: 2.4593\n",
      "[2/25][4/110] Loss_D: 2.6312 Loss_G: 5.8242\n",
      "[2/25][5/110] Loss_D: 1.3080 Loss_G: 3.3764\n",
      "[2/25][6/110] Loss_D: 0.7801 Loss_G: 2.9132\n",
      "[2/25][7/110] Loss_D: 0.9032 Loss_G: 7.1632\n",
      "[2/25][8/110] Loss_D: 0.8623 Loss_G: 4.6240\n",
      "[2/25][9/110] Loss_D: 0.8095 Loss_G: 4.3567\n",
      "[2/25][10/110] Loss_D: 1.2942 Loss_G: 8.1734\n",
      "[2/25][11/110] Loss_D: 3.0819 Loss_G: 2.6350\n",
      "[2/25][12/110] Loss_D: 1.4855 Loss_G: 3.8787\n",
      "[2/25][13/110] Loss_D: 1.5836 Loss_G: 7.4426\n",
      "[2/25][14/110] Loss_D: 0.8570 Loss_G: 5.5429\n",
      "[2/25][15/110] Loss_D: 0.4634 Loss_G: 4.6622\n",
      "[2/25][16/110] Loss_D: 1.0959 Loss_G: 6.1791\n",
      "[2/25][17/110] Loss_D: 0.6383 Loss_G: 4.2774\n",
      "[2/25][18/110] Loss_D: 0.9804 Loss_G: 5.4933\n",
      "[2/25][19/110] Loss_D: 0.5380 Loss_G: 5.5947\n",
      "[2/25][20/110] Loss_D: 1.0182 Loss_G: 5.0515\n",
      "[2/25][21/110] Loss_D: 1.1679 Loss_G: 2.2077\n",
      "[2/25][22/110] Loss_D: 2.8229 Loss_G: 12.2776\n",
      "[2/25][23/110] Loss_D: 3.5620 Loss_G: 7.2005\n",
      "[2/25][24/110] Loss_D: 1.2547 Loss_G: 1.4451\n",
      "[2/25][25/110] Loss_D: 2.0698 Loss_G: 3.6138\n",
      "[2/25][26/110] Loss_D: 1.1610 Loss_G: 5.7350\n",
      "[2/25][27/110] Loss_D: 1.3398 Loss_G: 2.8953\n",
      "[2/25][28/110] Loss_D: 1.0020 Loss_G: 2.9488\n",
      "[2/25][29/110] Loss_D: 1.6884 Loss_G: 8.6605\n",
      "[2/25][30/110] Loss_D: 1.6751 Loss_G: 6.0201\n",
      "[2/25][31/110] Loss_D: 0.6581 Loss_G: 2.4204\n",
      "[2/25][32/110] Loss_D: 2.6091 Loss_G: 8.3333\n",
      "[2/25][33/110] Loss_D: 2.7847 Loss_G: 4.6233\n",
      "[2/25][34/110] Loss_D: 0.7616 Loss_G: 2.0064\n",
      "[2/25][35/110] Loss_D: 1.7800 Loss_G: 5.3239\n",
      "[2/25][36/110] Loss_D: 0.6086 Loss_G: 5.3318\n",
      "[2/25][37/110] Loss_D: 0.5762 Loss_G: 3.5668\n",
      "[2/25][38/110] Loss_D: 0.7932 Loss_G: 4.4941\n",
      "[2/25][39/110] Loss_D: 1.0209 Loss_G: 4.5903\n",
      "[2/25][40/110] Loss_D: 0.6864 Loss_G: 4.1161\n",
      "[2/25][41/110] Loss_D: 0.8983 Loss_G: 6.6573\n",
      "[2/25][42/110] Loss_D: 1.8957 Loss_G: 3.0295\n",
      "[2/25][43/110] Loss_D: 2.3664 Loss_G: 8.0414\n",
      "[2/25][44/110] Loss_D: 1.3566 Loss_G: 4.8728\n",
      "[2/25][45/110] Loss_D: 0.1538 Loss_G: 4.2022\n",
      "[2/25][46/110] Loss_D: 0.5060 Loss_G: 3.9464\n",
      "[2/25][47/110] Loss_D: 0.8133 Loss_G: 6.6193\n",
      "[2/25][48/110] Loss_D: 1.3151 Loss_G: 3.2130\n",
      "[2/25][49/110] Loss_D: 0.9609 Loss_G: 6.7290\n",
      "[2/25][50/110] Loss_D: 0.4428 Loss_G: 4.9635\n",
      "[2/25][51/110] Loss_D: 1.2826 Loss_G: 4.9336\n",
      "[2/25][52/110] Loss_D: 1.0671 Loss_G: 3.2526\n",
      "[2/25][53/110] Loss_D: 1.5167 Loss_G: 9.3271\n",
      "[2/25][54/110] Loss_D: 2.4786 Loss_G: 2.2703\n",
      "[2/25][55/110] Loss_D: 3.4283 Loss_G: 8.1610\n",
      "[2/25][56/110] Loss_D: 4.5452 Loss_G: 5.6092\n",
      "[2/25][57/110] Loss_D: 2.3823 Loss_G: 1.9494\n",
      "[2/25][58/110] Loss_D: 2.6814 Loss_G: 7.1773\n",
      "[2/25][59/110] Loss_D: 1.2532 Loss_G: 5.5455\n",
      "[2/25][60/110] Loss_D: 0.5855 Loss_G: 3.1269\n",
      "[2/25][61/110] Loss_D: 1.2870 Loss_G: 7.0218\n",
      "[2/25][62/110] Loss_D: 0.4677 Loss_G: 6.3608\n",
      "[2/25][63/110] Loss_D: 1.4786 Loss_G: 3.4145\n",
      "[2/25][64/110] Loss_D: 1.3209 Loss_G: 7.6336\n",
      "[2/25][65/110] Loss_D: 1.7563 Loss_G: 4.8191\n",
      "[2/25][66/110] Loss_D: 0.6687 Loss_G: 1.6368\n",
      "[2/25][67/110] Loss_D: 2.2380 Loss_G: 4.6728\n",
      "[2/25][68/110] Loss_D: 1.1668 Loss_G: 2.7003\n",
      "[2/25][69/110] Loss_D: 0.4477 Loss_G: 2.8963\n",
      "[2/25][70/110] Loss_D: 1.0443 Loss_G: 3.8237\n",
      "[2/25][71/110] Loss_D: 1.5967 Loss_G: 1.7801\n",
      "[2/25][72/110] Loss_D: 1.6508 Loss_G: 5.0040\n",
      "[2/25][73/110] Loss_D: 2.0494 Loss_G: 3.1495\n",
      "[2/25][74/110] Loss_D: 0.7592 Loss_G: 3.7081\n",
      "[2/25][75/110] Loss_D: 1.0999 Loss_G: 3.7446\n",
      "[2/25][76/110] Loss_D: 1.8421 Loss_G: 2.6471\n",
      "[2/25][77/110] Loss_D: 1.3853 Loss_G: 5.7011\n",
      "[2/25][78/110] Loss_D: 0.6035 Loss_G: 4.6480\n",
      "[2/25][79/110] Loss_D: 0.9597 Loss_G: 1.3180\n",
      "[2/25][80/110] Loss_D: 2.5620 Loss_G: 9.1403\n",
      "[2/25][81/110] Loss_D: 1.7181 Loss_G: 6.7134\n",
      "[2/25][82/110] Loss_D: 0.7798 Loss_G: 2.7937\n",
      "[2/25][83/110] Loss_D: 3.0012 Loss_G: 2.7680\n",
      "[2/25][84/110] Loss_D: 0.5868 Loss_G: 5.1461\n",
      "[2/25][85/110] Loss_D: 1.8485 Loss_G: 2.4898\n",
      "[2/25][86/110] Loss_D: 1.0550 Loss_G: 2.1591\n",
      "[2/25][87/110] Loss_D: 0.6974 Loss_G: 2.7409\n",
      "[2/25][88/110] Loss_D: 2.1587 Loss_G: 3.5409\n",
      "[2/25][89/110] Loss_D: 1.8576 Loss_G: 2.2491\n",
      "[2/25][90/110] Loss_D: 0.9804 Loss_G: 5.0153\n",
      "[2/25][91/110] Loss_D: 0.8162 Loss_G: 4.3293\n",
      "[2/25][92/110] Loss_D: 1.2965 Loss_G: 2.5201\n",
      "[2/25][93/110] Loss_D: 2.0162 Loss_G: 3.3734\n",
      "[2/25][94/110] Loss_D: 2.3889 Loss_G: 4.2303\n",
      "[2/25][95/110] Loss_D: 1.0715 Loss_G: 3.1670\n",
      "[2/25][96/110] Loss_D: 1.2418 Loss_G: 2.4771\n",
      "[2/25][97/110] Loss_D: 0.8875 Loss_G: 4.9860\n",
      "[2/25][98/110] Loss_D: 1.1645 Loss_G: 3.1016\n",
      "[2/25][99/110] Loss_D: 1.6378 Loss_G: 1.3397\n",
      "[2/25][100/110] Loss_D: 2.2692 Loss_G: 3.1238\n",
      "[2/25][101/110] Loss_D: 1.4984 Loss_G: 3.5739\n",
      "[2/25][102/110] Loss_D: 1.5995 Loss_G: 1.7109\n",
      "[2/25][103/110] Loss_D: 0.9351 Loss_G: 3.0679\n",
      "[2/25][104/110] Loss_D: 0.7233 Loss_G: 2.9207\n",
      "[2/25][105/110] Loss_D: 1.3894 Loss_G: 1.6045\n",
      "[2/25][106/110] Loss_D: 1.0228 Loss_G: 3.3622\n",
      "[2/25][107/110] Loss_D: 1.3003 Loss_G: 3.2018\n",
      "[2/25][108/110] Loss_D: 1.2530 Loss_G: 2.2325\n",
      "[2/25][109/110] Loss_D: 1.5544 Loss_G: 2.9485\n",
      "[3/25][0/110] Loss_D: 0.8154 Loss_G: 4.4711\n",
      "[3/25][1/110] Loss_D: 1.3877 Loss_G: 3.0703\n",
      "[3/25][2/110] Loss_D: 1.1794 Loss_G: 2.6257\n",
      "[3/25][3/110] Loss_D: 1.3304 Loss_G: 3.6827\n",
      "[3/25][4/110] Loss_D: 0.8506 Loss_G: 3.4385\n",
      "[3/25][5/110] Loss_D: 0.5683 Loss_G: 2.8140\n",
      "[3/25][6/110] Loss_D: 0.6763 Loss_G: 3.6947\n",
      "[3/25][7/110] Loss_D: 0.8959 Loss_G: 2.8746\n",
      "[3/25][8/110] Loss_D: 1.2057 Loss_G: 1.9753\n",
      "[3/25][9/110] Loss_D: 1.2600 Loss_G: 3.8147\n",
      "[3/25][10/110] Loss_D: 1.2488 Loss_G: 3.9390\n",
      "[3/25][11/110] Loss_D: 0.7889 Loss_G: 2.2621\n",
      "[3/25][12/110] Loss_D: 1.1476 Loss_G: 2.8469\n",
      "[3/25][13/110] Loss_D: 0.8356 Loss_G: 3.3830\n",
      "[3/25][14/110] Loss_D: 0.8800 Loss_G: 3.5817\n",
      "[3/25][15/110] Loss_D: 0.8889 Loss_G: 2.5271\n",
      "[3/25][16/110] Loss_D: 0.9170 Loss_G: 4.4231\n",
      "[3/25][17/110] Loss_D: 1.2379 Loss_G: 1.9517\n",
      "[3/25][18/110] Loss_D: 1.1652 Loss_G: 3.5485\n",
      "[3/25][19/110] Loss_D: 0.8344 Loss_G: 2.9044\n",
      "[3/25][20/110] Loss_D: 0.5996 Loss_G: 2.6383\n",
      "[3/25][21/110] Loss_D: 0.9466 Loss_G: 4.9223\n",
      "[3/25][22/110] Loss_D: 1.3870 Loss_G: 2.1621\n",
      "[3/25][23/110] Loss_D: 1.1552 Loss_G: 3.0474\n",
      "[3/25][24/110] Loss_D: 1.0794 Loss_G: 2.4133\n",
      "[3/25][25/110] Loss_D: 0.6323 Loss_G: 3.4760\n",
      "[3/25][26/110] Loss_D: 0.5747 Loss_G: 3.2928\n",
      "[3/25][27/110] Loss_D: 0.7591 Loss_G: 3.8095\n",
      "[3/25][28/110] Loss_D: 1.0686 Loss_G: 3.2916\n",
      "[3/25][29/110] Loss_D: 0.4822 Loss_G: 2.5469\n",
      "[3/25][30/110] Loss_D: 0.7432 Loss_G: 2.7965\n",
      "[3/25][31/110] Loss_D: 1.2265 Loss_G: 6.0511\n",
      "[3/25][32/110] Loss_D: 1.2147 Loss_G: 2.7286\n",
      "[3/25][33/110] Loss_D: 1.3846 Loss_G: 5.2142\n",
      "[3/25][34/110] Loss_D: 1.5521 Loss_G: 3.1153\n",
      "[3/25][35/110] Loss_D: 1.5170 Loss_G: 5.2550\n",
      "[3/25][36/110] Loss_D: 1.4234 Loss_G: 2.6098\n",
      "[3/25][37/110] Loss_D: 0.8475 Loss_G: 3.6924\n",
      "[3/25][38/110] Loss_D: 0.4574 Loss_G: 3.8524\n",
      "[3/25][39/110] Loss_D: 1.0144 Loss_G: 2.7535\n",
      "[3/25][40/110] Loss_D: 0.9811 Loss_G: 3.5729\n",
      "[3/25][41/110] Loss_D: 0.6700 Loss_G: 3.7537\n",
      "[3/25][42/110] Loss_D: 0.8243 Loss_G: 5.5146\n",
      "[3/25][43/110] Loss_D: 2.1702 Loss_G: 2.4383\n",
      "[3/25][44/110] Loss_D: 1.0316 Loss_G: 7.5315\n",
      "[3/25][45/110] Loss_D: 2.1959 Loss_G: 4.3867\n",
      "[3/25][46/110] Loss_D: 0.4239 Loss_G: 2.4706\n",
      "[3/25][47/110] Loss_D: 0.7570 Loss_G: 5.3007\n",
      "[3/25][48/110] Loss_D: 0.7888 Loss_G: 2.7693\n",
      "[3/25][49/110] Loss_D: 0.9299 Loss_G: 3.8889\n",
      "[3/25][50/110] Loss_D: 0.9923 Loss_G: 3.1758\n",
      "[3/25][51/110] Loss_D: 1.8485 Loss_G: 7.4089\n",
      "[3/25][52/110] Loss_D: 2.4627 Loss_G: 2.9373\n",
      "[3/25][53/110] Loss_D: 1.3530 Loss_G: 3.3696\n",
      "[3/25][54/110] Loss_D: 0.8785 Loss_G: 4.5643\n",
      "[3/25][55/110] Loss_D: 1.0363 Loss_G: 6.9895\n",
      "[3/25][56/110] Loss_D: 1.5310 Loss_G: 1.6275\n",
      "[3/25][57/110] Loss_D: 1.4217 Loss_G: 3.5896\n",
      "[3/25][58/110] Loss_D: 0.6094 Loss_G: 5.9437\n",
      "[3/25][59/110] Loss_D: 1.8480 Loss_G: 2.6469\n",
      "[3/25][60/110] Loss_D: 1.0522 Loss_G: 2.6979\n",
      "[3/25][61/110] Loss_D: 1.1382 Loss_G: 3.0676\n",
      "[3/25][62/110] Loss_D: 0.8664 Loss_G: 4.1391\n",
      "[3/25][63/110] Loss_D: 0.5470 Loss_G: 3.8772\n",
      "[3/25][64/110] Loss_D: 0.6279 Loss_G: 3.4828\n",
      "[3/25][65/110] Loss_D: 1.1946 Loss_G: 7.2643\n",
      "[3/25][66/110] Loss_D: 3.3457 Loss_G: 3.8760\n",
      "[3/25][67/110] Loss_D: 0.8930 Loss_G: 2.1809\n",
      "[3/25][68/110] Loss_D: 1.5269 Loss_G: 6.7527\n",
      "[3/25][69/110] Loss_D: 1.4087 Loss_G: 3.7858\n",
      "[3/25][70/110] Loss_D: 0.5973 Loss_G: 2.2692\n",
      "[3/25][71/110] Loss_D: 1.1281 Loss_G: 4.1489\n",
      "[3/25][72/110] Loss_D: 0.6332 Loss_G: 3.8648\n",
      "[3/25][73/110] Loss_D: 1.4287 Loss_G: 5.1681\n",
      "[3/25][74/110] Loss_D: 1.3238 Loss_G: 3.2021\n",
      "[3/25][75/110] Loss_D: 0.9105 Loss_G: 3.2489\n",
      "[3/25][76/110] Loss_D: 0.8227 Loss_G: 5.6499\n",
      "[3/25][77/110] Loss_D: 0.9282 Loss_G: 4.0570\n",
      "[3/25][78/110] Loss_D: 1.0141 Loss_G: 4.5491\n",
      "[3/25][79/110] Loss_D: 0.6831 Loss_G: 4.3758\n",
      "[3/25][80/110] Loss_D: 0.6418 Loss_G: 4.4906\n",
      "[3/25][81/110] Loss_D: 0.6057 Loss_G: 5.0890\n",
      "[3/25][82/110] Loss_D: 1.2217 Loss_G: 3.6580\n",
      "[3/25][83/110] Loss_D: 1.4103 Loss_G: 2.6938\n",
      "[3/25][84/110] Loss_D: 1.4199 Loss_G: 5.4231\n",
      "[3/25][85/110] Loss_D: 0.6743 Loss_G: 4.0296\n",
      "[3/25][86/110] Loss_D: 1.5182 Loss_G: 2.5552\n",
      "[3/25][87/110] Loss_D: 2.1166 Loss_G: 7.1756\n",
      "[3/25][88/110] Loss_D: 2.2900 Loss_G: 3.1496\n",
      "[3/25][89/110] Loss_D: 0.8786 Loss_G: 2.4867\n",
      "[3/25][90/110] Loss_D: 1.1169 Loss_G: 3.9828\n",
      "[3/25][91/110] Loss_D: 1.3863 Loss_G: 3.0768\n",
      "[3/25][92/110] Loss_D: 1.0795 Loss_G: 2.0985\n",
      "[3/25][93/110] Loss_D: 1.2486 Loss_G: 3.1491\n",
      "[3/25][94/110] Loss_D: 1.4344 Loss_G: 3.8826\n",
      "[3/25][95/110] Loss_D: 1.8893 Loss_G: 2.0896\n",
      "[3/25][96/110] Loss_D: 1.4416 Loss_G: 3.7718\n",
      "[3/25][97/110] Loss_D: 1.2622 Loss_G: 2.1091\n",
      "[3/25][98/110] Loss_D: 1.2623 Loss_G: 3.2349\n",
      "[3/25][99/110] Loss_D: 0.5271 Loss_G: 4.9080\n",
      "[3/25][100/110] Loss_D: 1.0619 Loss_G: 3.1487\n",
      "[3/25][101/110] Loss_D: 1.4631 Loss_G: 3.3459\n",
      "[3/25][102/110] Loss_D: 0.9598 Loss_G: 6.5580\n",
      "[3/25][103/110] Loss_D: 2.2975 Loss_G: 1.9629\n",
      "[3/25][104/110] Loss_D: 1.8593 Loss_G: 7.4905\n",
      "[3/25][105/110] Loss_D: 2.1001 Loss_G: 3.7843\n",
      "[3/25][106/110] Loss_D: 0.7999 Loss_G: 1.9007\n",
      "[3/25][107/110] Loss_D: 1.3287 Loss_G: 5.1657\n",
      "[3/25][108/110] Loss_D: 1.1888 Loss_G: 5.0752\n",
      "[3/25][109/110] Loss_D: 1.7895 Loss_G: 2.0119\n",
      "[4/25][0/110] Loss_D: 1.6159 Loss_G: 3.5695\n",
      "[4/25][1/110] Loss_D: 0.7289 Loss_G: 3.4711\n",
      "[4/25][2/110] Loss_D: 0.8129 Loss_G: 2.4559\n",
      "[4/25][3/110] Loss_D: 0.9737 Loss_G: 3.7276\n",
      "[4/25][4/110] Loss_D: 0.5777 Loss_G: 4.0208\n",
      "[4/25][5/110] Loss_D: 1.0020 Loss_G: 2.5732\n",
      "[4/25][6/110] Loss_D: 1.6014 Loss_G: 6.6598\n",
      "[4/25][7/110] Loss_D: 0.4837 Loss_G: 5.0889\n",
      "[4/25][8/110] Loss_D: 0.9440 Loss_G: 2.4750\n",
      "[4/25][9/110] Loss_D: 1.3384 Loss_G: 4.3520\n",
      "[4/25][10/110] Loss_D: 0.5689 Loss_G: 4.4086\n",
      "[4/25][11/110] Loss_D: 1.4143 Loss_G: 1.6552\n",
      "[4/25][12/110] Loss_D: 2.1373 Loss_G: 3.3538\n",
      "[4/25][13/110] Loss_D: 1.0490 Loss_G: 3.3662\n",
      "[4/25][14/110] Loss_D: 1.0345 Loss_G: 2.8847\n",
      "[4/25][15/110] Loss_D: 0.8599 Loss_G: 3.9123\n",
      "[4/25][16/110] Loss_D: 1.1708 Loss_G: 3.4549\n",
      "[4/25][17/110] Loss_D: 1.6728 Loss_G: 3.6165\n",
      "[4/25][18/110] Loss_D: 1.4535 Loss_G: 6.9445\n",
      "[4/25][19/110] Loss_D: 2.6760 Loss_G: 3.1416\n",
      "[4/25][20/110] Loss_D: 0.7455 Loss_G: 4.4689\n",
      "[4/25][21/110] Loss_D: 1.5143 Loss_G: 6.5895\n",
      "[4/25][22/110] Loss_D: 1.1787 Loss_G: 4.3874\n",
      "[4/25][23/110] Loss_D: 0.7021 Loss_G: 2.9021\n",
      "[4/25][24/110] Loss_D: 0.7852 Loss_G: 4.7304\n",
      "[4/25][25/110] Loss_D: 0.6142 Loss_G: 3.9434\n",
      "[4/25][26/110] Loss_D: 1.3262 Loss_G: 3.4842\n",
      "[4/25][27/110] Loss_D: 1.1534 Loss_G: 5.1070\n",
      "[4/25][28/110] Loss_D: 0.3939 Loss_G: 4.5394\n",
      "[4/25][29/110] Loss_D: 0.6297 Loss_G: 3.3767\n",
      "[4/25][30/110] Loss_D: 1.3888 Loss_G: 1.8557\n",
      "[4/25][31/110] Loss_D: 0.9384 Loss_G: 4.9921\n",
      "[4/25][32/110] Loss_D: 0.6750 Loss_G: 3.5527\n",
      "[4/25][33/110] Loss_D: 0.6198 Loss_G: 3.9474\n",
      "[4/25][34/110] Loss_D: 1.3228 Loss_G: 2.1921\n",
      "[4/25][35/110] Loss_D: 1.0878 Loss_G: 4.5651\n",
      "[4/25][36/110] Loss_D: 0.4052 Loss_G: 4.2086\n",
      "[4/25][37/110] Loss_D: 1.4155 Loss_G: 3.3582\n",
      "[4/25][38/110] Loss_D: 1.2132 Loss_G: 3.3525\n",
      "[4/25][39/110] Loss_D: 1.1217 Loss_G: 3.7048\n",
      "[4/25][40/110] Loss_D: 1.0856 Loss_G: 3.0454\n",
      "[4/25][41/110] Loss_D: 0.6786 Loss_G: 4.3603\n",
      "[4/25][42/110] Loss_D: 0.7166 Loss_G: 5.6402\n",
      "[4/25][43/110] Loss_D: 0.3958 Loss_G: 3.6633\n",
      "[4/25][44/110] Loss_D: 0.7907 Loss_G: 4.8938\n",
      "[4/25][45/110] Loss_D: 0.6967 Loss_G: 3.3249\n",
      "[4/25][46/110] Loss_D: 0.8728 Loss_G: 5.9166\n",
      "[4/25][47/110] Loss_D: 0.9416 Loss_G: 3.1613\n",
      "[4/25][48/110] Loss_D: 1.5529 Loss_G: 7.0244\n",
      "[4/25][49/110] Loss_D: 1.4377 Loss_G: 1.4990\n",
      "[4/25][50/110] Loss_D: 3.6168 Loss_G: 9.8366\n",
      "[4/25][51/110] Loss_D: 2.7093 Loss_G: 6.9077\n",
      "[4/25][52/110] Loss_D: 1.0454 Loss_G: 2.1443\n",
      "[4/25][53/110] Loss_D: 1.0190 Loss_G: 2.8245\n",
      "[4/25][54/110] Loss_D: 1.1870 Loss_G: 4.4287\n",
      "[4/25][55/110] Loss_D: 0.8968 Loss_G: 3.5911\n",
      "[4/25][56/110] Loss_D: 1.1112 Loss_G: 2.8202\n",
      "[4/25][57/110] Loss_D: 0.9997 Loss_G: 2.4888\n",
      "[4/25][58/110] Loss_D: 1.0513 Loss_G: 5.7701\n",
      "[4/25][59/110] Loss_D: 1.9535 Loss_G: 2.8467\n",
      "[4/25][60/110] Loss_D: 1.0136 Loss_G: 2.0714\n",
      "[4/25][61/110] Loss_D: 1.2635 Loss_G: 3.2950\n",
      "[4/25][62/110] Loss_D: 0.7919 Loss_G: 3.7702\n",
      "[4/25][63/110] Loss_D: 0.9330 Loss_G: 2.2340\n",
      "[4/25][64/110] Loss_D: 1.2748 Loss_G: 3.5951\n",
      "[4/25][65/110] Loss_D: 0.6532 Loss_G: 5.0126\n",
      "[4/25][66/110] Loss_D: 2.1550 Loss_G: 1.6795\n",
      "[4/25][67/110] Loss_D: 1.6398 Loss_G: 5.3005\n",
      "[4/25][68/110] Loss_D: 1.7726 Loss_G: 2.4554\n",
      "[4/25][69/110] Loss_D: 1.0421 Loss_G: 3.9328\n",
      "[4/25][70/110] Loss_D: 1.5395 Loss_G: 2.5291\n",
      "[4/25][71/110] Loss_D: 2.0747 Loss_G: 7.2072\n",
      "[4/25][72/110] Loss_D: 2.4493 Loss_G: 3.8971\n",
      "[4/25][73/110] Loss_D: 0.4890 Loss_G: 2.1999\n",
      "[4/25][74/110] Loss_D: 2.4574 Loss_G: 6.6223\n",
      "[4/25][75/110] Loss_D: 1.7853 Loss_G: 2.8184\n",
      "[4/25][76/110] Loss_D: 0.9186 Loss_G: 3.9614\n",
      "[4/25][77/110] Loss_D: 1.7399 Loss_G: 2.8284\n",
      "[4/25][78/110] Loss_D: 1.8050 Loss_G: 2.0685\n",
      "[4/25][79/110] Loss_D: 1.7381 Loss_G: 4.6935\n",
      "[4/25][80/110] Loss_D: 0.8009 Loss_G: 3.1457\n",
      "[4/25][81/110] Loss_D: 0.8256 Loss_G: 2.5169\n",
      "[4/25][82/110] Loss_D: 1.5328 Loss_G: 2.7635\n",
      "[4/25][83/110] Loss_D: 0.9529 Loss_G: 4.2347\n",
      "[4/25][84/110] Loss_D: 1.3138 Loss_G: 1.8327\n",
      "[4/25][85/110] Loss_D: 1.2944 Loss_G: 3.1837\n",
      "[4/25][86/110] Loss_D: 0.3803 Loss_G: 4.2041\n",
      "[4/25][87/110] Loss_D: 1.5352 Loss_G: 1.3011\n",
      "[4/25][88/110] Loss_D: 1.4847 Loss_G: 5.9478\n",
      "[4/25][89/110] Loss_D: 1.2812 Loss_G: 3.5754\n",
      "[4/25][90/110] Loss_D: 1.1650 Loss_G: 1.3537\n",
      "[4/25][91/110] Loss_D: 1.8171 Loss_G: 4.4316\n",
      "[4/25][92/110] Loss_D: 0.6441 Loss_G: 3.6110\n",
      "[4/25][93/110] Loss_D: 0.8368 Loss_G: 2.2904\n",
      "[4/25][94/110] Loss_D: 1.1806 Loss_G: 4.8494\n",
      "[4/25][95/110] Loss_D: 0.9439 Loss_G: 2.9933\n",
      "[4/25][96/110] Loss_D: 1.2204 Loss_G: 2.9094\n",
      "[4/25][97/110] Loss_D: 1.2456 Loss_G: 5.0476\n",
      "[4/25][98/110] Loss_D: 1.1609 Loss_G: 2.7596\n",
      "[4/25][99/110] Loss_D: 0.9568 Loss_G: 3.3364\n",
      "[4/25][100/110] Loss_D: 1.4079 Loss_G: 5.4455\n",
      "[4/25][101/110] Loss_D: 0.8508 Loss_G: 3.7437\n",
      "[4/25][102/110] Loss_D: 1.2945 Loss_G: 5.3937\n",
      "[4/25][103/110] Loss_D: 0.5643 Loss_G: 4.7215\n",
      "[4/25][104/110] Loss_D: 1.0662 Loss_G: 2.8884\n",
      "[4/25][105/110] Loss_D: 0.9303 Loss_G: 3.5649\n",
      "[4/25][106/110] Loss_D: 0.8830 Loss_G: 3.7489\n",
      "[4/25][107/110] Loss_D: 0.7863 Loss_G: 5.0554\n",
      "[4/25][108/110] Loss_D: 0.3547 Loss_G: 4.2802\n",
      "[4/25][109/110] Loss_D: 1.7755 Loss_G: 1.8165\n",
      "[5/25][0/110] Loss_D: 2.0353 Loss_G: 7.0559\n",
      "[5/25][1/110] Loss_D: 2.1299 Loss_G: 5.3975\n",
      "[5/25][2/110] Loss_D: 1.2932 Loss_G: 0.7719\n",
      "[5/25][3/110] Loss_D: 2.9863 Loss_G: 4.8624\n",
      "[5/25][4/110] Loss_D: 0.3142 Loss_G: 5.9122\n",
      "[5/25][5/110] Loss_D: 0.9677 Loss_G: 2.9432\n",
      "[5/25][6/110] Loss_D: 1.0925 Loss_G: 3.0869\n",
      "[5/25][7/110] Loss_D: 0.8940 Loss_G: 3.9836\n",
      "[5/25][8/110] Loss_D: 1.6426 Loss_G: 2.2515\n",
      "[5/25][9/110] Loss_D: 1.4246 Loss_G: 4.9445\n",
      "[5/25][10/110] Loss_D: 0.5239 Loss_G: 4.2188\n",
      "[5/25][11/110] Loss_D: 0.9201 Loss_G: 2.8534\n",
      "[5/25][12/110] Loss_D: 1.4984 Loss_G: 3.7538\n",
      "[5/25][13/110] Loss_D: 0.8455 Loss_G: 3.2069\n",
      "[5/25][14/110] Loss_D: 0.7353 Loss_G: 2.5517\n",
      "[5/25][15/110] Loss_D: 1.2094 Loss_G: 4.4926\n",
      "[5/25][16/110] Loss_D: 1.1577 Loss_G: 5.0577\n",
      "[5/25][17/110] Loss_D: 1.2754 Loss_G: 2.1762\n",
      "[5/25][18/110] Loss_D: 1.3093 Loss_G: 2.4669\n",
      "[5/25][19/110] Loss_D: 0.7795 Loss_G: 4.5905\n",
      "[5/25][20/110] Loss_D: 0.8318 Loss_G: 3.5285\n",
      "[5/25][21/110] Loss_D: 1.1809 Loss_G: 1.8038\n",
      "[5/25][22/110] Loss_D: 1.9434 Loss_G: 5.6435\n",
      "[5/25][23/110] Loss_D: 1.2794 Loss_G: 4.8188\n",
      "[5/25][24/110] Loss_D: 0.9604 Loss_G: 3.3912\n",
      "[5/25][25/110] Loss_D: 2.1924 Loss_G: 3.4304\n",
      "[5/25][26/110] Loss_D: 1.8448 Loss_G: 2.7535\n",
      "[5/25][27/110] Loss_D: 1.3390 Loss_G: 3.8288\n",
      "[5/25][28/110] Loss_D: 1.8868 Loss_G: 2.8210\n",
      "[5/25][29/110] Loss_D: 1.5693 Loss_G: 3.5348\n",
      "[5/25][30/110] Loss_D: 1.4596 Loss_G: 2.4272\n",
      "[5/25][31/110] Loss_D: 1.1118 Loss_G: 2.5351\n",
      "[5/25][32/110] Loss_D: 0.9875 Loss_G: 3.8988\n",
      "[5/25][33/110] Loss_D: 0.8453 Loss_G: 2.6291\n",
      "[5/25][34/110] Loss_D: 0.9066 Loss_G: 3.7498\n",
      "[5/25][35/110] Loss_D: 0.9640 Loss_G: 2.9668\n",
      "[5/25][36/110] Loss_D: 1.0754 Loss_G: 2.2054\n",
      "[5/25][37/110] Loss_D: 0.9108 Loss_G: 3.3322\n",
      "[5/25][38/110] Loss_D: 0.5872 Loss_G: 3.6308\n",
      "[5/25][39/110] Loss_D: 1.1617 Loss_G: 2.2776\n",
      "[5/25][40/110] Loss_D: 1.2903 Loss_G: 5.3597\n",
      "[5/25][41/110] Loss_D: 1.1676 Loss_G: 1.8721\n",
      "[5/25][42/110] Loss_D: 1.3589 Loss_G: 5.4703\n",
      "[5/25][43/110] Loss_D: 1.1492 Loss_G: 3.4694\n",
      "[5/25][44/110] Loss_D: 0.7230 Loss_G: 2.8800\n",
      "[5/25][45/110] Loss_D: 0.6571 Loss_G: 4.2309\n",
      "[5/25][46/110] Loss_D: 0.7910 Loss_G: 3.1164\n",
      "[5/25][47/110] Loss_D: 1.2472 Loss_G: 3.6332\n",
      "[5/25][48/110] Loss_D: 1.2185 Loss_G: 3.6997\n",
      "[5/25][49/110] Loss_D: 0.8040 Loss_G: 5.0146\n",
      "[5/25][50/110] Loss_D: 0.5366 Loss_G: 3.8953\n",
      "[5/25][51/110] Loss_D: 1.3588 Loss_G: 2.1850\n",
      "[5/25][52/110] Loss_D: 2.9031 Loss_G: 5.9827\n",
      "[5/25][53/110] Loss_D: 1.6987 Loss_G: 5.3533\n",
      "[5/25][54/110] Loss_D: 1.4210 Loss_G: 3.6552\n",
      "[5/25][55/110] Loss_D: 1.9842 Loss_G: 7.3668\n",
      "[5/25][56/110] Loss_D: 0.6449 Loss_G: 6.4946\n",
      "[5/25][57/110] Loss_D: 0.7681 Loss_G: 2.5926\n",
      "[5/25][58/110] Loss_D: 1.9600 Loss_G: 5.4776\n",
      "[5/25][59/110] Loss_D: 0.7450 Loss_G: 4.1662\n",
      "[5/25][60/110] Loss_D: 0.6703 Loss_G: 4.3896\n",
      "[5/25][61/110] Loss_D: 2.0468 Loss_G: 8.9320\n",
      "[5/25][62/110] Loss_D: 3.0629 Loss_G: 6.0219\n",
      "[5/25][63/110] Loss_D: 0.6492 Loss_G: 2.0226\n",
      "[5/25][64/110] Loss_D: 2.2674 Loss_G: 5.4058\n",
      "[5/25][65/110] Loss_D: 1.1219 Loss_G: 3.8044\n",
      "[5/25][66/110] Loss_D: 2.2111 Loss_G: 2.5754\n",
      "[5/25][67/110] Loss_D: 1.1912 Loss_G: 4.8027\n",
      "[5/25][68/110] Loss_D: 1.1010 Loss_G: 3.4776\n",
      "[5/25][69/110] Loss_D: 1.2051 Loss_G: 4.6130\n",
      "[5/25][70/110] Loss_D: 1.4039 Loss_G: 1.6687\n",
      "[5/25][71/110] Loss_D: 1.9382 Loss_G: 6.8715\n",
      "[5/25][72/110] Loss_D: 1.3632 Loss_G: 4.3997\n",
      "[5/25][73/110] Loss_D: 1.2026 Loss_G: 2.3982\n",
      "[5/25][74/110] Loss_D: 1.6698 Loss_G: 6.7086\n",
      "[5/25][75/110] Loss_D: 1.1426 Loss_G: 4.5655\n",
      "[5/25][76/110] Loss_D: 0.4899 Loss_G: 2.8055\n",
      "[5/25][77/110] Loss_D: 0.6033 Loss_G: 3.8608\n",
      "[5/25][78/110] Loss_D: 0.6785 Loss_G: 4.7975\n",
      "[5/25][79/110] Loss_D: 0.5290 Loss_G: 4.1589\n",
      "[5/25][80/110] Loss_D: 0.2852 Loss_G: 4.2594\n",
      "[5/25][81/110] Loss_D: 0.7524 Loss_G: 4.4705\n",
      "[5/25][82/110] Loss_D: 1.2148 Loss_G: 5.9797\n",
      "[5/25][83/110] Loss_D: 2.5585 Loss_G: 1.2475\n",
      "[5/25][84/110] Loss_D: 4.3180 Loss_G: 7.1580\n",
      "[5/25][85/110] Loss_D: 2.3389 Loss_G: 3.7597\n",
      "[5/25][86/110] Loss_D: 1.3170 Loss_G: 1.4299\n",
      "[5/25][87/110] Loss_D: 1.5025 Loss_G: 2.1213\n",
      "[5/25][88/110] Loss_D: 1.0318 Loss_G: 2.5680\n",
      "[5/25][89/110] Loss_D: 1.3688 Loss_G: 2.7945\n",
      "[5/25][90/110] Loss_D: 0.8165 Loss_G: 3.0429\n",
      "[5/25][91/110] Loss_D: 0.5162 Loss_G: 2.9279\n",
      "[5/25][92/110] Loss_D: 0.6579 Loss_G: 3.5583\n",
      "[5/25][93/110] Loss_D: 1.4063 Loss_G: 3.2436\n",
      "[5/25][94/110] Loss_D: 1.1736 Loss_G: 3.0669\n",
      "[5/25][95/110] Loss_D: 0.9169 Loss_G: 2.5663\n",
      "[5/25][96/110] Loss_D: 1.0872 Loss_G: 4.3530\n",
      "[5/25][97/110] Loss_D: 1.0113 Loss_G: 2.8814\n",
      "[5/25][98/110] Loss_D: 1.9677 Loss_G: 6.8757\n",
      "[5/25][99/110] Loss_D: 1.4599 Loss_G: 3.5652\n",
      "[5/25][100/110] Loss_D: 1.2545 Loss_G: 1.8882\n",
      "[5/25][101/110] Loss_D: 2.2360 Loss_G: 4.6170\n",
      "[5/25][102/110] Loss_D: 0.7502 Loss_G: 4.5359\n",
      "[5/25][103/110] Loss_D: 0.9697 Loss_G: 2.5237\n",
      "[5/25][104/110] Loss_D: 1.4746 Loss_G: 4.4825\n",
      "[5/25][105/110] Loss_D: 0.9934 Loss_G: 3.2620\n",
      "[5/25][106/110] Loss_D: 1.2462 Loss_G: 2.2900\n",
      "[5/25][107/110] Loss_D: 1.1929 Loss_G: 4.2999\n",
      "[5/25][108/110] Loss_D: 1.3774 Loss_G: 3.5067\n",
      "[5/25][109/110] Loss_D: 0.8319 Loss_G: 5.6831\n",
      "[6/25][0/110] Loss_D: 0.6067 Loss_G: 4.3069\n",
      "[6/25][1/110] Loss_D: 0.4711 Loss_G: 3.1048\n",
      "[6/25][2/110] Loss_D: 1.1957 Loss_G: 2.6569\n",
      "[6/25][3/110] Loss_D: 1.6760 Loss_G: 4.4300\n",
      "[6/25][4/110] Loss_D: 2.3610 Loss_G: 2.4474\n",
      "[6/25][5/110] Loss_D: 0.9611 Loss_G: 3.9781\n",
      "[6/25][6/110] Loss_D: 0.7191 Loss_G: 3.3994\n",
      "[6/25][7/110] Loss_D: 1.1959 Loss_G: 2.6237\n",
      "[6/25][8/110] Loss_D: 0.9427 Loss_G: 2.8259\n",
      "[6/25][9/110] Loss_D: 0.6148 Loss_G: 3.6829\n",
      "[6/25][10/110] Loss_D: 0.8198 Loss_G: 3.8603\n",
      "[6/25][11/110] Loss_D: 0.9734 Loss_G: 2.7452\n",
      "[6/25][12/110] Loss_D: 1.0370 Loss_G: 4.0126\n",
      "[6/25][13/110] Loss_D: 1.3135 Loss_G: 4.1938\n",
      "[6/25][14/110] Loss_D: 0.6776 Loss_G: 3.2309\n",
      "[6/25][15/110] Loss_D: 0.8182 Loss_G: 3.1412\n",
      "[6/25][16/110] Loss_D: 1.0805 Loss_G: 3.1205\n",
      "[6/25][17/110] Loss_D: 1.3233 Loss_G: 2.9998\n",
      "[6/25][18/110] Loss_D: 1.2702 Loss_G: 3.2849\n",
      "[6/25][19/110] Loss_D: 1.8345 Loss_G: 3.5034\n",
      "[6/25][20/110] Loss_D: 1.3055 Loss_G: 3.8290\n",
      "[6/25][21/110] Loss_D: 0.6994 Loss_G: 4.6043\n",
      "[6/25][22/110] Loss_D: 0.9779 Loss_G: 2.3987\n",
      "[6/25][23/110] Loss_D: 0.9673 Loss_G: 2.4880\n",
      "[6/25][24/110] Loss_D: 2.0520 Loss_G: 4.8028\n",
      "[6/25][25/110] Loss_D: 0.9632 Loss_G: 3.3843\n",
      "[6/25][26/110] Loss_D: 0.9889 Loss_G: 1.7282\n",
      "[6/25][27/110] Loss_D: 0.9937 Loss_G: 3.7383\n",
      "[6/25][28/110] Loss_D: 0.9106 Loss_G: 2.6282\n",
      "[6/25][29/110] Loss_D: 0.7581 Loss_G: 2.6192\n",
      "[6/25][30/110] Loss_D: 0.7765 Loss_G: 3.8344\n",
      "[6/25][31/110] Loss_D: 1.2547 Loss_G: 2.2629\n",
      "[6/25][32/110] Loss_D: 0.6327 Loss_G: 3.8000\n",
      "[6/25][33/110] Loss_D: 0.5305 Loss_G: 3.6021\n",
      "[6/25][34/110] Loss_D: 0.6516 Loss_G: 2.4801\n",
      "[6/25][35/110] Loss_D: 1.0777 Loss_G: 4.8062\n",
      "[6/25][36/110] Loss_D: 1.3895 Loss_G: 2.3497\n",
      "[6/25][37/110] Loss_D: 1.3157 Loss_G: 5.0044\n",
      "[6/25][38/110] Loss_D: 1.6847 Loss_G: 3.3546\n",
      "[6/25][39/110] Loss_D: 1.7957 Loss_G: 4.0036\n",
      "[6/25][40/110] Loss_D: 1.3627 Loss_G: 8.7560\n",
      "[6/25][41/110] Loss_D: 1.8941 Loss_G: 4.3158\n",
      "[6/25][42/110] Loss_D: 1.0510 Loss_G: 2.9209\n",
      "[6/25][43/110] Loss_D: 1.0268 Loss_G: 6.9533\n",
      "[6/25][44/110] Loss_D: 0.5438 Loss_G: 4.3441\n",
      "[6/25][45/110] Loss_D: 0.7073 Loss_G: 2.4976\n",
      "[6/25][46/110] Loss_D: 1.0770 Loss_G: 2.9742\n",
      "[6/25][47/110] Loss_D: 1.0264 Loss_G: 3.0387\n",
      "[6/25][48/110] Loss_D: 1.0005 Loss_G: 4.2660\n",
      "[6/25][49/110] Loss_D: 1.3591 Loss_G: 2.4904\n",
      "[6/25][50/110] Loss_D: 0.7432 Loss_G: 2.6777\n",
      "[6/25][51/110] Loss_D: 1.2014 Loss_G: 5.7697\n",
      "[6/25][52/110] Loss_D: 2.4053 Loss_G: 3.5310\n",
      "[6/25][53/110] Loss_D: 0.5641 Loss_G: 2.4713\n",
      "[6/25][54/110] Loss_D: 0.7186 Loss_G: 4.0858\n",
      "[6/25][55/110] Loss_D: 1.3198 Loss_G: 2.7911\n",
      "[6/25][56/110] Loss_D: 0.6008 Loss_G: 3.5364\n",
      "[6/25][57/110] Loss_D: 0.8049 Loss_G: 2.2480\n",
      "[6/25][58/110] Loss_D: 1.8187 Loss_G: 5.4541\n",
      "[6/25][59/110] Loss_D: 2.1743 Loss_G: 3.7351\n",
      "[6/25][60/110] Loss_D: 0.6933 Loss_G: 2.5334\n",
      "[6/25][61/110] Loss_D: 1.1233 Loss_G: 3.0006\n",
      "[6/25][62/110] Loss_D: 1.0482 Loss_G: 3.5196\n",
      "[6/25][63/110] Loss_D: 0.7400 Loss_G: 3.1261\n",
      "[6/25][64/110] Loss_D: 0.9020 Loss_G: 3.6473\n",
      "[6/25][65/110] Loss_D: 0.6777 Loss_G: 4.3457\n",
      "[6/25][66/110] Loss_D: 0.5982 Loss_G: 2.7529\n",
      "[6/25][67/110] Loss_D: 0.4952 Loss_G: 4.0596\n",
      "[6/25][68/110] Loss_D: 0.9240 Loss_G: 4.4800\n",
      "[6/25][69/110] Loss_D: 1.1715 Loss_G: 3.2016\n",
      "[6/25][70/110] Loss_D: 1.5732 Loss_G: 5.5850\n",
      "[6/25][71/110] Loss_D: 1.6605 Loss_G: 3.5300\n",
      "[6/25][72/110] Loss_D: 1.2877 Loss_G: 5.5581\n",
      "[6/25][73/110] Loss_D: 1.6305 Loss_G: 2.9531\n",
      "[6/25][74/110] Loss_D: 0.9833 Loss_G: 4.4898\n",
      "[6/25][75/110] Loss_D: 0.6681 Loss_G: 4.0463\n",
      "[6/25][76/110] Loss_D: 0.9016 Loss_G: 2.2778\n",
      "[6/25][77/110] Loss_D: 1.4674 Loss_G: 3.8500\n",
      "[6/25][78/110] Loss_D: 0.6620 Loss_G: 5.0315\n",
      "[6/25][79/110] Loss_D: 1.4536 Loss_G: 2.7652\n",
      "[6/25][80/110] Loss_D: 0.9266 Loss_G: 4.3851\n",
      "[6/25][81/110] Loss_D: 0.7197 Loss_G: 5.3546\n",
      "[6/25][82/110] Loss_D: 0.5909 Loss_G: 4.1242\n",
      "[6/25][83/110] Loss_D: 1.3838 Loss_G: 4.9178\n",
      "[6/25][84/110] Loss_D: 0.6074 Loss_G: 3.8691\n",
      "[6/25][85/110] Loss_D: 0.8639 Loss_G: 8.1805\n",
      "[6/25][86/110] Loss_D: 0.9639 Loss_G: 3.9605\n",
      "[6/25][87/110] Loss_D: 1.7215 Loss_G: 9.6482\n",
      "[6/25][88/110] Loss_D: 2.4103 Loss_G: 4.4666\n",
      "[6/25][89/110] Loss_D: 1.5002 Loss_G: 2.6692\n",
      "[6/25][90/110] Loss_D: 1.1464 Loss_G: 4.3654\n",
      "[6/25][91/110] Loss_D: 0.5207 Loss_G: 3.3668\n",
      "[6/25][92/110] Loss_D: 1.1586 Loss_G: 3.6270\n",
      "[6/25][93/110] Loss_D: 1.1139 Loss_G: 2.9166\n",
      "[6/25][94/110] Loss_D: 1.3847 Loss_G: 4.0096\n",
      "[6/25][95/110] Loss_D: 0.6807 Loss_G: 4.1143\n",
      "[6/25][96/110] Loss_D: 1.7442 Loss_G: 1.7421\n",
      "[6/25][97/110] Loss_D: 1.5381 Loss_G: 5.3916\n",
      "[6/25][98/110] Loss_D: 0.3525 Loss_G: 5.1915\n",
      "[6/25][99/110] Loss_D: 0.8281 Loss_G: 2.1657\n",
      "[6/25][100/110] Loss_D: 3.6322 Loss_G: 9.1227\n",
      "[6/25][101/110] Loss_D: 2.5924 Loss_G: 6.8682\n",
      "[6/25][102/110] Loss_D: 1.5921 Loss_G: 1.6996\n",
      "[6/25][103/110] Loss_D: 1.9595 Loss_G: 3.4439\n",
      "[6/25][104/110] Loss_D: 1.3885 Loss_G: 3.7886\n",
      "[6/25][105/110] Loss_D: 1.4540 Loss_G: 3.1879\n",
      "[6/25][106/110] Loss_D: 1.4320 Loss_G: 1.2751\n",
      "[6/25][107/110] Loss_D: 1.4486 Loss_G: 3.7735\n",
      "[6/25][108/110] Loss_D: 0.8351 Loss_G: 3.6494\n",
      "[6/25][109/110] Loss_D: 1.2013 Loss_G: 3.8222\n",
      "[7/25][0/110] Loss_D: 0.6668 Loss_G: 3.4887\n",
      "[7/25][1/110] Loss_D: 1.3544 Loss_G: 2.2774\n",
      "[7/25][2/110] Loss_D: 1.7951 Loss_G: 2.8345\n",
      "[7/25][3/110] Loss_D: 1.2048 Loss_G: 3.3985\n",
      "[7/25][4/110] Loss_D: 0.8839 Loss_G: 2.5180\n",
      "[7/25][5/110] Loss_D: 1.3577 Loss_G: 3.5612\n",
      "[7/25][6/110] Loss_D: 1.0066 Loss_G: 3.9218\n",
      "[7/25][7/110] Loss_D: 1.0454 Loss_G: 2.0818\n",
      "[7/25][8/110] Loss_D: 1.2681 Loss_G: 2.6083\n",
      "[7/25][9/110] Loss_D: 0.7034 Loss_G: 3.3305\n",
      "[7/25][10/110] Loss_D: 0.7995 Loss_G: 3.1542\n",
      "[7/25][11/110] Loss_D: 0.6530 Loss_G: 2.7082\n",
      "[7/25][12/110] Loss_D: 0.9910 Loss_G: 3.3571\n",
      "[7/25][13/110] Loss_D: 0.9299 Loss_G: 2.7008\n",
      "[7/25][14/110] Loss_D: 0.8592 Loss_G: 3.1278\n",
      "[7/25][15/110] Loss_D: 1.8377 Loss_G: 8.6896\n",
      "[7/25][16/110] Loss_D: 1.7296 Loss_G: 3.7725\n",
      "[7/25][17/110] Loss_D: 1.1857 Loss_G: 2.7691\n",
      "[7/25][18/110] Loss_D: 1.0611 Loss_G: 3.1228\n",
      "[7/25][19/110] Loss_D: 1.1069 Loss_G: 3.7582\n",
      "[7/25][20/110] Loss_D: 1.2291 Loss_G: 2.6764\n",
      "[7/25][21/110] Loss_D: 0.7239 Loss_G: 2.4085\n",
      "[7/25][22/110] Loss_D: 1.0854 Loss_G: 4.1001\n",
      "[7/25][23/110] Loss_D: 0.7277 Loss_G: 4.0452\n",
      "[7/25][24/110] Loss_D: 1.0162 Loss_G: 2.4183\n",
      "[7/25][25/110] Loss_D: 2.0014 Loss_G: 4.4937\n",
      "[7/25][26/110] Loss_D: 1.8591 Loss_G: 2.8041\n",
      "[7/25][27/110] Loss_D: 0.7132 Loss_G: 3.6121\n",
      "[7/25][28/110] Loss_D: 1.1787 Loss_G: 5.2382\n",
      "[7/25][29/110] Loss_D: 0.7846 Loss_G: 3.7127\n",
      "[7/25][30/110] Loss_D: 0.5731 Loss_G: 4.4870\n",
      "[7/25][31/110] Loss_D: 1.0314 Loss_G: 4.8677\n",
      "[7/25][32/110] Loss_D: 0.9581 Loss_G: 3.4060\n",
      "[7/25][33/110] Loss_D: 0.3945 Loss_G: 3.2881\n",
      "[7/25][34/110] Loss_D: 0.5587 Loss_G: 4.1328\n",
      "[7/25][35/110] Loss_D: 0.8105 Loss_G: 3.6932\n",
      "[7/25][36/110] Loss_D: 0.5868 Loss_G: 3.4468\n",
      "[7/25][37/110] Loss_D: 0.8742 Loss_G: 4.2130\n",
      "[7/25][38/110] Loss_D: 0.6296 Loss_G: 5.2123\n",
      "[7/25][39/110] Loss_D: 0.7433 Loss_G: 4.0246\n",
      "[7/25][40/110] Loss_D: 0.6056 Loss_G: 3.0812\n",
      "[7/25][41/110] Loss_D: 0.8521 Loss_G: 6.8923\n",
      "[7/25][42/110] Loss_D: 0.8283 Loss_G: 4.0699\n",
      "[7/25][43/110] Loss_D: 0.9258 Loss_G: 2.5004\n",
      "[7/25][44/110] Loss_D: 1.1543 Loss_G: 4.7004\n",
      "[7/25][45/110] Loss_D: 1.3883 Loss_G: 3.4883\n",
      "[7/25][46/110] Loss_D: 1.8361 Loss_G: 6.7420\n",
      "[7/25][47/110] Loss_D: 0.9524 Loss_G: 4.3810\n",
      "[7/25][48/110] Loss_D: 0.4825 Loss_G: 3.0677\n",
      "[7/25][49/110] Loss_D: 1.8009 Loss_G: 8.0323\n",
      "[7/25][50/110] Loss_D: 0.8775 Loss_G: 5.5440\n",
      "[7/25][51/110] Loss_D: 1.5051 Loss_G: 1.3457\n",
      "[7/25][52/110] Loss_D: 1.2408 Loss_G: 2.3770\n",
      "[7/25][53/110] Loss_D: 1.5419 Loss_G: 6.2684\n",
      "[7/25][54/110] Loss_D: 0.3920 Loss_G: 5.5349\n",
      "[7/25][55/110] Loss_D: 0.6761 Loss_G: 2.9348\n",
      "[7/25][56/110] Loss_D: 1.1039 Loss_G: 3.7419\n",
      "[7/25][57/110] Loss_D: 1.5402 Loss_G: 3.2490\n",
      "[7/25][58/110] Loss_D: 2.6781 Loss_G: 4.2462\n",
      "[7/25][59/110] Loss_D: 1.7683 Loss_G: 2.8745\n",
      "[7/25][60/110] Loss_D: 1.5582 Loss_G: 3.4419\n",
      "[7/25][61/110] Loss_D: 1.0306 Loss_G: 3.2740\n",
      "[7/25][62/110] Loss_D: 0.7149 Loss_G: 4.1654\n",
      "[7/25][63/110] Loss_D: 0.9473 Loss_G: 2.8247\n",
      "[7/25][64/110] Loss_D: 1.1508 Loss_G: 3.5689\n",
      "[7/25][65/110] Loss_D: 1.4007 Loss_G: 8.8579\n",
      "[7/25][66/110] Loss_D: 1.6767 Loss_G: 5.9715\n",
      "[7/25][67/110] Loss_D: 0.7956 Loss_G: 1.6189\n",
      "[7/25][68/110] Loss_D: 1.9647 Loss_G: 5.7218\n",
      "[7/25][69/110] Loss_D: 0.2193 Loss_G: 6.0171\n",
      "[7/25][70/110] Loss_D: 0.6810 Loss_G: 2.7249\n",
      "[7/25][71/110] Loss_D: 1.2737 Loss_G: 4.3423\n",
      "[7/25][72/110] Loss_D: 0.9180 Loss_G: 3.5883\n",
      "[7/25][73/110] Loss_D: 0.6233 Loss_G: 4.7967\n",
      "[7/25][74/110] Loss_D: 0.6239 Loss_G: 3.9190\n",
      "[7/25][75/110] Loss_D: 0.4770 Loss_G: 4.2258\n",
      "[7/25][76/110] Loss_D: 0.9729 Loss_G: 6.0748\n",
      "[7/25][77/110] Loss_D: 0.5038 Loss_G: 4.6240\n",
      "[7/25][78/110] Loss_D: 1.0248 Loss_G: 7.2065\n",
      "[7/25][79/110] Loss_D: 1.1074 Loss_G: 4.8275\n",
      "[7/25][80/110] Loss_D: 0.4139 Loss_G: 3.8719\n",
      "[7/25][81/110] Loss_D: 0.5498 Loss_G: 3.9658\n",
      "[7/25][82/110] Loss_D: 1.0997 Loss_G: 2.7129\n",
      "[7/25][83/110] Loss_D: 1.8999 Loss_G: 6.2678\n",
      "[7/25][84/110] Loss_D: 0.6292 Loss_G: 5.0341\n",
      "[7/25][85/110] Loss_D: 0.6574 Loss_G: 2.2824\n",
      "[7/25][86/110] Loss_D: 1.2923 Loss_G: 3.9187\n",
      "[7/25][87/110] Loss_D: 0.2761 Loss_G: 4.8555\n",
      "[7/25][88/110] Loss_D: 0.9597 Loss_G: 2.0521\n",
      "[7/25][89/110] Loss_D: 0.7362 Loss_G: 4.9704\n",
      "[7/25][90/110] Loss_D: 0.5077 Loss_G: 4.5891\n",
      "[7/25][91/110] Loss_D: 1.0574 Loss_G: 3.3082\n",
      "[7/25][92/110] Loss_D: 1.3654 Loss_G: 2.7911\n",
      "[7/25][93/110] Loss_D: 1.5856 Loss_G: 3.7182\n",
      "[7/25][94/110] Loss_D: 1.1637 Loss_G: 2.6910\n",
      "[7/25][95/110] Loss_D: 1.4986 Loss_G: 1.2387\n",
      "[7/25][96/110] Loss_D: 1.0938 Loss_G: 3.1124\n",
      "[7/25][97/110] Loss_D: 0.6049 Loss_G: 3.4985\n",
      "[7/25][98/110] Loss_D: 0.7159 Loss_G: 2.7153\n",
      "[7/25][99/110] Loss_D: 0.9851 Loss_G: 3.2127\n",
      "[7/25][100/110] Loss_D: 0.8363 Loss_G: 2.7308\n",
      "[7/25][101/110] Loss_D: 0.8649 Loss_G: 2.2613\n",
      "[7/25][102/110] Loss_D: 0.9365 Loss_G: 3.8390\n",
      "[7/25][103/110] Loss_D: 0.9877 Loss_G: 3.2250\n",
      "[7/25][104/110] Loss_D: 0.6645 Loss_G: 5.4087\n",
      "[7/25][105/110] Loss_D: 1.4195 Loss_G: 2.3480\n",
      "[7/25][106/110] Loss_D: 2.5128 Loss_G: 8.0384\n",
      "[7/25][107/110] Loss_D: 2.1297 Loss_G: 1.6067\n",
      "[7/25][108/110] Loss_D: 1.3144 Loss_G: 8.3884\n",
      "[7/25][109/110] Loss_D: 0.1695 Loss_G: 6.3068\n",
      "[8/25][0/110] Loss_D: 1.0473 Loss_G: 2.4727\n",
      "[8/25][1/110] Loss_D: 1.0001 Loss_G: 3.6539\n",
      "[8/25][2/110] Loss_D: 0.8048 Loss_G: 2.5882\n",
      "[8/25][3/110] Loss_D: 0.6862 Loss_G: 4.6496\n",
      "[8/25][4/110] Loss_D: 0.9029 Loss_G: 3.2961\n",
      "[8/25][5/110] Loss_D: 0.9082 Loss_G: 4.5026\n",
      "[8/25][6/110] Loss_D: 0.4186 Loss_G: 3.4362\n",
      "[8/25][7/110] Loss_D: 0.8938 Loss_G: 3.0926\n",
      "[8/25][8/110] Loss_D: 1.1010 Loss_G: 3.4097\n",
      "[8/25][9/110] Loss_D: 1.2378 Loss_G: 3.6035\n",
      "[8/25][10/110] Loss_D: 1.2774 Loss_G: 5.6024\n",
      "[8/25][11/110] Loss_D: 1.0884 Loss_G: 2.8957\n",
      "[8/25][12/110] Loss_D: 0.9696 Loss_G: 3.8641\n",
      "[8/25][13/110] Loss_D: 1.5739 Loss_G: 5.8338\n",
      "[8/25][14/110] Loss_D: 0.8857 Loss_G: 2.4436\n",
      "[8/25][15/110] Loss_D: 1.4525 Loss_G: 4.8366\n",
      "[8/25][16/110] Loss_D: 0.9244 Loss_G: 3.8667\n",
      "[8/25][17/110] Loss_D: 0.8749 Loss_G: 2.5376\n",
      "[8/25][18/110] Loss_D: 2.2360 Loss_G: 8.2610\n",
      "[8/25][19/110] Loss_D: 2.1609 Loss_G: 5.1372\n",
      "[8/25][20/110] Loss_D: 1.2369 Loss_G: 0.6457\n",
      "[8/25][21/110] Loss_D: 4.0195 Loss_G: 4.1044\n",
      "[8/25][22/110] Loss_D: 0.7141 Loss_G: 4.5503\n",
      "[8/25][23/110] Loss_D: 1.6781 Loss_G: 1.6900\n",
      "[8/25][24/110] Loss_D: 1.1890 Loss_G: 2.9043\n",
      "[8/25][25/110] Loss_D: 0.8032 Loss_G: 4.4605\n",
      "[8/25][26/110] Loss_D: 0.4634 Loss_G: 6.3212\n",
      "[8/25][27/110] Loss_D: 1.5247 Loss_G: 3.1723\n",
      "[8/25][28/110] Loss_D: 0.9085 Loss_G: 2.8737\n",
      "[8/25][29/110] Loss_D: 1.4526 Loss_G: 8.0918\n",
      "[8/25][30/110] Loss_D: 2.2460 Loss_G: 3.3691\n",
      "[8/25][31/110] Loss_D: 1.1694 Loss_G: 1.7309\n",
      "[8/25][32/110] Loss_D: 2.0785 Loss_G: 5.9018\n",
      "[8/25][33/110] Loss_D: 2.0390 Loss_G: 3.3512\n",
      "[8/25][34/110] Loss_D: 1.1496 Loss_G: 3.1795\n",
      "[8/25][35/110] Loss_D: 1.3307 Loss_G: 3.1248\n",
      "[8/25][36/110] Loss_D: 0.6317 Loss_G: 3.7616\n",
      "[8/25][37/110] Loss_D: 1.5367 Loss_G: 3.0926\n",
      "[8/25][38/110] Loss_D: 0.9967 Loss_G: 4.9932\n",
      "[8/25][39/110] Loss_D: 0.7419 Loss_G: 3.4668\n",
      "[8/25][40/110] Loss_D: 0.5312 Loss_G: 3.3682\n",
      "[8/25][41/110] Loss_D: 0.9605 Loss_G: 4.8040\n",
      "[8/25][42/110] Loss_D: 1.4037 Loss_G: 2.1878\n",
      "[8/25][43/110] Loss_D: 1.8921 Loss_G: 4.5251\n",
      "[8/25][44/110] Loss_D: 2.1510 Loss_G: 3.0331\n",
      "[8/25][45/110] Loss_D: 1.9669 Loss_G: 5.2404\n",
      "[8/25][46/110] Loss_D: 1.0975 Loss_G: 3.5764\n",
      "[8/25][47/110] Loss_D: 0.6377 Loss_G: 2.1797\n",
      "[8/25][48/110] Loss_D: 1.9973 Loss_G: 6.4856\n",
      "[8/25][49/110] Loss_D: 1.6982 Loss_G: 4.1232\n",
      "[8/25][50/110] Loss_D: 1.1099 Loss_G: 1.4827\n",
      "[8/25][51/110] Loss_D: 2.5710 Loss_G: 5.5358\n",
      "[8/25][52/110] Loss_D: 1.9602 Loss_G: 3.9315\n",
      "[8/25][53/110] Loss_D: 1.0761 Loss_G: 1.5541\n",
      "[8/25][54/110] Loss_D: 1.6145 Loss_G: 3.0248\n",
      "[8/25][55/110] Loss_D: 0.7310 Loss_G: 3.9258\n",
      "[8/25][56/110] Loss_D: 0.8539 Loss_G: 3.0824\n",
      "[8/25][57/110] Loss_D: 1.1229 Loss_G: 3.6318\n",
      "[8/25][58/110] Loss_D: 1.3107 Loss_G: 2.8638\n",
      "[8/25][59/110] Loss_D: 1.2379 Loss_G: 4.2217\n",
      "[8/25][60/110] Loss_D: 0.9441 Loss_G: 3.1853\n",
      "[8/25][61/110] Loss_D: 0.9064 Loss_G: 2.8421\n",
      "[8/25][62/110] Loss_D: 1.5089 Loss_G: 4.8052\n",
      "[8/25][63/110] Loss_D: 0.9301 Loss_G: 4.1322\n",
      "[8/25][64/110] Loss_D: 0.3587 Loss_G: 2.3419\n",
      "[8/25][65/110] Loss_D: 1.3954 Loss_G: 1.6983\n",
      "[8/25][66/110] Loss_D: 2.6434 Loss_G: 4.4839\n",
      "[8/25][67/110] Loss_D: 1.0719 Loss_G: 3.1649\n",
      "[8/25][68/110] Loss_D: 0.8519 Loss_G: 2.0194\n",
      "[8/25][69/110] Loss_D: 1.7813 Loss_G: 4.0880\n",
      "[8/25][70/110] Loss_D: 0.7250 Loss_G: 3.2533\n",
      "[8/25][71/110] Loss_D: 0.4409 Loss_G: 2.5762\n",
      "[8/25][72/110] Loss_D: 1.4244 Loss_G: 4.8711\n",
      "[8/25][73/110] Loss_D: 1.1932 Loss_G: 4.1556\n",
      "[8/25][74/110] Loss_D: 0.7438 Loss_G: 5.5109\n",
      "[8/25][75/110] Loss_D: 0.8704 Loss_G: 4.3474\n",
      "[8/25][76/110] Loss_D: 1.0503 Loss_G: 2.1995\n",
      "[8/25][77/110] Loss_D: 1.4052 Loss_G: 3.8156\n",
      "[8/25][78/110] Loss_D: 1.7895 Loss_G: 5.0193\n",
      "[8/25][79/110] Loss_D: 2.0804 Loss_G: 3.0152\n",
      "[8/25][80/110] Loss_D: 1.4165 Loss_G: 3.1808\n",
      "[8/25][81/110] Loss_D: 0.8284 Loss_G: 3.0681\n",
      "[8/25][82/110] Loss_D: 0.8507 Loss_G: 2.6113\n",
      "[8/25][83/110] Loss_D: 0.9354 Loss_G: 4.1762\n",
      "[8/25][84/110] Loss_D: 1.1188 Loss_G: 2.2536\n",
      "[8/25][85/110] Loss_D: 1.6781 Loss_G: 4.0931\n",
      "[8/25][86/110] Loss_D: 1.5597 Loss_G: 2.2019\n",
      "[8/25][87/110] Loss_D: 1.5089 Loss_G: 5.1843\n",
      "[8/25][88/110] Loss_D: 1.7915 Loss_G: 3.0424\n",
      "[8/25][89/110] Loss_D: 0.9322 Loss_G: 4.1732\n",
      "[8/25][90/110] Loss_D: 0.9289 Loss_G: 4.0145\n",
      "[8/25][91/110] Loss_D: 1.3316 Loss_G: 1.4667\n",
      "[8/25][92/110] Loss_D: 1.4267 Loss_G: 2.9684\n",
      "[8/25][93/110] Loss_D: 1.0207 Loss_G: 5.1239\n",
      "[8/25][94/110] Loss_D: 1.0183 Loss_G: 3.0328\n",
      "[8/25][95/110] Loss_D: 1.4162 Loss_G: 4.0666\n",
      "[8/25][96/110] Loss_D: 1.0718 Loss_G: 3.2680\n",
      "[8/25][97/110] Loss_D: 1.2836 Loss_G: 4.2111\n",
      "[8/25][98/110] Loss_D: 1.3328 Loss_G: 2.0064\n",
      "[8/25][99/110] Loss_D: 1.4428 Loss_G: 2.7746\n",
      "[8/25][100/110] Loss_D: 0.8814 Loss_G: 4.6761\n",
      "[8/25][101/110] Loss_D: 0.9667 Loss_G: 3.7899\n",
      "[8/25][102/110] Loss_D: 1.0814 Loss_G: 2.8790\n",
      "[8/25][103/110] Loss_D: 1.2454 Loss_G: 7.4526\n",
      "[8/25][104/110] Loss_D: 0.9226 Loss_G: 4.1172\n",
      "[8/25][105/110] Loss_D: 0.7891 Loss_G: 2.7496\n",
      "[8/25][106/110] Loss_D: 1.0630 Loss_G: 4.2891\n",
      "[8/25][107/110] Loss_D: 0.5075 Loss_G: 5.1621\n",
      "[8/25][108/110] Loss_D: 1.3317 Loss_G: 2.2029\n",
      "[8/25][109/110] Loss_D: 3.1237 Loss_G: 5.4837\n",
      "[9/25][0/110] Loss_D: 0.3866 Loss_G: 5.8788\n",
      "[9/25][1/110] Loss_D: 0.5759 Loss_G: 3.2496\n",
      "[9/25][2/110] Loss_D: 0.9422 Loss_G: 5.4195\n",
      "[9/25][3/110] Loss_D: 0.4245 Loss_G: 4.9001\n",
      "[9/25][4/110] Loss_D: 1.9135 Loss_G: 1.2704\n",
      "[9/25][5/110] Loss_D: 2.8571 Loss_G: 6.3705\n",
      "[9/25][6/110] Loss_D: 0.7366 Loss_G: 5.9249\n",
      "[9/25][7/110] Loss_D: 1.1919 Loss_G: 1.2514\n",
      "[9/25][8/110] Loss_D: 1.7473 Loss_G: 4.3947\n",
      "[9/25][9/110] Loss_D: 0.4893 Loss_G: 4.2962\n",
      "[9/25][10/110] Loss_D: 0.8480 Loss_G: 2.7627\n",
      "[9/25][11/110] Loss_D: 0.9573 Loss_G: 3.3681\n",
      "[9/25][12/110] Loss_D: 1.1427 Loss_G: 4.8455\n",
      "[9/25][13/110] Loss_D: 0.5868 Loss_G: 3.6657\n",
      "[9/25][14/110] Loss_D: 0.7404 Loss_G: 3.0481\n",
      "[9/25][15/110] Loss_D: 0.8404 Loss_G: 5.5820\n",
      "[9/25][16/110] Loss_D: 2.0454 Loss_G: 2.4958\n",
      "[9/25][17/110] Loss_D: 1.4082 Loss_G: 5.7989\n",
      "[9/25][18/110] Loss_D: 0.4020 Loss_G: 4.5134\n",
      "[9/25][19/110] Loss_D: 1.1241 Loss_G: 1.7413\n",
      "[9/25][20/110] Loss_D: 1.2785 Loss_G: 3.9754\n",
      "[9/25][21/110] Loss_D: 0.7491 Loss_G: 4.9469\n",
      "[9/25][22/110] Loss_D: 0.4915 Loss_G: 3.5229\n",
      "[9/25][23/110] Loss_D: 0.6979 Loss_G: 3.1075\n",
      "[9/25][24/110] Loss_D: 1.1154 Loss_G: 3.1743\n",
      "[9/25][25/110] Loss_D: 1.2989 Loss_G: 5.8090\n",
      "[9/25][26/110] Loss_D: 1.1471 Loss_G: 3.2464\n",
      "[9/25][27/110] Loss_D: 0.9041 Loss_G: 3.8261\n",
      "[9/25][28/110] Loss_D: 1.2654 Loss_G: 5.1115\n",
      "[9/25][29/110] Loss_D: 0.3279 Loss_G: 4.4767\n",
      "[9/25][30/110] Loss_D: 0.6566 Loss_G: 2.7430\n",
      "[9/25][31/110] Loss_D: 1.1964 Loss_G: 3.8261\n",
      "[9/25][32/110] Loss_D: 0.8013 Loss_G: 2.8474\n",
      "[9/25][33/110] Loss_D: 1.7022 Loss_G: 2.8936\n",
      "[9/25][34/110] Loss_D: 0.8265 Loss_G: 4.7451\n",
      "[9/25][35/110] Loss_D: 1.4698 Loss_G: 3.1926\n",
      "[9/25][36/110] Loss_D: 0.9822 Loss_G: 4.0311\n",
      "[9/25][37/110] Loss_D: 0.4027 Loss_G: 3.5864\n",
      "[9/25][38/110] Loss_D: 1.2465 Loss_G: 4.2936\n",
      "[9/25][39/110] Loss_D: 0.7338 Loss_G: 2.0411\n",
      "[9/25][40/110] Loss_D: 1.6336 Loss_G: 7.2013\n",
      "[9/25][41/110] Loss_D: 0.5690 Loss_G: 5.0606\n",
      "[9/25][42/110] Loss_D: 1.0385 Loss_G: 1.4597\n",
      "[9/25][43/110] Loss_D: 0.9282 Loss_G: 4.2469\n",
      "[9/25][44/110] Loss_D: 0.9617 Loss_G: 4.9783\n",
      "[9/25][45/110] Loss_D: 1.5284 Loss_G: 2.5479\n",
      "[9/25][46/110] Loss_D: 0.7799 Loss_G: 4.7365\n",
      "[9/25][47/110] Loss_D: 1.0410 Loss_G: 4.8329\n",
      "[9/25][48/110] Loss_D: 1.4836 Loss_G: 4.1727\n",
      "[9/25][49/110] Loss_D: 1.4974 Loss_G: 5.6840\n",
      "[9/25][50/110] Loss_D: 1.5552 Loss_G: 2.4336\n",
      "[9/25][51/110] Loss_D: 1.2882 Loss_G: 2.7659\n",
      "[9/25][52/110] Loss_D: 1.7295 Loss_G: 7.3057\n",
      "[9/25][53/110] Loss_D: 2.1547 Loss_G: 3.9234\n",
      "[9/25][54/110] Loss_D: 0.7069 Loss_G: 1.8723\n",
      "[9/25][55/110] Loss_D: 0.6629 Loss_G: 2.5322\n",
      "[9/25][56/110] Loss_D: 0.4500 Loss_G: 3.3765\n",
      "[9/25][57/110] Loss_D: 0.1803 Loss_G: 4.0170\n",
      "[9/25][58/110] Loss_D: 0.8598 Loss_G: 2.5767\n",
      "[9/25][59/110] Loss_D: 1.0097 Loss_G: 2.5191\n",
      "[9/25][60/110] Loss_D: 1.9909 Loss_G: 5.4268\n",
      "[9/25][61/110] Loss_D: 1.0633 Loss_G: 3.2024\n",
      "[9/25][62/110] Loss_D: 0.7680 Loss_G: 3.1518\n",
      "[9/25][63/110] Loss_D: 1.4898 Loss_G: 8.1254\n",
      "[9/25][64/110] Loss_D: 2.5677 Loss_G: 4.5384\n",
      "[9/25][65/110] Loss_D: 0.4821 Loss_G: 2.5253\n",
      "[9/25][66/110] Loss_D: 0.5568 Loss_G: 1.7498\n",
      "[9/25][67/110] Loss_D: 1.0758 Loss_G: 2.7408\n",
      "[9/25][68/110] Loss_D: 0.4997 Loss_G: 3.7755\n",
      "[9/25][69/110] Loss_D: 0.5799 Loss_G: 2.8987\n",
      "[9/25][70/110] Loss_D: 1.4642 Loss_G: 2.4568\n",
      "[9/25][71/110] Loss_D: 0.6593 Loss_G: 2.6409\n",
      "[9/25][72/110] Loss_D: 1.4911 Loss_G: 7.7544\n",
      "[9/25][73/110] Loss_D: 2.6795 Loss_G: 5.6370\n",
      "[9/25][74/110] Loss_D: 1.2099 Loss_G: 1.2556\n",
      "[9/25][75/110] Loss_D: 1.4422 Loss_G: 2.9876\n",
      "[9/25][76/110] Loss_D: 0.6504 Loss_G: 5.1740\n",
      "[9/25][77/110] Loss_D: 1.0795 Loss_G: 3.3519\n",
      "[9/25][78/110] Loss_D: 0.9196 Loss_G: 2.4177\n",
      "[9/25][79/110] Loss_D: 0.8701 Loss_G: 3.6616\n",
      "[9/25][80/110] Loss_D: 1.3010 Loss_G: 3.3227\n",
      "[9/25][81/110] Loss_D: 1.6520 Loss_G: 2.9262\n",
      "[9/25][82/110] Loss_D: 2.0303 Loss_G: 7.5124\n",
      "[9/25][83/110] Loss_D: 1.1728 Loss_G: 4.8054\n",
      "[9/25][84/110] Loss_D: 0.5717 Loss_G: 2.1667\n",
      "[9/25][85/110] Loss_D: 2.9298 Loss_G: 6.1301\n",
      "[9/25][86/110] Loss_D: 2.6204 Loss_G: 3.2268\n",
      "[9/25][87/110] Loss_D: 0.9800 Loss_G: 1.6280\n",
      "[9/25][88/110] Loss_D: 2.1224 Loss_G: 4.4850\n",
      "[9/25][89/110] Loss_D: 1.3194 Loss_G: 2.6403\n",
      "[9/25][90/110] Loss_D: 0.8765 Loss_G: 3.9837\n",
      "[9/25][91/110] Loss_D: 1.1807 Loss_G: 4.3743\n",
      "[9/25][92/110] Loss_D: 1.5158 Loss_G: 3.6360\n",
      "[9/25][93/110] Loss_D: 1.3258 Loss_G: 4.4083\n",
      "[9/25][94/110] Loss_D: 1.0508 Loss_G: 1.7749\n",
      "[9/25][95/110] Loss_D: 1.2095 Loss_G: 3.8970\n",
      "[9/25][96/110] Loss_D: 1.6350 Loss_G: 2.1869\n",
      "[9/25][97/110] Loss_D: 0.7336 Loss_G: 3.3138\n",
      "[9/25][98/110] Loss_D: 1.1471 Loss_G: 2.1443\n",
      "[9/25][99/110] Loss_D: 0.9056 Loss_G: 3.2347\n",
      "[9/25][100/110] Loss_D: 0.7730 Loss_G: 3.9505\n",
      "[9/25][101/110] Loss_D: 1.3013 Loss_G: 1.5804\n",
      "[9/25][102/110] Loss_D: 1.4384 Loss_G: 3.9789\n",
      "[9/25][103/110] Loss_D: 1.8973 Loss_G: 1.9530\n",
      "[9/25][104/110] Loss_D: 1.3113 Loss_G: 4.5066\n",
      "[9/25][105/110] Loss_D: 0.5524 Loss_G: 3.7867\n",
      "[9/25][106/110] Loss_D: 1.0020 Loss_G: 1.3852\n",
      "[9/25][107/110] Loss_D: 1.3523 Loss_G: 1.8482\n",
      "[9/25][108/110] Loss_D: 1.3370 Loss_G: 3.3843\n",
      "[9/25][109/110] Loss_D: 0.6391 Loss_G: 3.5512\n",
      "[10/25][0/110] Loss_D: 0.9177 Loss_G: 3.0091\n",
      "[10/25][1/110] Loss_D: 1.2142 Loss_G: 3.2355\n",
      "[10/25][2/110] Loss_D: 0.9459 Loss_G: 5.5640\n",
      "[10/25][3/110] Loss_D: 1.2250 Loss_G: 3.3666\n",
      "[10/25][4/110] Loss_D: 1.1111 Loss_G: 4.8414\n",
      "[10/25][5/110] Loss_D: 0.5751 Loss_G: 5.0299\n",
      "[10/25][6/110] Loss_D: 1.3231 Loss_G: 3.0956\n",
      "[10/25][7/110] Loss_D: 1.4958 Loss_G: 5.4023\n",
      "[10/25][8/110] Loss_D: 1.3288 Loss_G: 4.0143\n",
      "[10/25][9/110] Loss_D: 1.3536 Loss_G: 3.0889\n",
      "[10/25][10/110] Loss_D: 1.0227 Loss_G: 5.6978\n",
      "[10/25][11/110] Loss_D: 1.1458 Loss_G: 3.5832\n",
      "[10/25][12/110] Loss_D: 1.3135 Loss_G: 1.5613\n",
      "[10/25][13/110] Loss_D: 0.9570 Loss_G: 3.0136\n",
      "[10/25][14/110] Loss_D: 0.8890 Loss_G: 2.8616\n",
      "[10/25][15/110] Loss_D: 1.4793 Loss_G: 2.5588\n",
      "[10/25][16/110] Loss_D: 0.7669 Loss_G: 3.4080\n",
      "[10/25][17/110] Loss_D: 1.2945 Loss_G: 1.9478\n",
      "[10/25][18/110] Loss_D: 0.8672 Loss_G: 4.4606\n",
      "[10/25][19/110] Loss_D: 1.0050 Loss_G: 2.5845\n",
      "[10/25][20/110] Loss_D: 1.2587 Loss_G: 4.0634\n",
      "[10/25][21/110] Loss_D: 0.9974 Loss_G: 4.3680\n",
      "[10/25][22/110] Loss_D: 1.4242 Loss_G: 5.9781\n",
      "[10/25][23/110] Loss_D: 1.8388 Loss_G: 1.1591\n",
      "[10/25][24/110] Loss_D: 1.5989 Loss_G: 5.6164\n",
      "[10/25][25/110] Loss_D: 0.8725 Loss_G: 3.7462\n",
      "[10/25][26/110] Loss_D: 0.9534 Loss_G: 2.2517\n",
      "[10/25][27/110] Loss_D: 1.1245 Loss_G: 4.2078\n",
      "[10/25][28/110] Loss_D: 0.6799 Loss_G: 3.7332\n",
      "[10/25][29/110] Loss_D: 0.9943 Loss_G: 2.5482\n",
      "[10/25][30/110] Loss_D: 0.9031 Loss_G: 4.5442\n",
      "[10/25][31/110] Loss_D: 0.8834 Loss_G: 1.9892\n",
      "[10/25][32/110] Loss_D: 1.2505 Loss_G: 3.1853\n",
      "[10/25][33/110] Loss_D: 1.3699 Loss_G: 2.3316\n",
      "[10/25][34/110] Loss_D: 1.6416 Loss_G: 5.9499\n",
      "[10/25][35/110] Loss_D: 1.0852 Loss_G: 3.8274\n",
      "[10/25][36/110] Loss_D: 1.1608 Loss_G: 1.7133\n",
      "[10/25][37/110] Loss_D: 1.9514 Loss_G: 5.9665\n",
      "[10/25][38/110] Loss_D: 1.2651 Loss_G: 2.7929\n",
      "[10/25][39/110] Loss_D: 0.8637 Loss_G: 3.4845\n",
      "[10/25][40/110] Loss_D: 1.0687 Loss_G: 4.8897\n",
      "[10/25][41/110] Loss_D: 0.8970 Loss_G: 3.0943\n",
      "[10/25][42/110] Loss_D: 1.3079 Loss_G: 2.5603\n",
      "[10/25][43/110] Loss_D: 0.8865 Loss_G: 4.7066\n",
      "[10/25][44/110] Loss_D: 1.1063 Loss_G: 3.2233\n",
      "[10/25][45/110] Loss_D: 1.5008 Loss_G: 5.4880\n",
      "[10/25][46/110] Loss_D: 1.2794 Loss_G: 2.2172\n",
      "[10/25][47/110] Loss_D: 1.1098 Loss_G: 4.6285\n",
      "[10/25][48/110] Loss_D: 1.0535 Loss_G: 3.8616\n",
      "[10/25][49/110] Loss_D: 0.5410 Loss_G: 5.2901\n",
      "[10/25][50/110] Loss_D: 0.3240 Loss_G: 4.3206\n",
      "[10/25][51/110] Loss_D: 0.2970 Loss_G: 3.8936\n",
      "[10/25][52/110] Loss_D: 0.9514 Loss_G: 3.8888\n",
      "[10/25][53/110] Loss_D: 0.5194 Loss_G: 3.7467\n",
      "[10/25][54/110] Loss_D: 0.6768 Loss_G: 3.7668\n",
      "[10/25][55/110] Loss_D: 0.4835 Loss_G: 3.2338\n",
      "[10/25][56/110] Loss_D: 0.8737 Loss_G: 3.0428\n",
      "[10/25][57/110] Loss_D: 0.9991 Loss_G: 5.1853\n",
      "[10/25][58/110] Loss_D: 0.7554 Loss_G: 3.1673\n",
      "[10/25][59/110] Loss_D: 0.5815 Loss_G: 5.4464\n",
      "[10/25][60/110] Loss_D: 0.5078 Loss_G: 4.1935\n",
      "[10/25][61/110] Loss_D: 1.1897 Loss_G: 1.7803\n",
      "[10/25][62/110] Loss_D: 1.7109 Loss_G: 6.4958\n",
      "[10/25][63/110] Loss_D: 0.5314 Loss_G: 4.3076\n",
      "[10/25][64/110] Loss_D: 0.6255 Loss_G: 3.5833\n",
      "[10/25][65/110] Loss_D: 0.9944 Loss_G: 5.4921\n",
      "[10/25][66/110] Loss_D: 1.1872 Loss_G: 2.5808\n",
      "[10/25][67/110] Loss_D: 1.1880 Loss_G: 4.4812\n",
      "[10/25][68/110] Loss_D: 0.7969 Loss_G: 6.1584\n",
      "[10/25][69/110] Loss_D: 1.5218 Loss_G: 2.1843\n",
      "[10/25][70/110] Loss_D: 2.7940 Loss_G: 9.3986\n",
      "[10/25][71/110] Loss_D: 3.4921 Loss_G: 3.3875\n",
      "[10/25][72/110] Loss_D: 0.8831 Loss_G: 3.9145\n",
      "[10/25][73/110] Loss_D: 1.2574 Loss_G: 4.5910\n",
      "[10/25][74/110] Loss_D: 1.0837 Loss_G: 3.7827\n",
      "[10/25][75/110] Loss_D: 1.2092 Loss_G: 5.4857\n",
      "[10/25][76/110] Loss_D: 1.3414 Loss_G: 2.1109\n",
      "[10/25][77/110] Loss_D: 1.5464 Loss_G: 5.9014\n",
      "[10/25][78/110] Loss_D: 0.9765 Loss_G: 3.4383\n",
      "[10/25][79/110] Loss_D: 0.5155 Loss_G: 2.8486\n",
      "[10/25][80/110] Loss_D: 1.0935 Loss_G: 4.0768\n",
      "[10/25][81/110] Loss_D: 1.0390 Loss_G: 2.7294\n",
      "[10/25][82/110] Loss_D: 0.8542 Loss_G: 4.4253\n",
      "[10/25][83/110] Loss_D: 0.6292 Loss_G: 3.1183\n",
      "[10/25][84/110] Loss_D: 0.7302 Loss_G: 4.5892\n",
      "[10/25][85/110] Loss_D: 0.7156 Loss_G: 3.0657\n",
      "[10/25][86/110] Loss_D: 0.7660 Loss_G: 2.6787\n",
      "[10/25][87/110] Loss_D: 0.4481 Loss_G: 3.1045\n",
      "[10/25][88/110] Loss_D: 0.9732 Loss_G: 4.6922\n",
      "[10/25][89/110] Loss_D: 0.4896 Loss_G: 3.3694\n",
      "[10/25][90/110] Loss_D: 0.8216 Loss_G: 4.1497\n",
      "[10/25][91/110] Loss_D: 0.5089 Loss_G: 3.3074\n",
      "[10/25][92/110] Loss_D: 1.2381 Loss_G: 4.0438\n",
      "[10/25][93/110] Loss_D: 1.5304 Loss_G: 2.0373\n",
      "[10/25][94/110] Loss_D: 0.9083 Loss_G: 3.0162\n",
      "[10/25][95/110] Loss_D: 0.8101 Loss_G: 4.0862\n",
      "[10/25][96/110] Loss_D: 0.7315 Loss_G: 2.9332\n",
      "[10/25][97/110] Loss_D: 0.7070 Loss_G: 3.0509\n",
      "[10/25][98/110] Loss_D: 1.0049 Loss_G: 4.8561\n",
      "[10/25][99/110] Loss_D: 0.7777 Loss_G: 3.7761\n",
      "[10/25][100/110] Loss_D: 0.8958 Loss_G: 3.3803\n",
      "[10/25][101/110] Loss_D: 0.8903 Loss_G: 5.3224\n",
      "[10/25][102/110] Loss_D: 0.6419 Loss_G: 4.9953\n",
      "[10/25][103/110] Loss_D: 0.5083 Loss_G: 5.4206\n",
      "[10/25][104/110] Loss_D: 1.2614 Loss_G: 3.8120\n",
      "[10/25][105/110] Loss_D: 1.3071 Loss_G: 7.4933\n",
      "[10/25][106/110] Loss_D: 1.7880 Loss_G: 3.3689\n",
      "[10/25][107/110] Loss_D: 0.9823 Loss_G: 4.8668\n",
      "[10/25][108/110] Loss_D: 0.9504 Loss_G: 3.4406\n",
      "[10/25][109/110] Loss_D: 0.8604 Loss_G: 2.9452\n",
      "[11/25][0/110] Loss_D: 0.9490 Loss_G: 3.9148\n",
      "[11/25][1/110] Loss_D: 0.8059 Loss_G: 3.2141\n",
      "[11/25][2/110] Loss_D: 0.5556 Loss_G: 3.1860\n",
      "[11/25][3/110] Loss_D: 1.2549 Loss_G: 3.4464\n",
      "[11/25][4/110] Loss_D: 0.8495 Loss_G: 2.5919\n",
      "[11/25][5/110] Loss_D: 0.9961 Loss_G: 6.3356\n",
      "[11/25][6/110] Loss_D: 2.2077 Loss_G: 1.3451\n",
      "[11/25][7/110] Loss_D: 0.9446 Loss_G: 3.6461\n",
      "[11/25][8/110] Loss_D: 0.9603 Loss_G: 3.7524\n",
      "[11/25][9/110] Loss_D: 0.2525 Loss_G: 4.8950\n",
      "[11/25][10/110] Loss_D: 0.2106 Loss_G: 4.0847\n",
      "[11/25][11/110] Loss_D: 0.4618 Loss_G: 4.2523\n",
      "[11/25][12/110] Loss_D: 0.9626 Loss_G: 4.1682\n",
      "[11/25][13/110] Loss_D: 2.3774 Loss_G: 3.1828\n",
      "[11/25][14/110] Loss_D: 1.4488 Loss_G: 8.6112\n",
      "[11/25][15/110] Loss_D: 0.6514 Loss_G: 4.1235\n",
      "[11/25][16/110] Loss_D: 0.6665 Loss_G: 3.1705\n",
      "[11/25][17/110] Loss_D: 1.5941 Loss_G: 5.5813\n",
      "[11/25][18/110] Loss_D: 0.6991 Loss_G: 4.3567\n",
      "[11/25][19/110] Loss_D: 0.6852 Loss_G: 1.9670\n",
      "[11/25][20/110] Loss_D: 2.0470 Loss_G: 3.6485\n",
      "[11/25][21/110] Loss_D: 1.5445 Loss_G: 2.6106\n",
      "[11/25][22/110] Loss_D: 0.6311 Loss_G: 2.8097\n",
      "[11/25][23/110] Loss_D: 1.1294 Loss_G: 2.1846\n",
      "[11/25][24/110] Loss_D: 0.7387 Loss_G: 3.2760\n",
      "[11/25][25/110] Loss_D: 0.5169 Loss_G: 4.4262\n",
      "[11/25][26/110] Loss_D: 1.4193 Loss_G: 2.3425\n",
      "[11/25][27/110] Loss_D: 0.8294 Loss_G: 4.4602\n",
      "[11/25][28/110] Loss_D: 1.3032 Loss_G: 1.6386\n",
      "[11/25][29/110] Loss_D: 1.4925 Loss_G: 5.5244\n",
      "[11/25][30/110] Loss_D: 1.7034 Loss_G: 1.7236\n",
      "[11/25][31/110] Loss_D: 1.0313 Loss_G: 4.0005\n",
      "[11/25][32/110] Loss_D: 0.8117 Loss_G: 5.2269\n",
      "[11/25][33/110] Loss_D: 0.5587 Loss_G: 3.5518\n",
      "[11/25][34/110] Loss_D: 0.8187 Loss_G: 3.5745\n",
      "[11/25][35/110] Loss_D: 1.2737 Loss_G: 5.2952\n",
      "[11/25][36/110] Loss_D: 1.0786 Loss_G: 3.6014\n",
      "[11/25][37/110] Loss_D: 0.3247 Loss_G: 3.2166\n",
      "[11/25][38/110] Loss_D: 1.4435 Loss_G: 5.3489\n",
      "[11/25][39/110] Loss_D: 1.1871 Loss_G: 2.8461\n",
      "[11/25][40/110] Loss_D: 0.9076 Loss_G: 4.1814\n",
      "[11/25][41/110] Loss_D: 0.9142 Loss_G: 2.9521\n",
      "[11/25][42/110] Loss_D: 1.0066 Loss_G: 6.1420\n",
      "[11/25][43/110] Loss_D: 1.0457 Loss_G: 3.0728\n",
      "[11/25][44/110] Loss_D: 0.6443 Loss_G: 2.7358\n",
      "[11/25][45/110] Loss_D: 1.5919 Loss_G: 5.9351\n",
      "[11/25][46/110] Loss_D: 2.4277 Loss_G: 1.8472\n",
      "[11/25][47/110] Loss_D: 1.5335 Loss_G: 5.6920\n",
      "[11/25][48/110] Loss_D: 0.2449 Loss_G: 5.4183\n",
      "[11/25][49/110] Loss_D: 1.7021 Loss_G: 1.8003\n",
      "[11/25][50/110] Loss_D: 2.8939 Loss_G: 6.7754\n",
      "[11/25][51/110] Loss_D: 0.7379 Loss_G: 4.2640\n",
      "[11/25][52/110] Loss_D: 0.8090 Loss_G: 3.0737\n",
      "[11/25][53/110] Loss_D: 1.7184 Loss_G: 4.1868\n",
      "[11/25][54/110] Loss_D: 1.2352 Loss_G: 7.3604\n",
      "[11/25][55/110] Loss_D: 1.6346 Loss_G: 3.2026\n",
      "[11/25][56/110] Loss_D: 0.6122 Loss_G: 2.8944\n",
      "[11/25][57/110] Loss_D: 0.8739 Loss_G: 5.1381\n",
      "[11/25][58/110] Loss_D: 1.4184 Loss_G: 3.5527\n",
      "[11/25][59/110] Loss_D: 1.0441 Loss_G: 4.7504\n",
      "[11/25][60/110] Loss_D: 1.4740 Loss_G: 5.2356\n",
      "[11/25][61/110] Loss_D: 1.8883 Loss_G: 2.0128\n",
      "[11/25][62/110] Loss_D: 1.2720 Loss_G: 4.5608\n",
      "[11/25][63/110] Loss_D: 0.4209 Loss_G: 4.3729\n",
      "[11/25][64/110] Loss_D: 0.5629 Loss_G: 3.1308\n",
      "[11/25][65/110] Loss_D: 0.7796 Loss_G: 3.9209\n",
      "[11/25][66/110] Loss_D: 0.7274 Loss_G: 4.0938\n",
      "[11/25][67/110] Loss_D: 0.5599 Loss_G: 4.3630\n",
      "[11/25][68/110] Loss_D: 0.5590 Loss_G: 3.3638\n",
      "[11/25][69/110] Loss_D: 0.9903 Loss_G: 3.0594\n",
      "[11/25][70/110] Loss_D: 2.3394 Loss_G: 7.9325\n",
      "[11/25][71/110] Loss_D: 1.9955 Loss_G: 5.0882\n",
      "[11/25][72/110] Loss_D: 0.4257 Loss_G: 4.0008\n",
      "[11/25][73/110] Loss_D: 0.8951 Loss_G: 4.9786\n",
      "[11/25][74/110] Loss_D: 1.3481 Loss_G: 4.9328\n",
      "[11/25][75/110] Loss_D: 0.8827 Loss_G: 6.1134\n",
      "[11/25][76/110] Loss_D: 0.8598 Loss_G: 2.5509\n",
      "[11/25][77/110] Loss_D: 1.0494 Loss_G: 4.4866\n",
      "[11/25][78/110] Loss_D: 1.8736 Loss_G: 3.0197\n",
      "[11/25][79/110] Loss_D: 1.2390 Loss_G: 3.3796\n",
      "[11/25][80/110] Loss_D: 0.7822 Loss_G: 5.0472\n",
      "[11/25][81/110] Loss_D: 1.2591 Loss_G: 1.6788\n",
      "[11/25][82/110] Loss_D: 1.5099 Loss_G: 4.5963\n",
      "[11/25][83/110] Loss_D: 1.0703 Loss_G: 2.5880\n",
      "[11/25][84/110] Loss_D: 1.0348 Loss_G: 3.9970\n",
      "[11/25][85/110] Loss_D: 1.2999 Loss_G: 2.9482\n",
      "[11/25][86/110] Loss_D: 0.4125 Loss_G: 3.2778\n",
      "[11/25][87/110] Loss_D: 0.4322 Loss_G: 3.4467\n",
      "[11/25][88/110] Loss_D: 1.0759 Loss_G: 4.3955\n",
      "[11/25][89/110] Loss_D: 0.3775 Loss_G: 4.4110\n",
      "[11/25][90/110] Loss_D: 0.8852 Loss_G: 2.6598\n",
      "[11/25][91/110] Loss_D: 1.9287 Loss_G: 7.5230\n",
      "[11/25][92/110] Loss_D: 1.9054 Loss_G: 5.1570\n",
      "[11/25][93/110] Loss_D: 0.9119 Loss_G: 2.7790\n",
      "[11/25][94/110] Loss_D: 1.4296 Loss_G: 3.9384\n",
      "[11/25][95/110] Loss_D: 0.3675 Loss_G: 4.9108\n",
      "[11/25][96/110] Loss_D: 0.8300 Loss_G: 2.7402\n",
      "[11/25][97/110] Loss_D: 2.1556 Loss_G: 2.4225\n",
      "[11/25][98/110] Loss_D: 1.7655 Loss_G: 3.3865\n",
      "[11/25][99/110] Loss_D: 1.1813 Loss_G: 2.4431\n",
      "[11/25][100/110] Loss_D: 0.9732 Loss_G: 3.4251\n",
      "[11/25][101/110] Loss_D: 0.6260 Loss_G: 5.0397\n",
      "[11/25][102/110] Loss_D: 0.6042 Loss_G: 3.4655\n",
      "[11/25][103/110] Loss_D: 0.7633 Loss_G: 4.9077\n",
      "[11/25][104/110] Loss_D: 0.7124 Loss_G: 6.3730\n",
      "[11/25][105/110] Loss_D: 1.2675 Loss_G: 2.0950\n",
      "[11/25][106/110] Loss_D: 2.0325 Loss_G: 4.6496\n",
      "[11/25][107/110] Loss_D: 1.5406 Loss_G: 2.6979\n",
      "[11/25][108/110] Loss_D: 0.9153 Loss_G: 5.1213\n",
      "[11/25][109/110] Loss_D: 0.3918 Loss_G: 6.8369\n",
      "[12/25][0/110] Loss_D: 1.4769 Loss_G: 2.8827\n",
      "[12/25][1/110] Loss_D: 1.0245 Loss_G: 7.3949\n",
      "[12/25][2/110] Loss_D: 1.6150 Loss_G: 2.7112\n",
      "[12/25][3/110] Loss_D: 1.5472 Loss_G: 5.2028\n",
      "[12/25][4/110] Loss_D: 1.0315 Loss_G: 4.5678\n",
      "[12/25][5/110] Loss_D: 1.6143 Loss_G: 2.3103\n",
      "[12/25][6/110] Loss_D: 1.2918 Loss_G: 5.5004\n",
      "[12/25][7/110] Loss_D: 0.4392 Loss_G: 5.3025\n",
      "[12/25][8/110] Loss_D: 0.6990 Loss_G: 2.9497\n",
      "[12/25][9/110] Loss_D: 0.2621 Loss_G: 2.8395\n",
      "[12/25][10/110] Loss_D: 0.8421 Loss_G: 3.2382\n",
      "[12/25][11/110] Loss_D: 1.4777 Loss_G: 6.0784\n",
      "[12/25][12/110] Loss_D: 1.0672 Loss_G: 3.1070\n",
      "[12/25][13/110] Loss_D: 1.0028 Loss_G: 4.6501\n",
      "[12/25][14/110] Loss_D: 0.6962 Loss_G: 4.0384\n",
      "[12/25][15/110] Loss_D: 0.7611 Loss_G: 3.9270\n",
      "[12/25][16/110] Loss_D: 0.5469 Loss_G: 3.6389\n",
      "[12/25][17/110] Loss_D: 0.7680 Loss_G: 3.0269\n",
      "[12/25][18/110] Loss_D: 0.8108 Loss_G: 3.3219\n",
      "[12/25][19/110] Loss_D: 1.2148 Loss_G: 4.9786\n",
      "[12/25][20/110] Loss_D: 1.9365 Loss_G: 1.3413\n",
      "[12/25][21/110] Loss_D: 1.2505 Loss_G: 4.2387\n",
      "[12/25][22/110] Loss_D: 0.4562 Loss_G: 4.3346\n",
      "[12/25][23/110] Loss_D: 0.1843 Loss_G: 3.8656\n",
      "[12/25][24/110] Loss_D: 1.0234 Loss_G: 1.4503\n",
      "[12/25][25/110] Loss_D: 1.6412 Loss_G: 5.9210\n",
      "[12/25][26/110] Loss_D: 1.7308 Loss_G: 3.4895\n",
      "[12/25][27/110] Loss_D: 0.7638 Loss_G: 2.3897\n",
      "[12/25][28/110] Loss_D: 0.8276 Loss_G: 5.0093\n",
      "[12/25][29/110] Loss_D: 0.8803 Loss_G: 3.1842\n",
      "[12/25][30/110] Loss_D: 0.3660 Loss_G: 3.0515\n",
      "[12/25][31/110] Loss_D: 0.5871 Loss_G: 3.6075\n",
      "[12/25][32/110] Loss_D: 1.3560 Loss_G: 1.8483\n",
      "[12/25][33/110] Loss_D: 1.3903 Loss_G: 5.6375\n",
      "[12/25][34/110] Loss_D: 1.0286 Loss_G: 3.6324\n",
      "[12/25][35/110] Loss_D: 0.4934 Loss_G: 2.9683\n",
      "[12/25][36/110] Loss_D: 1.2122 Loss_G: 1.9115\n",
      "[12/25][37/110] Loss_D: 1.3610 Loss_G: 6.0124\n",
      "[12/25][38/110] Loss_D: 0.9739 Loss_G: 3.2636\n",
      "[12/25][39/110] Loss_D: 0.4877 Loss_G: 2.0448\n",
      "[12/25][40/110] Loss_D: 1.0035 Loss_G: 2.7698\n",
      "[12/25][41/110] Loss_D: 1.1834 Loss_G: 4.7980\n",
      "[12/25][42/110] Loss_D: 0.2548 Loss_G: 3.9638\n",
      "[12/25][43/110] Loss_D: 0.4340 Loss_G: 3.1099\n",
      "[12/25][44/110] Loss_D: 0.8677 Loss_G: 3.2968\n",
      "[12/25][45/110] Loss_D: 0.5478 Loss_G: 3.5216\n",
      "[12/25][46/110] Loss_D: 1.0835 Loss_G: 4.2808\n",
      "[12/25][47/110] Loss_D: 1.2641 Loss_G: 2.2555\n",
      "[12/25][48/110] Loss_D: 0.5063 Loss_G: 4.4031\n",
      "[12/25][49/110] Loss_D: 0.7978 Loss_G: 4.6693\n",
      "[12/25][50/110] Loss_D: 0.7983 Loss_G: 2.4100\n",
      "[12/25][51/110] Loss_D: 1.2395 Loss_G: 6.5457\n",
      "[12/25][52/110] Loss_D: 2.1691 Loss_G: 2.1464\n",
      "[12/25][53/110] Loss_D: 1.1168 Loss_G: 3.3283\n",
      "[12/25][54/110] Loss_D: 1.2465 Loss_G: 6.6479\n",
      "[12/25][55/110] Loss_D: 1.1707 Loss_G: 4.1313\n",
      "[12/25][56/110] Loss_D: 0.6048 Loss_G: 2.3669\n",
      "[12/25][57/110] Loss_D: 1.2392 Loss_G: 6.0514\n",
      "[12/25][58/110] Loss_D: 0.4774 Loss_G: 5.3034\n",
      "[12/25][59/110] Loss_D: 0.8765 Loss_G: 1.4501\n",
      "[12/25][60/110] Loss_D: 1.4153 Loss_G: 4.5407\n",
      "[12/25][61/110] Loss_D: 0.5243 Loss_G: 6.0387\n",
      "[12/25][62/110] Loss_D: 0.7401 Loss_G: 3.6680\n",
      "[12/25][63/110] Loss_D: 1.1341 Loss_G: 5.0189\n",
      "[12/25][64/110] Loss_D: 0.3503 Loss_G: 4.7768\n",
      "[12/25][65/110] Loss_D: 1.4833 Loss_G: 1.3740\n",
      "[12/25][66/110] Loss_D: 1.5935 Loss_G: 6.4464\n",
      "[12/25][67/110] Loss_D: 0.2865 Loss_G: 6.0148\n",
      "[12/25][68/110] Loss_D: 0.8956 Loss_G: 2.3916\n",
      "[12/25][69/110] Loss_D: 1.5034 Loss_G: 4.6721\n",
      "[12/25][70/110] Loss_D: 1.1429 Loss_G: 4.3896\n",
      "[12/25][71/110] Loss_D: 1.4844 Loss_G: 1.6127\n",
      "[12/25][72/110] Loss_D: 1.7196 Loss_G: 4.4593\n",
      "[12/25][73/110] Loss_D: 1.0308 Loss_G: 5.8568\n",
      "[12/25][74/110] Loss_D: 0.7174 Loss_G: 6.2208\n",
      "[12/25][75/110] Loss_D: 0.9645 Loss_G: 3.6403\n",
      "[12/25][76/110] Loss_D: 1.5555 Loss_G: 4.1399\n",
      "[12/25][77/110] Loss_D: 1.1703 Loss_G: 5.7966\n",
      "[12/25][78/110] Loss_D: 1.7867 Loss_G: 3.3906\n",
      "[12/25][79/110] Loss_D: 1.6114 Loss_G: 5.5517\n",
      "[12/25][80/110] Loss_D: 0.7355 Loss_G: 5.0086\n",
      "[12/25][81/110] Loss_D: 1.1419 Loss_G: 4.2503\n",
      "[12/25][82/110] Loss_D: 0.4887 Loss_G: 4.7801\n",
      "[12/25][83/110] Loss_D: 1.6313 Loss_G: 1.5922\n",
      "[12/25][84/110] Loss_D: 1.6473 Loss_G: 4.2169\n",
      "[12/25][85/110] Loss_D: 0.6139 Loss_G: 4.5042\n",
      "[12/25][86/110] Loss_D: 0.3753 Loss_G: 2.4981\n",
      "[12/25][87/110] Loss_D: 0.4179 Loss_G: 2.6060\n",
      "[12/25][88/110] Loss_D: 0.7996 Loss_G: 3.3798\n",
      "[12/25][89/110] Loss_D: 0.4892 Loss_G: 4.1163\n",
      "[12/25][90/110] Loss_D: 0.8863 Loss_G: 1.9859\n",
      "[12/25][91/110] Loss_D: 0.8637 Loss_G: 3.1737\n",
      "[12/25][92/110] Loss_D: 0.6462 Loss_G: 3.8616\n",
      "[12/25][93/110] Loss_D: 0.9128 Loss_G: 2.7794\n",
      "[12/25][94/110] Loss_D: 1.4119 Loss_G: 2.6518\n",
      "[12/25][95/110] Loss_D: 1.0029 Loss_G: 2.3138\n",
      "[12/25][96/110] Loss_D: 0.6025 Loss_G: 2.5441\n",
      "[12/25][97/110] Loss_D: 0.8077 Loss_G: 4.5416\n",
      "[12/25][98/110] Loss_D: 0.4611 Loss_G: 3.8842\n",
      "[12/25][99/110] Loss_D: 0.6341 Loss_G: 2.4974\n",
      "[12/25][100/110] Loss_D: 0.9402 Loss_G: 4.4694\n",
      "[12/25][101/110] Loss_D: 0.6125 Loss_G: 4.2201\n",
      "[12/25][102/110] Loss_D: 0.7799 Loss_G: 4.9236\n",
      "[12/25][103/110] Loss_D: 0.8424 Loss_G: 2.1122\n",
      "[12/25][104/110] Loss_D: 2.2728 Loss_G: 4.5303\n",
      "[12/25][105/110] Loss_D: 1.4502 Loss_G: 1.5051\n",
      "[12/25][106/110] Loss_D: 1.4010 Loss_G: 5.1273\n",
      "[12/25][107/110] Loss_D: 0.8500 Loss_G: 3.3107\n",
      "[12/25][108/110] Loss_D: 1.0946 Loss_G: 1.9079\n",
      "[12/25][109/110] Loss_D: 1.8928 Loss_G: 9.7644\n",
      "[13/25][0/110] Loss_D: 1.5481 Loss_G: 4.5726\n",
      "[13/25][1/110] Loss_D: 0.4601 Loss_G: 1.7358\n",
      "[13/25][2/110] Loss_D: 1.0178 Loss_G: 4.2865\n",
      "[13/25][3/110] Loss_D: 0.3589 Loss_G: 4.9205\n",
      "[13/25][4/110] Loss_D: 1.8478 Loss_G: 1.5381\n",
      "[13/25][5/110] Loss_D: 1.6777 Loss_G: 6.7776\n",
      "[13/25][6/110] Loss_D: 1.0497 Loss_G: 3.8599\n",
      "[13/25][7/110] Loss_D: 0.5499 Loss_G: 1.8040\n",
      "[13/25][8/110] Loss_D: 1.2505 Loss_G: 3.2262\n",
      "[13/25][9/110] Loss_D: 1.1921 Loss_G: 2.3823\n",
      "[13/25][10/110] Loss_D: 1.0045 Loss_G: 4.6449\n",
      "[13/25][11/110] Loss_D: 1.0915 Loss_G: 3.3088\n",
      "[13/25][12/110] Loss_D: 0.8199 Loss_G: 1.8083\n",
      "[13/25][13/110] Loss_D: 2.9312 Loss_G: 5.7613\n",
      "[13/25][14/110] Loss_D: 0.5947 Loss_G: 5.2195\n",
      "[13/25][15/110] Loss_D: 1.5862 Loss_G: 1.8812\n",
      "[13/25][16/110] Loss_D: 1.1833 Loss_G: 5.4701\n",
      "[13/25][17/110] Loss_D: 0.3477 Loss_G: 4.5399\n",
      "[13/25][18/110] Loss_D: 0.6288 Loss_G: 3.2641\n",
      "[13/25][19/110] Loss_D: 1.0715 Loss_G: 3.6695\n",
      "[13/25][20/110] Loss_D: 1.1414 Loss_G: 2.9944\n",
      "[13/25][21/110] Loss_D: 1.0340 Loss_G: 4.1806\n",
      "[13/25][22/110] Loss_D: 0.8030 Loss_G: 3.0397\n",
      "[13/25][23/110] Loss_D: 0.7540 Loss_G: 2.7628\n",
      "[13/25][24/110] Loss_D: 1.3430 Loss_G: 2.5204\n",
      "[13/25][25/110] Loss_D: 1.2616 Loss_G: 2.5383\n",
      "[13/25][26/110] Loss_D: 1.5559 Loss_G: 6.9036\n",
      "[13/25][27/110] Loss_D: 0.4914 Loss_G: 4.4687\n",
      "[13/25][28/110] Loss_D: 0.2504 Loss_G: 3.2190\n",
      "[13/25][29/110] Loss_D: 0.9096 Loss_G: 2.0944\n",
      "[13/25][30/110] Loss_D: 1.0331 Loss_G: 3.8819\n",
      "[13/25][31/110] Loss_D: 0.6224 Loss_G: 4.2044\n",
      "[13/25][32/110] Loss_D: 2.7101 Loss_G: 0.8839\n",
      "[13/25][33/110] Loss_D: 1.6817 Loss_G: 5.2739\n",
      "[13/25][34/110] Loss_D: 0.9129 Loss_G: 3.2501\n",
      "[13/25][35/110] Loss_D: 0.6130 Loss_G: 2.2891\n",
      "[13/25][36/110] Loss_D: 1.4581 Loss_G: 3.0027\n",
      "[13/25][37/110] Loss_D: 1.1800 Loss_G: 3.1370\n",
      "[13/25][38/110] Loss_D: 0.5444 Loss_G: 3.3461\n",
      "[13/25][39/110] Loss_D: 0.7726 Loss_G: 6.5682\n",
      "[13/25][40/110] Loss_D: 0.8738 Loss_G: 3.1415\n",
      "[13/25][41/110] Loss_D: 1.2490 Loss_G: 2.0313\n",
      "[13/25][42/110] Loss_D: 1.5492 Loss_G: 6.4980\n",
      "[13/25][43/110] Loss_D: 1.2143 Loss_G: 4.4915\n",
      "[13/25][44/110] Loss_D: 0.5662 Loss_G: 2.3405\n",
      "[13/25][45/110] Loss_D: 1.4371 Loss_G: 5.8472\n",
      "[13/25][46/110] Loss_D: 1.1236 Loss_G: 3.0451\n",
      "[13/25][47/110] Loss_D: 0.8036 Loss_G: 3.3566\n",
      "[13/25][48/110] Loss_D: 0.9918 Loss_G: 6.2634\n",
      "[13/25][49/110] Loss_D: 0.9405 Loss_G: 3.3090\n",
      "[13/25][50/110] Loss_D: 0.7508 Loss_G: 1.3769\n",
      "[13/25][51/110] Loss_D: 2.7744 Loss_G: 8.3214\n",
      "[13/25][52/110] Loss_D: 1.1413 Loss_G: 5.2572\n",
      "[13/25][53/110] Loss_D: 1.2233 Loss_G: 0.6325\n",
      "[13/25][54/110] Loss_D: 1.4092 Loss_G: 3.2777\n",
      "[13/25][55/110] Loss_D: 0.9167 Loss_G: 4.3599\n",
      "[13/25][56/110] Loss_D: 0.7342 Loss_G: 2.6275\n",
      "[13/25][57/110] Loss_D: 0.6244 Loss_G: 2.4531\n",
      "[13/25][58/110] Loss_D: 0.6899 Loss_G: 3.7876\n",
      "[13/25][59/110] Loss_D: 0.8736 Loss_G: 2.9762\n",
      "[13/25][60/110] Loss_D: 1.1073 Loss_G: 3.6995\n",
      "[13/25][61/110] Loss_D: 0.6033 Loss_G: 5.0122\n",
      "[13/25][62/110] Loss_D: 0.8798 Loss_G: 2.9027\n",
      "[13/25][63/110] Loss_D: 0.9566 Loss_G: 3.0576\n",
      "[13/25][64/110] Loss_D: 0.5854 Loss_G: 4.5596\n",
      "[13/25][65/110] Loss_D: 1.1169 Loss_G: 4.3802\n",
      "[13/25][66/110] Loss_D: 0.9730 Loss_G: 3.2438\n",
      "[13/25][67/110] Loss_D: 2.2831 Loss_G: 7.7847\n",
      "[13/25][68/110] Loss_D: 0.6642 Loss_G: 5.6278\n",
      "[13/25][69/110] Loss_D: 0.7479 Loss_G: 2.2190\n",
      "[13/25][70/110] Loss_D: 2.0005 Loss_G: 7.7234\n",
      "[13/25][71/110] Loss_D: 0.4846 Loss_G: 7.5345\n",
      "[13/25][72/110] Loss_D: 0.7365 Loss_G: 2.5440\n",
      "[13/25][73/110] Loss_D: 0.7300 Loss_G: 3.9415\n",
      "[13/25][74/110] Loss_D: 1.9126 Loss_G: 2.8426\n",
      "[13/25][75/110] Loss_D: 0.7510 Loss_G: 3.9899\n",
      "[13/25][76/110] Loss_D: 0.5433 Loss_G: 3.8244\n",
      "[13/25][77/110] Loss_D: 0.8527 Loss_G: 2.4055\n",
      "[13/25][78/110] Loss_D: 0.6832 Loss_G: 4.5975\n",
      "[13/25][79/110] Loss_D: 0.7768 Loss_G: 4.1448\n",
      "[13/25][80/110] Loss_D: 0.6401 Loss_G: 2.4746\n",
      "[13/25][81/110] Loss_D: 0.8954 Loss_G: 5.7998\n",
      "[13/25][82/110] Loss_D: 0.5485 Loss_G: 4.0925\n",
      "[13/25][83/110] Loss_D: 1.0090 Loss_G: 2.2040\n",
      "[13/25][84/110] Loss_D: 0.8361 Loss_G: 3.9186\n",
      "[13/25][85/110] Loss_D: 1.8372 Loss_G: 9.3603\n",
      "[13/25][86/110] Loss_D: 1.8895 Loss_G: 5.0226\n",
      "[13/25][87/110] Loss_D: 0.7863 Loss_G: 1.2245\n",
      "[13/25][88/110] Loss_D: 4.6740 Loss_G: 6.7655\n",
      "[13/25][89/110] Loss_D: 1.6228 Loss_G: 4.0095\n",
      "[13/25][90/110] Loss_D: 1.1309 Loss_G: 2.5869\n",
      "[13/25][91/110] Loss_D: 0.5900 Loss_G: 3.0727\n",
      "[13/25][92/110] Loss_D: 1.1257 Loss_G: 2.9067\n",
      "[13/25][93/110] Loss_D: 0.6941 Loss_G: 5.9111\n",
      "[13/25][94/110] Loss_D: 0.5218 Loss_G: 5.0029\n",
      "[13/25][95/110] Loss_D: 0.8506 Loss_G: 4.2593\n",
      "[13/25][96/110] Loss_D: 1.6352 Loss_G: 6.0030\n",
      "[13/25][97/110] Loss_D: 1.3203 Loss_G: 3.5425\n",
      "[13/25][98/110] Loss_D: 0.9537 Loss_G: 4.0214\n",
      "[13/25][99/110] Loss_D: 0.4017 Loss_G: 3.9541\n",
      "[13/25][100/110] Loss_D: 0.4605 Loss_G: 4.3893\n",
      "[13/25][101/110] Loss_D: 0.5390 Loss_G: 4.1154\n",
      "[13/25][102/110] Loss_D: 0.6476 Loss_G: 4.0724\n",
      "[13/25][103/110] Loss_D: 0.2931 Loss_G: 3.9335\n",
      "[13/25][104/110] Loss_D: 0.6454 Loss_G: 2.8475\n",
      "[13/25][105/110] Loss_D: 1.6753 Loss_G: 6.4367\n",
      "[13/25][106/110] Loss_D: 2.5260 Loss_G: 2.5189\n",
      "[13/25][107/110] Loss_D: 0.9225 Loss_G: 4.7654\n",
      "[13/25][108/110] Loss_D: 0.7661 Loss_G: 4.4061\n",
      "[13/25][109/110] Loss_D: 0.8235 Loss_G: 3.0561\n",
      "[14/25][0/110] Loss_D: 0.4150 Loss_G: 4.2912\n",
      "[14/25][1/110] Loss_D: 0.2903 Loss_G: 4.4046\n",
      "[14/25][2/110] Loss_D: 0.6536 Loss_G: 2.8631\n",
      "[14/25][3/110] Loss_D: 0.5588 Loss_G: 4.7403\n",
      "[14/25][4/110] Loss_D: 0.8173 Loss_G: 2.5354\n",
      "[14/25][5/110] Loss_D: 0.7412 Loss_G: 3.0088\n",
      "[14/25][6/110] Loss_D: 0.5040 Loss_G: 3.9313\n",
      "[14/25][7/110] Loss_D: 0.7269 Loss_G: 5.2930\n",
      "[14/25][8/110] Loss_D: 0.7226 Loss_G: 3.0208\n",
      "[14/25][9/110] Loss_D: 1.4892 Loss_G: 4.0270\n",
      "[14/25][10/110] Loss_D: 0.7650 Loss_G: 3.4201\n",
      "[14/25][11/110] Loss_D: 0.7328 Loss_G: 3.2342\n",
      "[14/25][12/110] Loss_D: 1.0278 Loss_G: 9.1107\n",
      "[14/25][13/110] Loss_D: 1.0389 Loss_G: 6.8724\n",
      "[14/25][14/110] Loss_D: 0.3942 Loss_G: 2.5120\n",
      "[14/25][15/110] Loss_D: 2.1130 Loss_G: 8.9274\n",
      "[14/25][16/110] Loss_D: 0.3762 Loss_G: 6.9222\n",
      "[14/25][17/110] Loss_D: 0.6591 Loss_G: 2.0109\n",
      "[14/25][18/110] Loss_D: 1.3981 Loss_G: 5.3217\n",
      "[14/25][19/110] Loss_D: 0.7225 Loss_G: 3.8588\n",
      "[14/25][20/110] Loss_D: 0.9984 Loss_G: 1.9932\n",
      "[14/25][21/110] Loss_D: 1.2993 Loss_G: 5.3603\n",
      "[14/25][22/110] Loss_D: 0.3566 Loss_G: 4.3385\n",
      "[14/25][23/110] Loss_D: 0.2920 Loss_G: 3.3959\n",
      "[14/25][24/110] Loss_D: 0.6174 Loss_G: 3.9954\n",
      "[14/25][25/110] Loss_D: 0.9840 Loss_G: 2.7313\n",
      "[14/25][26/110] Loss_D: 1.0972 Loss_G: 6.2875\n",
      "[14/25][27/110] Loss_D: 0.3530 Loss_G: 4.9963\n",
      "[14/25][28/110] Loss_D: 0.7960 Loss_G: 1.9858\n",
      "[14/25][29/110] Loss_D: 1.8615 Loss_G: 8.2388\n",
      "[14/25][30/110] Loss_D: 1.2852 Loss_G: 4.3446\n",
      "[14/25][31/110] Loss_D: 1.0715 Loss_G: 2.8559\n",
      "[14/25][32/110] Loss_D: 0.8149 Loss_G: 4.1991\n",
      "[14/25][33/110] Loss_D: 0.5872 Loss_G: 3.7116\n",
      "[14/25][34/110] Loss_D: 1.3841 Loss_G: 5.1347\n",
      "[14/25][35/110] Loss_D: 1.0804 Loss_G: 4.4573\n",
      "[14/25][36/110] Loss_D: 1.0312 Loss_G: 4.9541\n",
      "[14/25][37/110] Loss_D: 0.7592 Loss_G: 3.9721\n",
      "[14/25][38/110] Loss_D: 0.4352 Loss_G: 5.0362\n",
      "[14/25][39/110] Loss_D: 0.3828 Loss_G: 4.8022\n",
      "[14/25][40/110] Loss_D: 0.5595 Loss_G: 4.0285\n",
      "[14/25][41/110] Loss_D: 0.5512 Loss_G: 4.3715\n",
      "[14/25][42/110] Loss_D: 1.3684 Loss_G: 5.8552\n",
      "[14/25][43/110] Loss_D: 0.9235 Loss_G: 3.3739\n",
      "[14/25][44/110] Loss_D: 1.9695 Loss_G: 9.8424\n",
      "[14/25][45/110] Loss_D: 1.4342 Loss_G: 4.9416\n",
      "[14/25][46/110] Loss_D: 0.3339 Loss_G: 2.9224\n",
      "[14/25][47/110] Loss_D: 1.5371 Loss_G: 6.2531\n",
      "[14/25][48/110] Loss_D: 0.5819 Loss_G: 5.4891\n",
      "[14/25][49/110] Loss_D: 0.6666 Loss_G: 2.9833\n",
      "[14/25][50/110] Loss_D: 0.7442 Loss_G: 6.3822\n",
      "[14/25][51/110] Loss_D: 0.6311 Loss_G: 4.4390\n",
      "[14/25][52/110] Loss_D: 0.5466 Loss_G: 3.6810\n",
      "[14/25][53/110] Loss_D: 0.7998 Loss_G: 5.9015\n",
      "[14/25][54/110] Loss_D: 0.6091 Loss_G: 4.4938\n",
      "[14/25][55/110] Loss_D: 1.5928 Loss_G: 3.2233\n",
      "[14/25][56/110] Loss_D: 0.8733 Loss_G: 7.8717\n",
      "[14/25][57/110] Loss_D: 0.7418 Loss_G: 5.7290\n",
      "[14/25][58/110] Loss_D: 0.8937 Loss_G: 3.2559\n",
      "[14/25][59/110] Loss_D: 1.3303 Loss_G: 6.1018\n",
      "[14/25][60/110] Loss_D: 0.4053 Loss_G: 5.8945\n",
      "[14/25][61/110] Loss_D: 0.5521 Loss_G: 4.6760\n",
      "[14/25][62/110] Loss_D: 0.8132 Loss_G: 5.6789\n",
      "[14/25][63/110] Loss_D: 0.5013 Loss_G: 5.2894\n",
      "[14/25][64/110] Loss_D: 1.0568 Loss_G: 5.1105\n",
      "[14/25][65/110] Loss_D: 0.7488 Loss_G: 4.0922\n",
      "[14/25][66/110] Loss_D: 0.6640 Loss_G: 2.7212\n",
      "[14/25][67/110] Loss_D: 1.5773 Loss_G: 7.5457\n",
      "[14/25][68/110] Loss_D: 0.8258 Loss_G: 6.4247\n",
      "[14/25][69/110] Loss_D: 0.4192 Loss_G: 3.1610\n",
      "[14/25][70/110] Loss_D: 0.6923 Loss_G: 3.3155\n",
      "[14/25][71/110] Loss_D: 1.9000 Loss_G: 7.5226\n",
      "[14/25][72/110] Loss_D: 1.8061 Loss_G: 4.6414\n",
      "[14/25][73/110] Loss_D: 0.5072 Loss_G: 3.6302\n",
      "[14/25][74/110] Loss_D: 0.8952 Loss_G: 5.5307\n",
      "[14/25][75/110] Loss_D: 0.5831 Loss_G: 4.7821\n",
      "[14/25][76/110] Loss_D: 0.8381 Loss_G: 2.0968\n",
      "[14/25][77/110] Loss_D: 1.4738 Loss_G: 6.4628\n",
      "[14/25][78/110] Loss_D: 2.1292 Loss_G: 3.2898\n",
      "[14/25][79/110] Loss_D: 0.6899 Loss_G: 3.8018\n",
      "[14/25][80/110] Loss_D: 1.3218 Loss_G: 6.7376\n",
      "[14/25][81/110] Loss_D: 0.5637 Loss_G: 4.7235\n",
      "[14/25][82/110] Loss_D: 0.3656 Loss_G: 2.8703\n",
      "[14/25][83/110] Loss_D: 1.5387 Loss_G: 6.3911\n",
      "[14/25][84/110] Loss_D: 1.5020 Loss_G: 2.2284\n",
      "[14/25][85/110] Loss_D: 0.9820 Loss_G: 4.1075\n",
      "[14/25][86/110] Loss_D: 0.5903 Loss_G: 4.2729\n",
      "[14/25][87/110] Loss_D: 0.1793 Loss_G: 4.3914\n",
      "[14/25][88/110] Loss_D: 0.2619 Loss_G: 3.6899\n",
      "[14/25][89/110] Loss_D: 0.4975 Loss_G: 2.6998\n",
      "[14/25][90/110] Loss_D: 0.7717 Loss_G: 3.7670\n",
      "[14/25][91/110] Loss_D: 0.8347 Loss_G: 4.5286\n",
      "[14/25][92/110] Loss_D: 1.4213 Loss_G: 0.7779\n",
      "[14/25][93/110] Loss_D: 2.0574 Loss_G: 8.5120\n",
      "[14/25][94/110] Loss_D: 1.2981 Loss_G: 6.5762\n",
      "[14/25][95/110] Loss_D: 0.3396 Loss_G: 2.9241\n",
      "[14/25][96/110] Loss_D: 0.6153 Loss_G: 4.4815\n",
      "[14/25][97/110] Loss_D: 0.4757 Loss_G: 4.6540\n",
      "[14/25][98/110] Loss_D: 0.7893 Loss_G: 3.0476\n",
      "[14/25][99/110] Loss_D: 1.3809 Loss_G: 4.7134\n",
      "[14/25][100/110] Loss_D: 0.6067 Loss_G: 3.2579\n",
      "[14/25][101/110] Loss_D: 0.6238 Loss_G: 3.8293\n",
      "[14/25][102/110] Loss_D: 1.2553 Loss_G: 5.2966\n",
      "[14/25][103/110] Loss_D: 1.7165 Loss_G: 2.2097\n",
      "[14/25][104/110] Loss_D: 1.6251 Loss_G: 6.6050\n",
      "[14/25][105/110] Loss_D: 0.6158 Loss_G: 4.6049\n",
      "[14/25][106/110] Loss_D: 0.6457 Loss_G: 3.2238\n",
      "[14/25][107/110] Loss_D: 1.3858 Loss_G: 4.4525\n",
      "[14/25][108/110] Loss_D: 0.5776 Loss_G: 4.3913\n",
      "[14/25][109/110] Loss_D: 0.7031 Loss_G: 3.0607\n",
      "[15/25][0/110] Loss_D: 1.0758 Loss_G: 6.8707\n",
      "[15/25][1/110] Loss_D: 0.8050 Loss_G: 3.5278\n",
      "[15/25][2/110] Loss_D: 0.3181 Loss_G: 3.1384\n",
      "[15/25][3/110] Loss_D: 1.5646 Loss_G: 9.5138\n",
      "[15/25][4/110] Loss_D: 2.9038 Loss_G: 6.1455\n",
      "[15/25][5/110] Loss_D: 0.9542 Loss_G: 0.9084\n",
      "[15/25][6/110] Loss_D: 2.5912 Loss_G: 8.1219\n",
      "[15/25][7/110] Loss_D: 0.7460 Loss_G: 5.3850\n",
      "[15/25][8/110] Loss_D: 0.5746 Loss_G: 2.7566\n",
      "[15/25][9/110] Loss_D: 1.5792 Loss_G: 5.9858\n",
      "[15/25][10/110] Loss_D: 0.9485 Loss_G: 3.7333\n",
      "[15/25][11/110] Loss_D: 0.6293 Loss_G: 2.7730\n",
      "[15/25][12/110] Loss_D: 1.4440 Loss_G: 4.2680\n",
      "[15/25][13/110] Loss_D: 0.2922 Loss_G: 4.3704\n",
      "[15/25][14/110] Loss_D: 0.5387 Loss_G: 3.3455\n",
      "[15/25][15/110] Loss_D: 1.1600 Loss_G: 6.0930\n",
      "[15/25][16/110] Loss_D: 0.8922 Loss_G: 3.8506\n",
      "[15/25][17/110] Loss_D: 0.3707 Loss_G: 2.9093\n",
      "[15/25][18/110] Loss_D: 0.8346 Loss_G: 5.9532\n",
      "[15/25][19/110] Loss_D: 1.4021 Loss_G: 2.5790\n",
      "[15/25][20/110] Loss_D: 1.8872 Loss_G: 7.1906\n",
      "[15/25][21/110] Loss_D: 1.2367 Loss_G: 5.2126\n",
      "[15/25][22/110] Loss_D: 0.2300 Loss_G: 2.8273\n",
      "[15/25][23/110] Loss_D: 0.8092 Loss_G: 4.1981\n",
      "[15/25][24/110] Loss_D: 0.5764 Loss_G: 3.8376\n",
      "[15/25][25/110] Loss_D: 0.5051 Loss_G: 3.3552\n",
      "[15/25][26/110] Loss_D: 0.5411 Loss_G: 3.5480\n",
      "[15/25][27/110] Loss_D: 0.5659 Loss_G: 2.7234\n",
      "[15/25][28/110] Loss_D: 1.2215 Loss_G: 6.7474\n",
      "[15/25][29/110] Loss_D: 1.3118 Loss_G: 4.1984\n",
      "[15/25][30/110] Loss_D: 0.2636 Loss_G: 3.3205\n",
      "[15/25][31/110] Loss_D: 1.3880 Loss_G: 5.7834\n",
      "[15/25][32/110] Loss_D: 0.6599 Loss_G: 4.6637\n",
      "[15/25][33/110] Loss_D: 0.3636 Loss_G: 4.9066\n",
      "[15/25][34/110] Loss_D: 0.8447 Loss_G: 5.4805\n",
      "[15/25][35/110] Loss_D: 1.0628 Loss_G: 2.5292\n",
      "[15/25][36/110] Loss_D: 2.1626 Loss_G: 7.4183\n",
      "[15/25][37/110] Loss_D: 2.0334 Loss_G: 2.8316\n",
      "[15/25][38/110] Loss_D: 0.9911 Loss_G: 0.7720\n",
      "[15/25][39/110] Loss_D: 1.9560 Loss_G: 4.9391\n",
      "[15/25][40/110] Loss_D: 0.2546 Loss_G: 4.3868\n",
      "[15/25][41/110] Loss_D: 0.1578 Loss_G: 4.7446\n",
      "[15/25][42/110] Loss_D: 0.5560 Loss_G: 3.1162\n",
      "[15/25][43/110] Loss_D: 0.6759 Loss_G: 4.4313\n",
      "[15/25][44/110] Loss_D: 0.5734 Loss_G: 4.0778\n",
      "[15/25][45/110] Loss_D: 0.5489 Loss_G: 2.6083\n",
      "[15/25][46/110] Loss_D: 0.4766 Loss_G: 4.0619\n",
      "[15/25][47/110] Loss_D: 1.4015 Loss_G: 1.6047\n",
      "[15/25][48/110] Loss_D: 0.3940 Loss_G: 3.7235\n",
      "[15/25][49/110] Loss_D: 0.9678 Loss_G: 6.4099\n",
      "[15/25][50/110] Loss_D: 1.0334 Loss_G: 4.5676\n",
      "[15/25][51/110] Loss_D: 0.4624 Loss_G: 2.9588\n",
      "[15/25][52/110] Loss_D: 1.0731 Loss_G: 6.2014\n",
      "[15/25][53/110] Loss_D: 1.1176 Loss_G: 2.9104\n",
      "[15/25][54/110] Loss_D: 0.8124 Loss_G: 4.6874\n",
      "[15/25][55/110] Loss_D: 0.6857 Loss_G: 5.5257\n",
      "[15/25][56/110] Loss_D: 1.3591 Loss_G: 2.0085\n",
      "[15/25][57/110] Loss_D: 2.8452 Loss_G: 6.7456\n",
      "[15/25][58/110] Loss_D: 0.5989 Loss_G: 5.4023\n",
      "[15/25][59/110] Loss_D: 0.5461 Loss_G: 3.0275\n",
      "[15/25][60/110] Loss_D: 0.8559 Loss_G: 3.7325\n",
      "[15/25][61/110] Loss_D: 1.1785 Loss_G: 4.2733\n",
      "[15/25][62/110] Loss_D: 0.9271 Loss_G: 2.7207\n",
      "[15/25][63/110] Loss_D: 1.0276 Loss_G: 4.9022\n",
      "[15/25][64/110] Loss_D: 1.1078 Loss_G: 3.3249\n",
      "[15/25][65/110] Loss_D: 0.6514 Loss_G: 4.2512\n",
      "[15/25][66/110] Loss_D: 0.6892 Loss_G: 3.8062\n",
      "[15/25][67/110] Loss_D: 0.5747 Loss_G: 5.2870\n",
      "[15/25][68/110] Loss_D: 0.8496 Loss_G: 3.3106\n",
      "[15/25][69/110] Loss_D: 0.6755 Loss_G: 5.6006\n",
      "[15/25][70/110] Loss_D: 0.5574 Loss_G: 4.4441\n",
      "[15/25][71/110] Loss_D: 0.8800 Loss_G: 1.9115\n",
      "[15/25][72/110] Loss_D: 1.0282 Loss_G: 4.2886\n",
      "[15/25][73/110] Loss_D: 0.6853 Loss_G: 6.9663\n",
      "[15/25][74/110] Loss_D: 0.4513 Loss_G: 4.8744\n",
      "[15/25][75/110] Loss_D: 0.7070 Loss_G: 2.7276\n",
      "[15/25][76/110] Loss_D: 1.5525 Loss_G: 6.9086\n",
      "[15/25][77/110] Loss_D: 0.3662 Loss_G: 5.7792\n",
      "[15/25][78/110] Loss_D: 2.7034 Loss_G: 1.4938\n",
      "[15/25][79/110] Loss_D: 1.3061 Loss_G: 5.2023\n",
      "[15/25][80/110] Loss_D: 1.4619 Loss_G: 5.4260\n",
      "[15/25][81/110] Loss_D: 0.6245 Loss_G: 2.7921\n",
      "[15/25][82/110] Loss_D: 0.5023 Loss_G: 4.1145\n",
      "[15/25][83/110] Loss_D: 0.7904 Loss_G: 6.5795\n",
      "[15/25][84/110] Loss_D: 0.7683 Loss_G: 4.0451\n",
      "[15/25][85/110] Loss_D: 1.2161 Loss_G: 4.2767\n",
      "[15/25][86/110] Loss_D: 0.8417 Loss_G: 2.6743\n",
      "[15/25][87/110] Loss_D: 1.4872 Loss_G: 5.0142\n",
      "[15/25][88/110] Loss_D: 0.5409 Loss_G: 4.0040\n",
      "[15/25][89/110] Loss_D: 0.6856 Loss_G: 3.1562\n",
      "[15/25][90/110] Loss_D: 0.3087 Loss_G: 3.4600\n",
      "[15/25][91/110] Loss_D: 0.3889 Loss_G: 4.2572\n",
      "[15/25][92/110] Loss_D: 1.0510 Loss_G: 2.1868\n",
      "[15/25][93/110] Loss_D: 0.8840 Loss_G: 6.1124\n",
      "[15/25][94/110] Loss_D: 1.3516 Loss_G: 3.3322\n",
      "[15/25][95/110] Loss_D: 1.0555 Loss_G: 4.9216\n",
      "[15/25][96/110] Loss_D: 0.5519 Loss_G: 3.9419\n",
      "[15/25][97/110] Loss_D: 0.3874 Loss_G: 2.5332\n",
      "[15/25][98/110] Loss_D: 1.4094 Loss_G: 6.3961\n",
      "[15/25][99/110] Loss_D: 0.7125 Loss_G: 4.6174\n",
      "[15/25][100/110] Loss_D: 0.6275 Loss_G: 2.3205\n",
      "[15/25][101/110] Loss_D: 1.5456 Loss_G: 3.4069\n",
      "[15/25][102/110] Loss_D: 0.5593 Loss_G: 4.7587\n",
      "[15/25][103/110] Loss_D: 0.2562 Loss_G: 5.6285\n",
      "[15/25][104/110] Loss_D: 0.3367 Loss_G: 4.1978\n",
      "[15/25][105/110] Loss_D: 0.5916 Loss_G: 2.7075\n",
      "[15/25][106/110] Loss_D: 1.5656 Loss_G: 6.7541\n",
      "[15/25][107/110] Loss_D: 0.1738 Loss_G: 5.8002\n",
      "[15/25][108/110] Loss_D: 1.4065 Loss_G: 1.3801\n",
      "[15/25][109/110] Loss_D: 1.8942 Loss_G: 6.4541\n",
      "[16/25][0/110] Loss_D: 0.4531 Loss_G: 4.7874\n",
      "[16/25][1/110] Loss_D: 0.8254 Loss_G: 2.0369\n",
      "[16/25][2/110] Loss_D: 2.3003 Loss_G: 5.2327\n",
      "[16/25][3/110] Loss_D: 0.4043 Loss_G: 4.2461\n",
      "[16/25][4/110] Loss_D: 0.7205 Loss_G: 3.0932\n",
      "[16/25][5/110] Loss_D: 0.8404 Loss_G: 4.8061\n",
      "[16/25][6/110] Loss_D: 0.6987 Loss_G: 3.6044\n",
      "[16/25][7/110] Loss_D: 0.9135 Loss_G: 3.3769\n",
      "[16/25][8/110] Loss_D: 0.8463 Loss_G: 4.1624\n",
      "[16/25][9/110] Loss_D: 0.4290 Loss_G: 4.0430\n",
      "[16/25][10/110] Loss_D: 0.4172 Loss_G: 5.8892\n",
      "[16/25][11/110] Loss_D: 0.3612 Loss_G: 4.5300\n",
      "[16/25][12/110] Loss_D: 1.8633 Loss_G: 0.3823\n",
      "[16/25][13/110] Loss_D: 3.1216 Loss_G: 6.2779\n",
      "[16/25][14/110] Loss_D: 0.4286 Loss_G: 6.1673\n",
      "[16/25][15/110] Loss_D: 0.2009 Loss_G: 4.3928\n",
      "[16/25][16/110] Loss_D: 0.6276 Loss_G: 2.7307\n",
      "[16/25][17/110] Loss_D: 1.5048 Loss_G: 6.9298\n",
      "[16/25][18/110] Loss_D: 0.1656 Loss_G: 5.4066\n",
      "[16/25][19/110] Loss_D: 0.6245 Loss_G: 2.6377\n",
      "[16/25][20/110] Loss_D: 1.6687 Loss_G: 5.6453\n",
      "[16/25][21/110] Loss_D: 2.3578 Loss_G: 1.4087\n",
      "[16/25][22/110] Loss_D: 1.4631 Loss_G: 5.6915\n",
      "[16/25][23/110] Loss_D: 0.6743 Loss_G: 3.9344\n",
      "[16/25][24/110] Loss_D: 0.8854 Loss_G: 3.2248\n",
      "[16/25][25/110] Loss_D: 0.8737 Loss_G: 4.3152\n",
      "[16/25][26/110] Loss_D: 0.5715 Loss_G: 3.4931\n",
      "[16/25][27/110] Loss_D: 0.3970 Loss_G: 4.1733\n",
      "[16/25][28/110] Loss_D: 0.6663 Loss_G: 4.3368\n",
      "[16/25][29/110] Loss_D: 0.5601 Loss_G: 4.1482\n",
      "[16/25][30/110] Loss_D: 0.4442 Loss_G: 5.1132\n",
      "[16/25][31/110] Loss_D: 1.1683 Loss_G: 1.7654\n",
      "[16/25][32/110] Loss_D: 0.7253 Loss_G: 4.8261\n",
      "[16/25][33/110] Loss_D: 0.5738 Loss_G: 5.1972\n",
      "[16/25][34/110] Loss_D: 1.2878 Loss_G: 3.1186\n",
      "[16/25][35/110] Loss_D: 2.0022 Loss_G: 7.5794\n",
      "[16/25][36/110] Loss_D: 1.2561 Loss_G: 4.2434\n",
      "[16/25][37/110] Loss_D: 0.6524 Loss_G: 2.8523\n",
      "[16/25][38/110] Loss_D: 1.2665 Loss_G: 7.0760\n",
      "[16/25][39/110] Loss_D: 1.1192 Loss_G: 3.4012\n",
      "[16/25][40/110] Loss_D: 0.9015 Loss_G: 2.2381\n",
      "[16/25][41/110] Loss_D: 1.9174 Loss_G: 6.1314\n",
      "[16/25][42/110] Loss_D: 1.1701 Loss_G: 4.0422\n",
      "[16/25][43/110] Loss_D: 0.2766 Loss_G: 2.8784\n",
      "[16/25][44/110] Loss_D: 1.0000 Loss_G: 6.5259\n",
      "[16/25][45/110] Loss_D: 0.3903 Loss_G: 5.1478\n",
      "[16/25][46/110] Loss_D: 0.7391 Loss_G: 2.2751\n",
      "[16/25][47/110] Loss_D: 1.4063 Loss_G: 5.5026\n",
      "[16/25][48/110] Loss_D: 0.6628 Loss_G: 4.4819\n",
      "[16/25][49/110] Loss_D: 0.4884 Loss_G: 3.1069\n",
      "[16/25][50/110] Loss_D: 0.7997 Loss_G: 4.7453\n",
      "[16/25][51/110] Loss_D: 0.5010 Loss_G: 3.9059\n",
      "[16/25][52/110] Loss_D: 1.6620 Loss_G: 0.9574\n",
      "[16/25][53/110] Loss_D: 1.8365 Loss_G: 6.7423\n",
      "[16/25][54/110] Loss_D: 1.1903 Loss_G: 4.5997\n",
      "[16/25][55/110] Loss_D: 0.3941 Loss_G: 1.8171\n",
      "[16/25][56/110] Loss_D: 1.2469 Loss_G: 7.9939\n",
      "[16/25][57/110] Loss_D: 0.3391 Loss_G: 6.2103\n",
      "[16/25][58/110] Loss_D: 0.8523 Loss_G: 2.2912\n",
      "[16/25][59/110] Loss_D: 0.8764 Loss_G: 4.5350\n",
      "[16/25][60/110] Loss_D: 0.8169 Loss_G: 3.1331\n",
      "[16/25][61/110] Loss_D: 0.8159 Loss_G: 4.7873\n",
      "[16/25][62/110] Loss_D: 1.0155 Loss_G: 4.6170\n",
      "[16/25][63/110] Loss_D: 0.4741 Loss_G: 3.6991\n",
      "[16/25][64/110] Loss_D: 0.7217 Loss_G: 4.9645\n",
      "[16/25][65/110] Loss_D: 0.8334 Loss_G: 2.5891\n",
      "[16/25][66/110] Loss_D: 0.8282 Loss_G: 4.5410\n",
      "[16/25][67/110] Loss_D: 1.0841 Loss_G: 2.0932\n",
      "[16/25][68/110] Loss_D: 1.4218 Loss_G: 7.0590\n",
      "[16/25][69/110] Loss_D: 0.4336 Loss_G: 6.6051\n",
      "[16/25][70/110] Loss_D: 1.1297 Loss_G: 2.4854\n",
      "[16/25][71/110] Loss_D: 1.6427 Loss_G: 7.7798\n",
      "[16/25][72/110] Loss_D: 1.0893 Loss_G: 3.8722\n",
      "[16/25][73/110] Loss_D: 0.6368 Loss_G: 5.5664\n",
      "[16/25][74/110] Loss_D: 1.4039 Loss_G: 1.6257\n",
      "[16/25][75/110] Loss_D: 0.7153 Loss_G: 7.2450\n",
      "[16/25][76/110] Loss_D: 0.4940 Loss_G: 5.2929\n",
      "[16/25][77/110] Loss_D: 0.7655 Loss_G: 3.0521\n",
      "[16/25][78/110] Loss_D: 0.5829 Loss_G: 4.5119\n",
      "[16/25][79/110] Loss_D: 0.7606 Loss_G: 4.5971\n",
      "[16/25][80/110] Loss_D: 0.3817 Loss_G: 4.9746\n",
      "[16/25][81/110] Loss_D: 1.0843 Loss_G: 2.9021\n",
      "[16/25][82/110] Loss_D: 1.0814 Loss_G: 4.5268\n",
      "[16/25][83/110] Loss_D: 0.7078 Loss_G: 5.9875\n",
      "[16/25][84/110] Loss_D: 0.7078 Loss_G: 4.3921\n",
      "[16/25][85/110] Loss_D: 0.8521 Loss_G: 5.7467\n",
      "[16/25][86/110] Loss_D: 0.5720 Loss_G: 4.4323\n",
      "[16/25][87/110] Loss_D: 0.6709 Loss_G: 2.6880\n",
      "[16/25][88/110] Loss_D: 1.4340 Loss_G: 7.0446\n",
      "[16/25][89/110] Loss_D: 1.8035 Loss_G: 2.6202\n",
      "[16/25][90/110] Loss_D: 1.3081 Loss_G: 7.4640\n",
      "[16/25][91/110] Loss_D: 0.7273 Loss_G: 5.8645\n",
      "[16/25][92/110] Loss_D: 0.2314 Loss_G: 5.5476\n",
      "[16/25][93/110] Loss_D: 0.7561 Loss_G: 6.5681\n",
      "[16/25][94/110] Loss_D: 0.6151 Loss_G: 7.2971\n",
      "[16/25][95/110] Loss_D: 0.7275 Loss_G: 5.0178\n",
      "[16/25][96/110] Loss_D: 1.2575 Loss_G: 2.7710\n",
      "[16/25][97/110] Loss_D: 1.8394 Loss_G: 9.8778\n",
      "[16/25][98/110] Loss_D: 1.3407 Loss_G: 6.3306\n",
      "[16/25][99/110] Loss_D: 0.4858 Loss_G: 2.6715\n",
      "[16/25][100/110] Loss_D: 3.1345 Loss_G: 10.5958\n",
      "[16/25][101/110] Loss_D: 1.5161 Loss_G: 6.2532\n",
      "[16/25][102/110] Loss_D: 0.4597 Loss_G: 2.1905\n",
      "[16/25][103/110] Loss_D: 0.9840 Loss_G: 3.5148\n",
      "[16/25][104/110] Loss_D: 0.7780 Loss_G: 4.6857\n",
      "[16/25][105/110] Loss_D: 0.7888 Loss_G: 4.2969\n",
      "[16/25][106/110] Loss_D: 0.2715 Loss_G: 4.3790\n",
      "[16/25][107/110] Loss_D: 0.4417 Loss_G: 3.1574\n",
      "[16/25][108/110] Loss_D: 1.3771 Loss_G: 8.4184\n",
      "[16/25][109/110] Loss_D: 3.8497 Loss_G: 3.4490\n",
      "[17/25][0/110] Loss_D: 0.5536 Loss_G: 3.4043\n",
      "[17/25][1/110] Loss_D: 0.9762 Loss_G: 6.5009\n",
      "[17/25][2/110] Loss_D: 1.3861 Loss_G: 3.6191\n",
      "[17/25][3/110] Loss_D: 0.8735 Loss_G: 2.7015\n",
      "[17/25][4/110] Loss_D: 0.9077 Loss_G: 5.1620\n",
      "[17/25][5/110] Loss_D: 0.5933 Loss_G: 5.0046\n",
      "[17/25][6/110] Loss_D: 0.2992 Loss_G: 3.9815\n",
      "[17/25][7/110] Loss_D: 0.9996 Loss_G: 4.9197\n",
      "[17/25][8/110] Loss_D: 2.1212 Loss_G: 3.2592\n",
      "[17/25][9/110] Loss_D: 0.7060 Loss_G: 5.0972\n",
      "[17/25][10/110] Loss_D: 0.9176 Loss_G: 3.0808\n",
      "[17/25][11/110] Loss_D: 1.5132 Loss_G: 6.8208\n",
      "[17/25][12/110] Loss_D: 1.5296 Loss_G: 3.4778\n",
      "[17/25][13/110] Loss_D: 0.6352 Loss_G: 3.2600\n",
      "[17/25][14/110] Loss_D: 0.8605 Loss_G: 4.8237\n",
      "[17/25][15/110] Loss_D: 0.3282 Loss_G: 4.6389\n",
      "[17/25][16/110] Loss_D: 0.6948 Loss_G: 3.1338\n",
      "[17/25][17/110] Loss_D: 1.3275 Loss_G: 2.0114\n",
      "[17/25][18/110] Loss_D: 1.9142 Loss_G: 6.6422\n",
      "[17/25][19/110] Loss_D: 0.5764 Loss_G: 6.1207\n",
      "[17/25][20/110] Loss_D: 0.5112 Loss_G: 3.3101\n",
      "[17/25][21/110] Loss_D: 0.9019 Loss_G: 3.6013\n",
      "[17/25][22/110] Loss_D: 0.9818 Loss_G: 5.1614\n",
      "[17/25][23/110] Loss_D: 0.5192 Loss_G: 3.6807\n",
      "[17/25][24/110] Loss_D: 1.2152 Loss_G: 1.2382\n",
      "[17/25][25/110] Loss_D: 2.0220 Loss_G: 6.8887\n",
      "[17/25][26/110] Loss_D: 0.6395 Loss_G: 6.9536\n",
      "[17/25][27/110] Loss_D: 0.4797 Loss_G: 3.6018\n",
      "[17/25][28/110] Loss_D: 0.5794 Loss_G: 3.4589\n",
      "[17/25][29/110] Loss_D: 0.5272 Loss_G: 3.8499\n",
      "[17/25][30/110] Loss_D: 0.7190 Loss_G: 4.9845\n",
      "[17/25][31/110] Loss_D: 0.8621 Loss_G: 2.8000\n",
      "[17/25][32/110] Loss_D: 2.0726 Loss_G: 5.3578\n",
      "[17/25][33/110] Loss_D: 1.9555 Loss_G: 1.9434\n",
      "[17/25][34/110] Loss_D: 1.4954 Loss_G: 5.3099\n",
      "[17/25][35/110] Loss_D: 0.6588 Loss_G: 4.7114\n",
      "[17/25][36/110] Loss_D: 0.3781 Loss_G: 4.6953\n",
      "[17/25][37/110] Loss_D: 0.5502 Loss_G: 4.2490\n",
      "[17/25][38/110] Loss_D: 0.4907 Loss_G: 4.0392\n",
      "[17/25][39/110] Loss_D: 1.2617 Loss_G: 5.6170\n",
      "[17/25][40/110] Loss_D: 2.1321 Loss_G: 1.1569\n",
      "[17/25][41/110] Loss_D: 2.4416 Loss_G: 6.7548\n",
      "[17/25][42/110] Loss_D: 0.3749 Loss_G: 6.2941\n",
      "[17/25][43/110] Loss_D: 0.4079 Loss_G: 3.2766\n",
      "[17/25][44/110] Loss_D: 1.2144 Loss_G: 1.6290\n",
      "[17/25][45/110] Loss_D: 1.3955 Loss_G: 2.9419\n",
      "[17/25][46/110] Loss_D: 0.9721 Loss_G: 5.9028\n",
      "[17/25][47/110] Loss_D: 0.4978 Loss_G: 4.9502\n",
      "[17/25][48/110] Loss_D: 1.1809 Loss_G: 1.6121\n",
      "[17/25][49/110] Loss_D: 1.3691 Loss_G: 2.2635\n",
      "[17/25][50/110] Loss_D: 1.3760 Loss_G: 5.9901\n",
      "[17/25][51/110] Loss_D: 0.6322 Loss_G: 5.0271\n",
      "[17/25][52/110] Loss_D: 0.3151 Loss_G: 2.9856\n",
      "[17/25][53/110] Loss_D: 0.7348 Loss_G: 2.8086\n",
      "[17/25][54/110] Loss_D: 1.6520 Loss_G: 5.4957\n",
      "[17/25][55/110] Loss_D: 1.2114 Loss_G: 2.7753\n",
      "[17/25][56/110] Loss_D: 0.7025 Loss_G: 2.1979\n",
      "[17/25][57/110] Loss_D: 2.5175 Loss_G: 8.8247\n",
      "[17/25][58/110] Loss_D: 0.7562 Loss_G: 6.8390\n",
      "[17/25][59/110] Loss_D: 1.2176 Loss_G: 1.4144\n",
      "[17/25][60/110] Loss_D: 1.3178 Loss_G: 4.0728\n",
      "[17/25][61/110] Loss_D: 0.9096 Loss_G: 5.0752\n",
      "[17/25][62/110] Loss_D: 1.0198 Loss_G: 2.8073\n",
      "[17/25][63/110] Loss_D: 1.4132 Loss_G: 3.5399\n",
      "[17/25][64/110] Loss_D: 1.0278 Loss_G: 5.9107\n",
      "[17/25][65/110] Loss_D: 1.1174 Loss_G: 3.2297\n",
      "[17/25][66/110] Loss_D: 0.6656 Loss_G: 3.8988\n",
      "[17/25][67/110] Loss_D: 1.1164 Loss_G: 4.0034\n",
      "[17/25][68/110] Loss_D: 0.3648 Loss_G: 4.3183\n",
      "[17/25][69/110] Loss_D: 0.5002 Loss_G: 3.5144\n",
      "[17/25][70/110] Loss_D: 0.8784 Loss_G: 4.1998\n",
      "[17/25][71/110] Loss_D: 0.6791 Loss_G: 4.7937\n",
      "[17/25][72/110] Loss_D: 0.7517 Loss_G: 3.8775\n",
      "[17/25][73/110] Loss_D: 0.5536 Loss_G: 4.4885\n",
      "[17/25][74/110] Loss_D: 0.4169 Loss_G: 4.1332\n",
      "[17/25][75/110] Loss_D: 1.1890 Loss_G: 3.7483\n",
      "[17/25][76/110] Loss_D: 0.8126 Loss_G: 3.2871\n",
      "[17/25][77/110] Loss_D: 1.2048 Loss_G: 5.7300\n",
      "[17/25][78/110] Loss_D: 0.6831 Loss_G: 4.0630\n",
      "[17/25][79/110] Loss_D: 0.3849 Loss_G: 3.3155\n",
      "[17/25][80/110] Loss_D: 0.8787 Loss_G: 6.0063\n",
      "[17/25][81/110] Loss_D: 0.7125 Loss_G: 3.8955\n",
      "[17/25][82/110] Loss_D: 1.3435 Loss_G: 3.1083\n",
      "[17/25][83/110] Loss_D: 0.4269 Loss_G: 3.8604\n",
      "[17/25][84/110] Loss_D: 0.9481 Loss_G: 6.7844\n",
      "[17/25][85/110] Loss_D: 0.3126 Loss_G: 5.6487\n",
      "[17/25][86/110] Loss_D: 0.3566 Loss_G: 4.1516\n",
      "[17/25][87/110] Loss_D: 0.6681 Loss_G: 4.1377\n",
      "[17/25][88/110] Loss_D: 0.4349 Loss_G: 4.2240\n",
      "[17/25][89/110] Loss_D: 0.6510 Loss_G: 3.8127\n",
      "[17/25][90/110] Loss_D: 0.3595 Loss_G: 3.6386\n",
      "[17/25][91/110] Loss_D: 0.5520 Loss_G: 6.1973\n",
      "[17/25][92/110] Loss_D: 0.6554 Loss_G: 4.4646\n",
      "[17/25][93/110] Loss_D: 1.2579 Loss_G: 3.6498\n",
      "[17/25][94/110] Loss_D: 2.1814 Loss_G: 11.1539\n",
      "[17/25][95/110] Loss_D: 2.8490 Loss_G: 4.4389\n",
      "[17/25][96/110] Loss_D: 0.5672 Loss_G: 2.1289\n",
      "[17/25][97/110] Loss_D: 1.8447 Loss_G: 7.3853\n",
      "[17/25][98/110] Loss_D: 1.0600 Loss_G: 4.2642\n",
      "[17/25][99/110] Loss_D: 0.5827 Loss_G: 2.1175\n",
      "[17/25][100/110] Loss_D: 0.7809 Loss_G: 4.3058\n",
      "[17/25][101/110] Loss_D: 1.0774 Loss_G: 2.2546\n",
      "[17/25][102/110] Loss_D: 0.6656 Loss_G: 2.9784\n",
      "[17/25][103/110] Loss_D: 0.4945 Loss_G: 3.1217\n",
      "[17/25][104/110] Loss_D: 1.3788 Loss_G: 5.4059\n",
      "[17/25][105/110] Loss_D: 0.8897 Loss_G: 2.2393\n",
      "[17/25][106/110] Loss_D: 0.9671 Loss_G: 5.4536\n",
      "[17/25][107/110] Loss_D: 1.1040 Loss_G: 3.1592\n",
      "[17/25][108/110] Loss_D: 0.4378 Loss_G: 2.9664\n",
      "[17/25][109/110] Loss_D: 0.6774 Loss_G: 4.3346\n",
      "[18/25][0/110] Loss_D: 0.2571 Loss_G: 4.6664\n",
      "[18/25][1/110] Loss_D: 0.2618 Loss_G: 3.6422\n",
      "[18/25][2/110] Loss_D: 0.5388 Loss_G: 2.9991\n",
      "[18/25][3/110] Loss_D: 0.7017 Loss_G: 3.3648\n",
      "[18/25][4/110] Loss_D: 0.8413 Loss_G: 4.4658\n",
      "[18/25][5/110] Loss_D: 1.4719 Loss_G: 1.5512\n",
      "[18/25][6/110] Loss_D: 2.8146 Loss_G: 8.2752\n",
      "[18/25][7/110] Loss_D: 1.2998 Loss_G: 6.1242\n",
      "[18/25][8/110] Loss_D: 1.3120 Loss_G: 1.9392\n",
      "[18/25][9/110] Loss_D: 1.1009 Loss_G: 3.8284\n",
      "[18/25][10/110] Loss_D: 0.4014 Loss_G: 5.1048\n",
      "[18/25][11/110] Loss_D: 0.3721 Loss_G: 4.2692\n",
      "[18/25][12/110] Loss_D: 0.6594 Loss_G: 4.0764\n",
      "[18/25][13/110] Loss_D: 0.3566 Loss_G: 3.7810\n",
      "[18/25][14/110] Loss_D: 0.4918 Loss_G: 4.4093\n",
      "[18/25][15/110] Loss_D: 0.5786 Loss_G: 3.3926\n",
      "[18/25][16/110] Loss_D: 0.6666 Loss_G: 2.2366\n",
      "[18/25][17/110] Loss_D: 1.1766 Loss_G: 5.6316\n",
      "[18/25][18/110] Loss_D: 0.1824 Loss_G: 5.8115\n",
      "[18/25][19/110] Loss_D: 0.4813 Loss_G: 4.2091\n",
      "[18/25][20/110] Loss_D: 0.2030 Loss_G: 2.9696\n",
      "[18/25][21/110] Loss_D: 0.6239 Loss_G: 4.4133\n",
      "[18/25][22/110] Loss_D: 0.2592 Loss_G: 4.2789\n",
      "[18/25][23/110] Loss_D: 0.2258 Loss_G: 4.5132\n",
      "[18/25][24/110] Loss_D: 1.0124 Loss_G: 2.3746\n",
      "[18/25][25/110] Loss_D: 1.0025 Loss_G: 4.4075\n",
      "[18/25][26/110] Loss_D: 0.2181 Loss_G: 5.3157\n",
      "[18/25][27/110] Loss_D: 0.5028 Loss_G: 4.5087\n",
      "[18/25][28/110] Loss_D: 0.5165 Loss_G: 4.2382\n",
      "[18/25][29/110] Loss_D: 0.5400 Loss_G: 3.1672\n",
      "[18/25][30/110] Loss_D: 2.2614 Loss_G: 8.1860\n",
      "[18/25][31/110] Loss_D: 3.2960 Loss_G: 4.6799\n",
      "[18/25][32/110] Loss_D: 0.3841 Loss_G: 3.2529\n",
      "[18/25][33/110] Loss_D: 2.0091 Loss_G: 7.6934\n",
      "[18/25][34/110] Loss_D: 0.6046 Loss_G: 6.5461\n",
      "[18/25][35/110] Loss_D: 1.2426 Loss_G: 1.9923\n",
      "[18/25][36/110] Loss_D: 1.3736 Loss_G: 4.7465\n",
      "[18/25][37/110] Loss_D: 0.3341 Loss_G: 5.7033\n",
      "[18/25][38/110] Loss_D: 0.7990 Loss_G: 2.5936\n",
      "[18/25][39/110] Loss_D: 0.6301 Loss_G: 2.6265\n",
      "[18/25][40/110] Loss_D: 1.3355 Loss_G: 2.6491\n",
      "[18/25][41/110] Loss_D: 0.3911 Loss_G: 3.1267\n",
      "[18/25][42/110] Loss_D: 0.6786 Loss_G: 3.2680\n",
      "[18/25][43/110] Loss_D: 0.5068 Loss_G: 3.9953\n",
      "[18/25][44/110] Loss_D: 0.5241 Loss_G: 3.7113\n",
      "[18/25][45/110] Loss_D: 0.3562 Loss_G: 3.1061\n",
      "[18/25][46/110] Loss_D: 0.6415 Loss_G: 2.6864\n",
      "[18/25][47/110] Loss_D: 0.5235 Loss_G: 3.8310\n",
      "[18/25][48/110] Loss_D: 0.4076 Loss_G: 4.8979\n",
      "[18/25][49/110] Loss_D: 0.4471 Loss_G: 4.9490\n",
      "[18/25][50/110] Loss_D: 0.9248 Loss_G: 2.6659\n",
      "[18/25][51/110] Loss_D: 1.0788 Loss_G: 6.7136\n",
      "[18/25][52/110] Loss_D: 0.6460 Loss_G: 4.8832\n",
      "[18/25][53/110] Loss_D: 0.4444 Loss_G: 3.7335\n",
      "[18/25][54/110] Loss_D: 0.8844 Loss_G: 2.5914\n",
      "[18/25][55/110] Loss_D: 1.4438 Loss_G: 8.4434\n",
      "[18/25][56/110] Loss_D: 1.5435 Loss_G: 5.9961\n",
      "[18/25][57/110] Loss_D: 0.5005 Loss_G: 2.4489\n",
      "[18/25][58/110] Loss_D: 1.0908 Loss_G: 6.8114\n",
      "[18/25][59/110] Loss_D: 1.4408 Loss_G: 2.6208\n",
      "[18/25][60/110] Loss_D: 1.5086 Loss_G: 6.2290\n",
      "[18/25][61/110] Loss_D: 0.5654 Loss_G: 4.4875\n",
      "[18/25][62/110] Loss_D: 0.2867 Loss_G: 4.1155\n",
      "[18/25][63/110] Loss_D: 0.6801 Loss_G: 2.3318\n",
      "[18/25][64/110] Loss_D: 1.3477 Loss_G: 4.3019\n",
      "[18/25][65/110] Loss_D: 0.4640 Loss_G: 4.4951\n",
      "[18/25][66/110] Loss_D: 0.5014 Loss_G: 3.3103\n",
      "[18/25][67/110] Loss_D: 0.9871 Loss_G: 3.2314\n",
      "[18/25][68/110] Loss_D: 1.2853 Loss_G: 6.2109\n",
      "[18/25][69/110] Loss_D: 1.2103 Loss_G: 2.9780\n",
      "[18/25][70/110] Loss_D: 0.8108 Loss_G: 5.1435\n",
      "[18/25][71/110] Loss_D: 0.5398 Loss_G: 5.0846\n",
      "[18/25][72/110] Loss_D: 0.4867 Loss_G: 4.4045\n",
      "[18/25][73/110] Loss_D: 0.6162 Loss_G: 4.6024\n",
      "[18/25][74/110] Loss_D: 0.9302 Loss_G: 7.2920\n",
      "[18/25][75/110] Loss_D: 0.5062 Loss_G: 4.3006\n",
      "[18/25][76/110] Loss_D: 1.1279 Loss_G: 1.8392\n",
      "[18/25][77/110] Loss_D: 1.3625 Loss_G: 6.9915\n",
      "[18/25][78/110] Loss_D: 0.4634 Loss_G: 5.6199\n",
      "[18/25][79/110] Loss_D: 0.7373 Loss_G: 2.6223\n",
      "[18/25][80/110] Loss_D: 1.7046 Loss_G: 8.0955\n",
      "[18/25][81/110] Loss_D: 0.5214 Loss_G: 6.4352\n",
      "[18/25][82/110] Loss_D: 1.1038 Loss_G: 1.9843\n",
      "[18/25][83/110] Loss_D: 1.7904 Loss_G: 5.8596\n",
      "[18/25][84/110] Loss_D: 0.2319 Loss_G: 5.9846\n",
      "[18/25][85/110] Loss_D: 0.5229 Loss_G: 3.2619\n",
      "[18/25][86/110] Loss_D: 0.6097 Loss_G: 2.7046\n",
      "[18/25][87/110] Loss_D: 1.0874 Loss_G: 2.7186\n",
      "[18/25][88/110] Loss_D: 1.2225 Loss_G: 6.0052\n",
      "[18/25][89/110] Loss_D: 0.9308 Loss_G: 5.6184\n",
      "[18/25][90/110] Loss_D: 0.7171 Loss_G: 1.8536\n",
      "[18/25][91/110] Loss_D: 0.9145 Loss_G: 5.7395\n",
      "[18/25][92/110] Loss_D: 0.0410 Loss_G: 6.2624\n",
      "[18/25][93/110] Loss_D: 0.4818 Loss_G: 3.4097\n",
      "[18/25][94/110] Loss_D: 0.2539 Loss_G: 2.6411\n",
      "[18/25][95/110] Loss_D: 0.6450 Loss_G: 4.0099\n",
      "[18/25][96/110] Loss_D: 0.8187 Loss_G: 4.2505\n",
      "[18/25][97/110] Loss_D: 0.2010 Loss_G: 4.5860\n",
      "[18/25][98/110] Loss_D: 0.4230 Loss_G: 2.9695\n",
      "[18/25][99/110] Loss_D: 0.5854 Loss_G: 3.8767\n",
      "[18/25][100/110] Loss_D: 0.5497 Loss_G: 4.0231\n",
      "[18/25][101/110] Loss_D: 0.1952 Loss_G: 4.7171\n",
      "[18/25][102/110] Loss_D: 0.2851 Loss_G: 3.7695\n",
      "[18/25][103/110] Loss_D: 0.8017 Loss_G: 3.8849\n",
      "[18/25][104/110] Loss_D: 0.2000 Loss_G: 4.4268\n",
      "[18/25][105/110] Loss_D: 1.3296 Loss_G: 2.7360\n",
      "[18/25][106/110] Loss_D: 0.6211 Loss_G: 4.8793\n",
      "[18/25][107/110] Loss_D: 0.5701 Loss_G: 4.2195\n",
      "[18/25][108/110] Loss_D: 0.6105 Loss_G: 4.4524\n",
      "[18/25][109/110] Loss_D: 1.0285 Loss_G: 3.8399\n",
      "[19/25][0/110] Loss_D: 2.1704 Loss_G: 9.3103\n",
      "[19/25][1/110] Loss_D: 3.5034 Loss_G: 4.7308\n",
      "[19/25][2/110] Loss_D: 0.3102 Loss_G: 3.0889\n",
      "[19/25][3/110] Loss_D: 0.7719 Loss_G: 6.2339\n",
      "[19/25][4/110] Loss_D: 0.2867 Loss_G: 4.1065\n",
      "[19/25][5/110] Loss_D: 0.6215 Loss_G: 3.2669\n",
      "[19/25][6/110] Loss_D: 1.3045 Loss_G: 3.4460\n",
      "[19/25][7/110] Loss_D: 0.7703 Loss_G: 2.9436\n",
      "[19/25][8/110] Loss_D: 0.4425 Loss_G: 4.4941\n",
      "[19/25][9/110] Loss_D: 0.5432 Loss_G: 3.2815\n",
      "[19/25][10/110] Loss_D: 0.8392 Loss_G: 2.0572\n",
      "[19/25][11/110] Loss_D: 0.4838 Loss_G: 4.1491\n",
      "[19/25][12/110] Loss_D: 0.3909 Loss_G: 3.4861\n",
      "[19/25][13/110] Loss_D: 0.5523 Loss_G: 3.1592\n",
      "[19/25][14/110] Loss_D: 0.6678 Loss_G: 2.9791\n",
      "[19/25][15/110] Loss_D: 0.8067 Loss_G: 2.7235\n",
      "[19/25][16/110] Loss_D: 0.6331 Loss_G: 5.3697\n",
      "[19/25][17/110] Loss_D: 0.2844 Loss_G: 5.1599\n",
      "[19/25][18/110] Loss_D: 0.4379 Loss_G: 3.3680\n",
      "[19/25][19/110] Loss_D: 0.7391 Loss_G: 2.3090\n",
      "[19/25][20/110] Loss_D: 0.9004 Loss_G: 5.6811\n",
      "[19/25][21/110] Loss_D: 1.3198 Loss_G: 2.8571\n",
      "[19/25][22/110] Loss_D: 0.8702 Loss_G: 6.4890\n",
      "[19/25][23/110] Loss_D: 0.4516 Loss_G: 4.8384\n",
      "[19/25][24/110] Loss_D: 0.2489 Loss_G: 3.6227\n",
      "[19/25][25/110] Loss_D: 0.6868 Loss_G: 4.8596\n",
      "[19/25][26/110] Loss_D: 0.2219 Loss_G: 4.6295\n",
      "[19/25][27/110] Loss_D: 0.2914 Loss_G: 3.9774\n",
      "[19/25][28/110] Loss_D: 0.2523 Loss_G: 4.4330\n",
      "[19/25][29/110] Loss_D: 0.4268 Loss_G: 5.3323\n",
      "[19/25][30/110] Loss_D: 1.3685 Loss_G: 2.0963\n",
      "[19/25][31/110] Loss_D: 0.9040 Loss_G: 3.7920\n",
      "[19/25][32/110] Loss_D: 0.7609 Loss_G: 3.6301\n",
      "[19/25][33/110] Loss_D: 0.9565 Loss_G: 6.5890\n",
      "[19/25][34/110] Loss_D: 0.5722 Loss_G: 4.7742\n",
      "[19/25][35/110] Loss_D: 0.4371 Loss_G: 3.3625\n",
      "[19/25][36/110] Loss_D: 1.1536 Loss_G: 6.6569\n",
      "[19/25][37/110] Loss_D: 0.6219 Loss_G: 4.9325\n",
      "[19/25][38/110] Loss_D: 0.3374 Loss_G: 4.5327\n",
      "[19/25][39/110] Loss_D: 0.6277 Loss_G: 5.6809\n",
      "[19/25][40/110] Loss_D: 0.3406 Loss_G: 4.8077\n",
      "[19/25][41/110] Loss_D: 0.9800 Loss_G: 2.8997\n",
      "[19/25][42/110] Loss_D: 1.4224 Loss_G: 8.6258\n",
      "[19/25][43/110] Loss_D: 1.3519 Loss_G: 3.2415\n",
      "[19/25][44/110] Loss_D: 1.5463 Loss_G: 6.8695\n",
      "[19/25][45/110] Loss_D: 0.7149 Loss_G: 4.5167\n",
      "[19/25][46/110] Loss_D: 0.4912 Loss_G: 3.9221\n",
      "[19/25][47/110] Loss_D: 1.0719 Loss_G: 6.2245\n",
      "[19/25][48/110] Loss_D: 0.7000 Loss_G: 4.2158\n",
      "[19/25][49/110] Loss_D: 0.4961 Loss_G: 5.5912\n",
      "[19/25][50/110] Loss_D: 0.7727 Loss_G: 2.5003\n",
      "[19/25][51/110] Loss_D: 1.9000 Loss_G: 8.1945\n",
      "[19/25][52/110] Loss_D: 1.1077 Loss_G: 3.0968\n",
      "[19/25][53/110] Loss_D: 1.2428 Loss_G: 3.8608\n",
      "[19/25][54/110] Loss_D: 1.1278 Loss_G: 8.7604\n",
      "[19/25][55/110] Loss_D: 1.4492 Loss_G: 4.9479\n",
      "[19/25][56/110] Loss_D: 0.2328 Loss_G: 3.6086\n",
      "[19/25][57/110] Loss_D: 0.9457 Loss_G: 7.5206\n",
      "[19/25][58/110] Loss_D: 0.6912 Loss_G: 3.8605\n",
      "[19/25][59/110] Loss_D: 0.2944 Loss_G: 4.4338\n",
      "[19/25][60/110] Loss_D: 0.6939 Loss_G: 3.6243\n",
      "[19/25][61/110] Loss_D: 0.2940 Loss_G: 4.3458\n",
      "[19/25][62/110] Loss_D: 0.5432 Loss_G: 5.4060\n",
      "[19/25][63/110] Loss_D: 0.4808 Loss_G: 3.6957\n",
      "[19/25][64/110] Loss_D: 0.6052 Loss_G: 4.4108\n",
      "[19/25][65/110] Loss_D: 0.5246 Loss_G: 6.5647\n",
      "[19/25][66/110] Loss_D: 1.2317 Loss_G: 1.9034\n",
      "[19/25][67/110] Loss_D: 0.6244 Loss_G: 4.8162\n",
      "[19/25][68/110] Loss_D: 0.4042 Loss_G: 5.4557\n",
      "[19/25][69/110] Loss_D: 0.7084 Loss_G: 2.5843\n",
      "[19/25][70/110] Loss_D: 0.8075 Loss_G: 4.1690\n",
      "[19/25][71/110] Loss_D: 0.7203 Loss_G: 5.5471\n",
      "[19/25][72/110] Loss_D: 1.2353 Loss_G: 1.6729\n",
      "[19/25][73/110] Loss_D: 1.6739 Loss_G: 8.8537\n",
      "[19/25][74/110] Loss_D: 0.2513 Loss_G: 8.1107\n",
      "[19/25][75/110] Loss_D: 0.5534 Loss_G: 4.1506\n",
      "[19/25][76/110] Loss_D: 0.5224 Loss_G: 4.9051\n",
      "[19/25][77/110] Loss_D: 0.7330 Loss_G: 6.1686\n",
      "[19/25][78/110] Loss_D: 1.1562 Loss_G: 3.7076\n",
      "[19/25][79/110] Loss_D: 0.8769 Loss_G: 7.8904\n",
      "[19/25][80/110] Loss_D: 0.5940 Loss_G: 5.4153\n",
      "[19/25][81/110] Loss_D: 0.5574 Loss_G: 4.6326\n",
      "[19/25][82/110] Loss_D: 1.3408 Loss_G: 2.3299\n",
      "[19/25][83/110] Loss_D: 2.8977 Loss_G: 10.0927\n",
      "[19/25][84/110] Loss_D: 2.2205 Loss_G: 5.4768\n",
      "[19/25][85/110] Loss_D: 0.5020 Loss_G: 3.0137\n",
      "[19/25][86/110] Loss_D: 0.7652 Loss_G: 4.2380\n",
      "[19/25][87/110] Loss_D: 0.4030 Loss_G: 3.8413\n",
      "[19/25][88/110] Loss_D: 0.8438 Loss_G: 6.0099\n",
      "[19/25][89/110] Loss_D: 1.2267 Loss_G: 3.5248\n",
      "[19/25][90/110] Loss_D: 0.5548 Loss_G: 2.5282\n",
      "[19/25][91/110] Loss_D: 0.8585 Loss_G: 4.2788\n",
      "[19/25][92/110] Loss_D: 0.3882 Loss_G: 4.2054\n",
      "[19/25][93/110] Loss_D: 0.5912 Loss_G: 5.2040\n",
      "[19/25][94/110] Loss_D: 0.8764 Loss_G: 3.5396\n",
      "[19/25][95/110] Loss_D: 0.6417 Loss_G: 2.8589\n",
      "[19/25][96/110] Loss_D: 0.6411 Loss_G: 3.5776\n",
      "[19/25][97/110] Loss_D: 1.0726 Loss_G: 7.8019\n",
      "[19/25][98/110] Loss_D: 1.4308 Loss_G: 4.2316\n",
      "[19/25][99/110] Loss_D: 0.4970 Loss_G: 2.7523\n",
      "[19/25][100/110] Loss_D: 0.7958 Loss_G: 5.8363\n",
      "[19/25][101/110] Loss_D: 1.1234 Loss_G: 3.9703\n",
      "[19/25][102/110] Loss_D: 0.6389 Loss_G: 5.1875\n",
      "[19/25][103/110] Loss_D: 1.0210 Loss_G: 2.3412\n",
      "[19/25][104/110] Loss_D: 2.3599 Loss_G: 5.4998\n",
      "[19/25][105/110] Loss_D: 0.4403 Loss_G: 5.6849\n",
      "[19/25][106/110] Loss_D: 0.3638 Loss_G: 4.6467\n",
      "[19/25][107/110] Loss_D: 0.9443 Loss_G: 4.4970\n",
      "[19/25][108/110] Loss_D: 0.4005 Loss_G: 4.6946\n",
      "[19/25][109/110] Loss_D: 1.0871 Loss_G: 2.1577\n",
      "[20/25][0/110] Loss_D: 1.7151 Loss_G: 8.2001\n",
      "[20/25][1/110] Loss_D: 1.3120 Loss_G: 5.5430\n",
      "[20/25][2/110] Loss_D: 0.1911 Loss_G: 3.5284\n",
      "[20/25][3/110] Loss_D: 0.3966 Loss_G: 4.6627\n",
      "[20/25][4/110] Loss_D: 0.3386 Loss_G: 3.8935\n",
      "[20/25][5/110] Loss_D: 1.1901 Loss_G: 8.4689\n",
      "[20/25][6/110] Loss_D: 2.1076 Loss_G: 3.1603\n",
      "[20/25][7/110] Loss_D: 0.5075 Loss_G: 2.7033\n",
      "[20/25][8/110] Loss_D: 0.6752 Loss_G: 4.9311\n",
      "[20/25][9/110] Loss_D: 0.5272 Loss_G: 4.7607\n",
      "[20/25][10/110] Loss_D: 0.4801 Loss_G: 4.0454\n",
      "[20/25][11/110] Loss_D: 2.3479 Loss_G: 1.5305\n",
      "[20/25][12/110] Loss_D: 3.0037 Loss_G: 4.9092\n",
      "[20/25][13/110] Loss_D: 0.5376 Loss_G: 5.2996\n",
      "[20/25][14/110] Loss_D: 0.3167 Loss_G: 3.3308\n",
      "[20/25][15/110] Loss_D: 0.6175 Loss_G: 3.3627\n",
      "[20/25][16/110] Loss_D: 1.0101 Loss_G: 3.1259\n",
      "[20/25][17/110] Loss_D: 1.0889 Loss_G: 4.5561\n",
      "[20/25][18/110] Loss_D: 0.3936 Loss_G: 3.9625\n",
      "[20/25][19/110] Loss_D: 0.6003 Loss_G: 2.5378\n",
      "[20/25][20/110] Loss_D: 1.0362 Loss_G: 5.1660\n",
      "[20/25][21/110] Loss_D: 0.8226 Loss_G: 3.3320\n",
      "[20/25][22/110] Loss_D: 0.7939 Loss_G: 3.3572\n",
      "[20/25][23/110] Loss_D: 1.0604 Loss_G: 4.9697\n",
      "[20/25][24/110] Loss_D: 0.4750 Loss_G: 4.8808\n",
      "[20/25][25/110] Loss_D: 0.9561 Loss_G: 3.3592\n",
      "[20/25][26/110] Loss_D: 1.0274 Loss_G: 4.9544\n",
      "[20/25][27/110] Loss_D: 0.4223 Loss_G: 4.5249\n",
      "[20/25][28/110] Loss_D: 0.7666 Loss_G: 5.8701\n",
      "[20/25][29/110] Loss_D: 0.2546 Loss_G: 4.8062\n",
      "[20/25][30/110] Loss_D: 0.8754 Loss_G: 1.7483\n",
      "[20/25][31/110] Loss_D: 1.2304 Loss_G: 5.1968\n",
      "[20/25][32/110] Loss_D: 0.4288 Loss_G: 4.8862\n",
      "[20/25][33/110] Loss_D: 1.8307 Loss_G: 2.4645\n",
      "[20/25][34/110] Loss_D: 1.2113 Loss_G: 6.8298\n",
      "[20/25][35/110] Loss_D: 0.9419 Loss_G: 4.8700\n",
      "[20/25][36/110] Loss_D: 0.3563 Loss_G: 3.2916\n",
      "[20/25][37/110] Loss_D: 1.1898 Loss_G: 7.1072\n",
      "[20/25][38/110] Loss_D: 0.7016 Loss_G: 6.9824\n",
      "[20/25][39/110] Loss_D: 0.4408 Loss_G: 3.9446\n",
      "[20/25][40/110] Loss_D: 0.4280 Loss_G: 3.0812\n",
      "[20/25][41/110] Loss_D: 0.6941 Loss_G: 6.2166\n",
      "[20/25][42/110] Loss_D: 1.0213 Loss_G: 4.0004\n",
      "[20/25][43/110] Loss_D: 0.4294 Loss_G: 2.5712\n",
      "[20/25][44/110] Loss_D: 0.4154 Loss_G: 4.5607\n",
      "[20/25][45/110] Loss_D: 0.4822 Loss_G: 5.0253\n",
      "[20/25][46/110] Loss_D: 1.4452 Loss_G: 2.4702\n",
      "[20/25][47/110] Loss_D: 1.6503 Loss_G: 9.4367\n",
      "[20/25][48/110] Loss_D: 1.8237 Loss_G: 6.2093\n",
      "[20/25][49/110] Loss_D: 0.2024 Loss_G: 3.6707\n",
      "[20/25][50/110] Loss_D: 0.8753 Loss_G: 4.8456\n",
      "[20/25][51/110] Loss_D: 0.3633 Loss_G: 6.2971\n",
      "[20/25][52/110] Loss_D: 0.5742 Loss_G: 3.7395\n",
      "[20/25][53/110] Loss_D: 1.3232 Loss_G: 3.4386\n",
      "[20/25][54/110] Loss_D: 1.0814 Loss_G: 5.8701\n",
      "[20/25][55/110] Loss_D: 0.6360 Loss_G: 3.9402\n",
      "[20/25][56/110] Loss_D: 0.4597 Loss_G: 6.1565\n",
      "[20/25][57/110] Loss_D: 0.5084 Loss_G: 5.2885\n",
      "[20/25][58/110] Loss_D: 0.3338 Loss_G: 5.3791\n",
      "[20/25][59/110] Loss_D: 0.8145 Loss_G: 3.1345\n",
      "[20/25][60/110] Loss_D: 1.5254 Loss_G: 10.3207\n",
      "[20/25][61/110] Loss_D: 2.6634 Loss_G: 5.9032\n",
      "[20/25][62/110] Loss_D: 0.1368 Loss_G: 2.8119\n",
      "[20/25][63/110] Loss_D: 1.3888 Loss_G: 8.7429\n",
      "[20/25][64/110] Loss_D: 0.5569 Loss_G: 5.8494\n",
      "[20/25][65/110] Loss_D: 0.7131 Loss_G: 2.3056\n",
      "[20/25][66/110] Loss_D: 0.9994 Loss_G: 6.3935\n",
      "[20/25][67/110] Loss_D: 0.4206 Loss_G: 4.9268\n",
      "[20/25][68/110] Loss_D: 0.4567 Loss_G: 3.0004\n",
      "[20/25][69/110] Loss_D: 1.1661 Loss_G: 7.9925\n",
      "[20/25][70/110] Loss_D: 1.8852 Loss_G: 2.4780\n",
      "[20/25][71/110] Loss_D: 0.7778 Loss_G: 2.8273\n",
      "[20/25][72/110] Loss_D: 1.1131 Loss_G: 7.7999\n",
      "[20/25][73/110] Loss_D: 0.5256 Loss_G: 5.2556\n",
      "[20/25][74/110] Loss_D: 0.7139 Loss_G: 2.4886\n",
      "[20/25][75/110] Loss_D: 0.8970 Loss_G: 4.9329\n",
      "[20/25][76/110] Loss_D: 1.1182 Loss_G: 2.7374\n",
      "[20/25][77/110] Loss_D: 1.6293 Loss_G: 4.8222\n",
      "[20/25][78/110] Loss_D: 0.5174 Loss_G: 4.3277\n",
      "[20/25][79/110] Loss_D: 0.8911 Loss_G: 1.5938\n",
      "[20/25][80/110] Loss_D: 1.6976 Loss_G: 6.5112\n",
      "[20/25][81/110] Loss_D: 0.8100 Loss_G: 4.8386\n",
      "[20/25][82/110] Loss_D: 0.7084 Loss_G: 2.3919\n",
      "[20/25][83/110] Loss_D: 1.6757 Loss_G: 7.9594\n",
      "[20/25][84/110] Loss_D: 1.3333 Loss_G: 3.8489\n",
      "[20/25][85/110] Loss_D: 0.7453 Loss_G: 2.5884\n",
      "[20/25][86/110] Loss_D: 0.5179 Loss_G: 3.9402\n",
      "[20/25][87/110] Loss_D: 0.3845 Loss_G: 3.7454\n",
      "[20/25][88/110] Loss_D: 0.7980 Loss_G: 3.7399\n",
      "[20/25][89/110] Loss_D: 0.4136 Loss_G: 3.8029\n",
      "[20/25][90/110] Loss_D: 0.3990 Loss_G: 3.9689\n",
      "[20/25][91/110] Loss_D: 0.3234 Loss_G: 3.5827\n",
      "[20/25][92/110] Loss_D: 0.5449 Loss_G: 3.6556\n",
      "[20/25][93/110] Loss_D: 1.3131 Loss_G: 5.2423\n",
      "[20/25][94/110] Loss_D: 0.9659 Loss_G: 2.8362\n",
      "[20/25][95/110] Loss_D: 1.0789 Loss_G: 6.5260\n",
      "[20/25][96/110] Loss_D: 0.4876 Loss_G: 4.7227\n",
      "[20/25][97/110] Loss_D: 0.8722 Loss_G: 1.8350\n",
      "[20/25][98/110] Loss_D: 0.9935 Loss_G: 6.8182\n",
      "[20/25][99/110] Loss_D: 0.5114 Loss_G: 5.2589\n",
      "[20/25][100/110] Loss_D: 0.5469 Loss_G: 2.7598\n",
      "[20/25][101/110] Loss_D: 1.0591 Loss_G: 4.8424\n",
      "[20/25][102/110] Loss_D: 0.2438 Loss_G: 5.9041\n",
      "[20/25][103/110] Loss_D: 0.3871 Loss_G: 4.3751\n",
      "[20/25][104/110] Loss_D: 0.7225 Loss_G: 2.5307\n",
      "[20/25][105/110] Loss_D: 1.4253 Loss_G: 11.3313\n",
      "[20/25][106/110] Loss_D: 2.0783 Loss_G: 8.2228\n",
      "[20/25][107/110] Loss_D: 0.8452 Loss_G: 2.1873\n",
      "[20/25][108/110] Loss_D: 1.2816 Loss_G: 6.4058\n",
      "[20/25][109/110] Loss_D: 0.3365 Loss_G: 6.4550\n",
      "[21/25][0/110] Loss_D: 0.3663 Loss_G: 4.9313\n",
      "[21/25][1/110] Loss_D: 1.1390 Loss_G: 4.2350\n",
      "[21/25][2/110] Loss_D: 1.1271 Loss_G: 3.3905\n",
      "[21/25][3/110] Loss_D: 0.5626 Loss_G: 4.0700\n",
      "[21/25][4/110] Loss_D: 0.5123 Loss_G: 3.6876\n",
      "[21/25][5/110] Loss_D: 0.5876 Loss_G: 3.4797\n",
      "[21/25][6/110] Loss_D: 0.7211 Loss_G: 6.0280\n",
      "[21/25][7/110] Loss_D: 0.1717 Loss_G: 5.3340\n",
      "[21/25][8/110] Loss_D: 0.5570 Loss_G: 3.3403\n",
      "[21/25][9/110] Loss_D: 0.5990 Loss_G: 3.2703\n",
      "[21/25][10/110] Loss_D: 0.5034 Loss_G: 3.7027\n",
      "[21/25][11/110] Loss_D: 0.4903 Loss_G: 4.4923\n",
      "[21/25][12/110] Loss_D: 0.5061 Loss_G: 3.8281\n",
      "[21/25][13/110] Loss_D: 0.6002 Loss_G: 2.9100\n",
      "[21/25][14/110] Loss_D: 0.7987 Loss_G: 7.7492\n",
      "[21/25][15/110] Loss_D: 2.7640 Loss_G: 3.7886\n",
      "[21/25][16/110] Loss_D: 0.7472 Loss_G: 4.1268\n",
      "[21/25][17/110] Loss_D: 0.5329 Loss_G: 5.4215\n",
      "[21/25][18/110] Loss_D: 0.5258 Loss_G: 4.3244\n",
      "[21/25][19/110] Loss_D: 0.5932 Loss_G: 5.0228\n",
      "[21/25][20/110] Loss_D: 0.5594 Loss_G: 3.7943\n",
      "[21/25][21/110] Loss_D: 1.3133 Loss_G: 2.3055\n",
      "[21/25][22/110] Loss_D: 1.9750 Loss_G: 7.3824\n",
      "[21/25][23/110] Loss_D: 1.4551 Loss_G: 3.0319\n",
      "[21/25][24/110] Loss_D: 0.2690 Loss_G: 2.9557\n",
      "[21/25][25/110] Loss_D: 2.2158 Loss_G: 9.5125\n",
      "[21/25][26/110] Loss_D: 1.0799 Loss_G: 5.3638\n",
      "[21/25][27/110] Loss_D: 0.6852 Loss_G: 2.0691\n",
      "[21/25][28/110] Loss_D: 1.1769 Loss_G: 5.2399\n",
      "[21/25][29/110] Loss_D: 0.7092 Loss_G: 4.2148\n",
      "[21/25][30/110] Loss_D: 0.4987 Loss_G: 3.2594\n",
      "[21/25][31/110] Loss_D: 0.8745 Loss_G: 5.3142\n",
      "[21/25][32/110] Loss_D: 2.5190 Loss_G: 4.4601\n",
      "[21/25][33/110] Loss_D: 1.3289 Loss_G: 1.7687\n",
      "[21/25][34/110] Loss_D: 1.3090 Loss_G: 7.0517\n",
      "[21/25][35/110] Loss_D: 0.3102 Loss_G: 5.2342\n",
      "[21/25][36/110] Loss_D: 0.2040 Loss_G: 4.4624\n",
      "[21/25][37/110] Loss_D: 0.2808 Loss_G: 3.0643\n",
      "[21/25][38/110] Loss_D: 0.3555 Loss_G: 5.2100\n",
      "[21/25][39/110] Loss_D: 0.7043 Loss_G: 5.8810\n",
      "[21/25][40/110] Loss_D: 0.5806 Loss_G: 4.1398\n",
      "[21/25][41/110] Loss_D: 0.8430 Loss_G: 4.7206\n",
      "[21/25][42/110] Loss_D: 0.5333 Loss_G: 4.3756\n",
      "[21/25][43/110] Loss_D: 0.7443 Loss_G: 2.8752\n",
      "[21/25][44/110] Loss_D: 1.5607 Loss_G: 7.2853\n",
      "[21/25][45/110] Loss_D: 0.9029 Loss_G: 3.3716\n",
      "[21/25][46/110] Loss_D: 0.5553 Loss_G: 4.0713\n",
      "[21/25][47/110] Loss_D: 0.3843 Loss_G: 5.5280\n",
      "[21/25][48/110] Loss_D: 0.5131 Loss_G: 4.4028\n",
      "[21/25][49/110] Loss_D: 0.4018 Loss_G: 3.7940\n",
      "[21/25][50/110] Loss_D: 0.4430 Loss_G: 5.0785\n",
      "[21/25][51/110] Loss_D: 0.3045 Loss_G: 4.8189\n",
      "[21/25][52/110] Loss_D: 0.6186 Loss_G: 2.4692\n",
      "[21/25][53/110] Loss_D: 0.5727 Loss_G: 4.6033\n",
      "[21/25][54/110] Loss_D: 0.4875 Loss_G: 4.1660\n",
      "[21/25][55/110] Loss_D: 0.3186 Loss_G: 3.2796\n",
      "[21/25][56/110] Loss_D: 0.3014 Loss_G: 3.8967\n",
      "[21/25][57/110] Loss_D: 1.0110 Loss_G: 5.0101\n",
      "[21/25][58/110] Loss_D: 0.1372 Loss_G: 5.0898\n",
      "[21/25][59/110] Loss_D: 0.6107 Loss_G: 2.4312\n",
      "[21/25][60/110] Loss_D: 1.1530 Loss_G: 7.0800\n",
      "[21/25][61/110] Loss_D: 0.3642 Loss_G: 5.2068\n",
      "[21/25][62/110] Loss_D: 0.5799 Loss_G: 2.9640\n",
      "[21/25][63/110] Loss_D: 0.3711 Loss_G: 3.6313\n",
      "[21/25][64/110] Loss_D: 0.5954 Loss_G: 5.5595\n",
      "[21/25][65/110] Loss_D: 0.7035 Loss_G: 4.2705\n",
      "[21/25][66/110] Loss_D: 0.3932 Loss_G: 3.4823\n",
      "[21/25][67/110] Loss_D: 1.0945 Loss_G: 7.5099\n",
      "[21/25][68/110] Loss_D: 0.4281 Loss_G: 6.4835\n",
      "[21/25][69/110] Loss_D: 0.4333 Loss_G: 3.9777\n",
      "[21/25][70/110] Loss_D: 0.8524 Loss_G: 6.3760\n",
      "[21/25][71/110] Loss_D: 0.8175 Loss_G: 3.2923\n",
      "[21/25][72/110] Loss_D: 0.8329 Loss_G: 2.1458\n",
      "[21/25][73/110] Loss_D: 2.1718 Loss_G: 10.0767\n",
      "[21/25][74/110] Loss_D: 1.8399 Loss_G: 4.3986\n",
      "[21/25][75/110] Loss_D: 0.4087 Loss_G: 3.1063\n",
      "[21/25][76/110] Loss_D: 1.2191 Loss_G: 6.1793\n",
      "[21/25][77/110] Loss_D: 1.2590 Loss_G: 3.6526\n",
      "[21/25][78/110] Loss_D: 0.8042 Loss_G: 3.6036\n",
      "[21/25][79/110] Loss_D: 0.7149 Loss_G: 3.1869\n",
      "[21/25][80/110] Loss_D: 0.7200 Loss_G: 5.7671\n",
      "[21/25][81/110] Loss_D: 0.4032 Loss_G: 3.6221\n",
      "[21/25][82/110] Loss_D: 0.3771 Loss_G: 4.2790\n",
      "[21/25][83/110] Loss_D: 0.2937 Loss_G: 5.4129\n",
      "[21/25][84/110] Loss_D: 0.4323 Loss_G: 3.7235\n",
      "[21/25][85/110] Loss_D: 0.7762 Loss_G: 4.8465\n",
      "[21/25][86/110] Loss_D: 0.2817 Loss_G: 5.6845\n",
      "[21/25][87/110] Loss_D: 0.5985 Loss_G: 3.6052\n",
      "[21/25][88/110] Loss_D: 0.5345 Loss_G: 4.5223\n",
      "[21/25][89/110] Loss_D: 0.7077 Loss_G: 4.0671\n",
      "[21/25][90/110] Loss_D: 0.4258 Loss_G: 4.3231\n",
      "[21/25][91/110] Loss_D: 1.6076 Loss_G: 8.7786\n",
      "[21/25][92/110] Loss_D: 1.6430 Loss_G: 3.6590\n",
      "[21/25][93/110] Loss_D: 0.6232 Loss_G: 5.0563\n",
      "[21/25][94/110] Loss_D: 0.5318 Loss_G: 5.1951\n",
      "[21/25][95/110] Loss_D: 0.6503 Loss_G: 7.9685\n",
      "[21/25][96/110] Loss_D: 0.8333 Loss_G: 3.8056\n",
      "[21/25][97/110] Loss_D: 0.3647 Loss_G: 3.4012\n",
      "[21/25][98/110] Loss_D: 1.4829 Loss_G: 8.6363\n",
      "[21/25][99/110] Loss_D: 1.8864 Loss_G: 4.3183\n",
      "[21/25][100/110] Loss_D: 0.7930 Loss_G: 1.5323\n",
      "[21/25][101/110] Loss_D: 3.1842 Loss_G: 6.9076\n",
      "[21/25][102/110] Loss_D: 0.1706 Loss_G: 7.0753\n",
      "[21/25][103/110] Loss_D: 2.4319 Loss_G: 0.6450\n",
      "[21/25][104/110] Loss_D: 4.4334 Loss_G: 6.1341\n",
      "[21/25][105/110] Loss_D: 0.0781 Loss_G: 8.4008\n",
      "[21/25][106/110] Loss_D: 0.1512 Loss_G: 6.1488\n",
      "[21/25][107/110] Loss_D: 0.7152 Loss_G: 3.3145\n",
      "[21/25][108/110] Loss_D: 1.4565 Loss_G: 5.5827\n",
      "[21/25][109/110] Loss_D: 0.6862 Loss_G: 5.3667\n",
      "[22/25][0/110] Loss_D: 1.3216 Loss_G: 2.6517\n",
      "[22/25][1/110] Loss_D: 2.3357 Loss_G: 4.7955\n",
      "[22/25][2/110] Loss_D: 0.9050 Loss_G: 5.5774\n",
      "[22/25][3/110] Loss_D: 0.9963 Loss_G: 2.3430\n",
      "[22/25][4/110] Loss_D: 1.9111 Loss_G: 7.8465\n",
      "[22/25][5/110] Loss_D: 0.2588 Loss_G: 7.4617\n",
      "[22/25][6/110] Loss_D: 0.8799 Loss_G: 3.7106\n",
      "[22/25][7/110] Loss_D: 1.2235 Loss_G: 3.3068\n",
      "[22/25][8/110] Loss_D: 0.5901 Loss_G: 5.4509\n",
      "[22/25][9/110] Loss_D: 0.8658 Loss_G: 6.3785\n",
      "[22/25][10/110] Loss_D: 0.3191 Loss_G: 4.9861\n",
      "[22/25][11/110] Loss_D: 0.2047 Loss_G: 3.3670\n",
      "[22/25][12/110] Loss_D: 1.0367 Loss_G: 7.2327\n",
      "[22/25][13/110] Loss_D: 0.7316 Loss_G: 5.4067\n",
      "[22/25][14/110] Loss_D: 0.8794 Loss_G: 3.3123\n",
      "[22/25][15/110] Loss_D: 1.3395 Loss_G: 7.4063\n",
      "[22/25][16/110] Loss_D: 1.2662 Loss_G: 3.6487\n",
      "[22/25][17/110] Loss_D: 0.3014 Loss_G: 3.1122\n",
      "[22/25][18/110] Loss_D: 1.6378 Loss_G: 8.8599\n",
      "[22/25][19/110] Loss_D: 0.8658 Loss_G: 4.9766\n",
      "[22/25][20/110] Loss_D: 0.2793 Loss_G: 2.7030\n",
      "[22/25][21/110] Loss_D: 1.5799 Loss_G: 7.4545\n",
      "[22/25][22/110] Loss_D: 0.6316 Loss_G: 6.3937\n",
      "[22/25][23/110] Loss_D: 0.5928 Loss_G: 2.6027\n",
      "[22/25][24/110] Loss_D: 0.7426 Loss_G: 4.6070\n",
      "[22/25][25/110] Loss_D: 0.5628 Loss_G: 5.6964\n",
      "[22/25][26/110] Loss_D: 1.2151 Loss_G: 2.3593\n",
      "[22/25][27/110] Loss_D: 1.6932 Loss_G: 6.7921\n",
      "[22/25][28/110] Loss_D: 1.0346 Loss_G: 3.5785\n",
      "[22/25][29/110] Loss_D: 0.7127 Loss_G: 5.2947\n",
      "[22/25][30/110] Loss_D: 1.0393 Loss_G: 7.6683\n",
      "[22/25][31/110] Loss_D: 1.9548 Loss_G: 2.8601\n",
      "[22/25][32/110] Loss_D: 0.8745 Loss_G: 4.2671\n",
      "[22/25][33/110] Loss_D: 0.7940 Loss_G: 4.6207\n",
      "[22/25][34/110] Loss_D: 0.5456 Loss_G: 4.0992\n",
      "[22/25][35/110] Loss_D: 0.8672 Loss_G: 5.0426\n",
      "[22/25][36/110] Loss_D: 1.4419 Loss_G: 2.2908\n",
      "[22/25][37/110] Loss_D: 0.5901 Loss_G: 4.1316\n",
      "[22/25][38/110] Loss_D: 0.4824 Loss_G: 5.7987\n",
      "[22/25][39/110] Loss_D: 0.5792 Loss_G: 2.9592\n",
      "[22/25][40/110] Loss_D: 0.5127 Loss_G: 4.3518\n",
      "[22/25][41/110] Loss_D: 0.4186 Loss_G: 4.6819\n",
      "[22/25][42/110] Loss_D: 0.9142 Loss_G: 2.5400\n",
      "[22/25][43/110] Loss_D: 0.5809 Loss_G: 5.8684\n",
      "[22/25][44/110] Loss_D: 1.3384 Loss_G: 3.4615\n",
      "[22/25][45/110] Loss_D: 0.3762 Loss_G: 4.4537\n",
      "[22/25][46/110] Loss_D: 0.3869 Loss_G: 3.7716\n",
      "[22/25][47/110] Loss_D: 0.5454 Loss_G: 6.7635\n",
      "[22/25][48/110] Loss_D: 0.5163 Loss_G: 4.4692\n",
      "[22/25][49/110] Loss_D: 1.1554 Loss_G: 1.1065\n",
      "[22/25][50/110] Loss_D: 1.0920 Loss_G: 6.1304\n",
      "[22/25][51/110] Loss_D: 0.2209 Loss_G: 5.8657\n",
      "[22/25][52/110] Loss_D: 0.3622 Loss_G: 4.6910\n",
      "[22/25][53/110] Loss_D: 0.7503 Loss_G: 1.7810\n",
      "[22/25][54/110] Loss_D: 2.2730 Loss_G: 7.9432\n",
      "[22/25][55/110] Loss_D: 0.5682 Loss_G: 5.3344\n",
      "[22/25][56/110] Loss_D: 0.5839 Loss_G: 2.4319\n",
      "[22/25][57/110] Loss_D: 1.0525 Loss_G: 5.5797\n",
      "[22/25][58/110] Loss_D: 1.0615 Loss_G: 4.6660\n",
      "[22/25][59/110] Loss_D: 0.6369 Loss_G: 3.5664\n",
      "[22/25][60/110] Loss_D: 0.6712 Loss_G: 3.0013\n",
      "[22/25][61/110] Loss_D: 0.7553 Loss_G: 5.2420\n",
      "[22/25][62/110] Loss_D: 0.4103 Loss_G: 7.0605\n",
      "[22/25][63/110] Loss_D: 0.7794 Loss_G: 4.3098\n",
      "[22/25][64/110] Loss_D: 2.1135 Loss_G: 0.3519\n",
      "[22/25][65/110] Loss_D: 3.2539 Loss_G: 6.3434\n",
      "[22/25][66/110] Loss_D: 0.5629 Loss_G: 5.6909\n",
      "[22/25][67/110] Loss_D: 0.4030 Loss_G: 4.3094\n",
      "[22/25][68/110] Loss_D: 0.3714 Loss_G: 2.9476\n",
      "[22/25][69/110] Loss_D: 0.3392 Loss_G: 3.9702\n",
      "[22/25][70/110] Loss_D: 0.8197 Loss_G: 2.9171\n",
      "[22/25][71/110] Loss_D: 1.1695 Loss_G: 4.5709\n",
      "[22/25][72/110] Loss_D: 0.2529 Loss_G: 4.5930\n",
      "[22/25][73/110] Loss_D: 0.1806 Loss_G: 4.2767\n",
      "[22/25][74/110] Loss_D: 0.4656 Loss_G: 4.6520\n",
      "[22/25][75/110] Loss_D: 0.4034 Loss_G: 3.0995\n",
      "[22/25][76/110] Loss_D: 0.9315 Loss_G: 5.1109\n",
      "[22/25][77/110] Loss_D: 1.2248 Loss_G: 3.1446\n",
      "[22/25][78/110] Loss_D: 0.7887 Loss_G: 4.8934\n",
      "[22/25][79/110] Loss_D: 1.0534 Loss_G: 3.1464\n",
      "[22/25][80/110] Loss_D: 1.0674 Loss_G: 5.5149\n",
      "[22/25][81/110] Loss_D: 0.3574 Loss_G: 5.1444\n",
      "[22/25][82/110] Loss_D: 1.1567 Loss_G: 1.7919\n",
      "[22/25][83/110] Loss_D: 1.8162 Loss_G: 6.2517\n",
      "[22/25][84/110] Loss_D: 0.4780 Loss_G: 5.6014\n",
      "[22/25][85/110] Loss_D: 0.9855 Loss_G: 2.5296\n",
      "[22/25][86/110] Loss_D: 0.7544 Loss_G: 3.6255\n",
      "[22/25][87/110] Loss_D: 0.6373 Loss_G: 4.8548\n",
      "[22/25][88/110] Loss_D: 0.8945 Loss_G: 2.9439\n",
      "[22/25][89/110] Loss_D: 0.8008 Loss_G: 5.0493\n",
      "[22/25][90/110] Loss_D: 0.4516 Loss_G: 4.8791\n",
      "[22/25][91/110] Loss_D: 0.2061 Loss_G: 4.3990\n",
      "[22/25][92/110] Loss_D: 0.6318 Loss_G: 2.8504\n",
      "[22/25][93/110] Loss_D: 0.8426 Loss_G: 2.6264\n",
      "[22/25][94/110] Loss_D: 1.1929 Loss_G: 6.6946\n",
      "[22/25][95/110] Loss_D: 0.7363 Loss_G: 3.6368\n",
      "[22/25][96/110] Loss_D: 0.5880 Loss_G: 3.8144\n",
      "[22/25][97/110] Loss_D: 0.8217 Loss_G: 5.6042\n",
      "[22/25][98/110] Loss_D: 0.4570 Loss_G: 3.5624\n",
      "[22/25][99/110] Loss_D: 0.4204 Loss_G: 3.1140\n",
      "[22/25][100/110] Loss_D: 1.2761 Loss_G: 5.1078\n",
      "[22/25][101/110] Loss_D: 0.3859 Loss_G: 4.2822\n",
      "[22/25][102/110] Loss_D: 0.2617 Loss_G: 3.8647\n",
      "[22/25][103/110] Loss_D: 0.5182 Loss_G: 3.4110\n",
      "[22/25][104/110] Loss_D: 0.5653 Loss_G: 5.4550\n",
      "[22/25][105/110] Loss_D: 0.6489 Loss_G: 3.5064\n",
      "[22/25][106/110] Loss_D: 0.5721 Loss_G: 4.2339\n",
      "[22/25][107/110] Loss_D: 0.2279 Loss_G: 4.3954\n",
      "[22/25][108/110] Loss_D: 0.4998 Loss_G: 3.2775\n",
      "[22/25][109/110] Loss_D: 0.2013 Loss_G: 4.7099\n",
      "[23/25][0/110] Loss_D: 0.4626 Loss_G: 5.7051\n",
      "[23/25][1/110] Loss_D: 0.7085 Loss_G: 3.2328\n",
      "[23/25][2/110] Loss_D: 0.3683 Loss_G: 3.7622\n",
      "[23/25][3/110] Loss_D: 1.0904 Loss_G: 6.8046\n",
      "[23/25][4/110] Loss_D: 0.8212 Loss_G: 4.7254\n",
      "[23/25][5/110] Loss_D: 0.3706 Loss_G: 2.8926\n",
      "[23/25][6/110] Loss_D: 0.6147 Loss_G: 4.6984\n",
      "[23/25][7/110] Loss_D: 0.3904 Loss_G: 6.4706\n",
      "[23/25][8/110] Loss_D: 0.3003 Loss_G: 5.5765\n",
      "[23/25][9/110] Loss_D: 0.3890 Loss_G: 4.1311\n",
      "[23/25][10/110] Loss_D: 0.6816 Loss_G: 4.6129\n",
      "[23/25][11/110] Loss_D: 1.0725 Loss_G: 4.7470\n",
      "[23/25][12/110] Loss_D: 0.6798 Loss_G: 5.5181\n",
      "[23/25][13/110] Loss_D: 0.4147 Loss_G: 3.4980\n",
      "[23/25][14/110] Loss_D: 2.0294 Loss_G: 7.8583\n",
      "[23/25][15/110] Loss_D: 0.2004 Loss_G: 6.8747\n",
      "[23/25][16/110] Loss_D: 0.2258 Loss_G: 4.9122\n",
      "[23/25][17/110] Loss_D: 0.6410 Loss_G: 2.7494\n",
      "[23/25][18/110] Loss_D: 1.4483 Loss_G: 7.9156\n",
      "[23/25][19/110] Loss_D: 1.6198 Loss_G: 3.9990\n",
      "[23/25][20/110] Loss_D: 0.6338 Loss_G: 4.9177\n",
      "[23/25][21/110] Loss_D: 0.4609 Loss_G: 5.0887\n",
      "[23/25][22/110] Loss_D: 0.6803 Loss_G: 2.5923\n",
      "[23/25][23/110] Loss_D: 0.7126 Loss_G: 4.7492\n",
      "[23/25][24/110] Loss_D: 0.9338 Loss_G: 6.4635\n",
      "[23/25][25/110] Loss_D: 0.1944 Loss_G: 4.8557\n",
      "[23/25][26/110] Loss_D: 0.2516 Loss_G: 3.3338\n",
      "[23/25][27/110] Loss_D: 1.3015 Loss_G: 4.1393\n",
      "[23/25][28/110] Loss_D: 0.3996 Loss_G: 5.8487\n",
      "[23/25][29/110] Loss_D: 0.1214 Loss_G: 5.3788\n",
      "[23/25][30/110] Loss_D: 0.5741 Loss_G: 2.4091\n",
      "[23/25][31/110] Loss_D: 1.7427 Loss_G: 8.9209\n",
      "[23/25][32/110] Loss_D: 0.4972 Loss_G: 7.6131\n",
      "[23/25][33/110] Loss_D: 1.0461 Loss_G: 1.7062\n",
      "[23/25][34/110] Loss_D: 0.4461 Loss_G: 3.7085\n",
      "[23/25][35/110] Loss_D: 1.3541 Loss_G: 4.3628\n",
      "[23/25][36/110] Loss_D: 0.6869 Loss_G: 5.6516\n",
      "[23/25][37/110] Loss_D: 0.2033 Loss_G: 5.0245\n",
      "[23/25][38/110] Loss_D: 0.1960 Loss_G: 3.7600\n",
      "[23/25][39/110] Loss_D: 0.7045 Loss_G: 5.0758\n",
      "[23/25][40/110] Loss_D: 0.3169 Loss_G: 4.5240\n",
      "[23/25][41/110] Loss_D: 1.1652 Loss_G: 2.4934\n",
      "[23/25][42/110] Loss_D: 1.6235 Loss_G: 7.9031\n",
      "[23/25][43/110] Loss_D: 1.0686 Loss_G: 2.5454\n",
      "[23/25][44/110] Loss_D: 1.1973 Loss_G: 7.8693\n",
      "[23/25][45/110] Loss_D: 0.5344 Loss_G: 6.4251\n",
      "[23/25][46/110] Loss_D: 0.5514 Loss_G: 2.5744\n",
      "[23/25][47/110] Loss_D: 1.0157 Loss_G: 8.6752\n",
      "[23/25][48/110] Loss_D: 0.9738 Loss_G: 3.6192\n",
      "[23/25][49/110] Loss_D: 0.4223 Loss_G: 3.6778\n",
      "[23/25][50/110] Loss_D: 0.6259 Loss_G: 6.5072\n",
      "[23/25][51/110] Loss_D: 0.3283 Loss_G: 5.4017\n",
      "[23/25][52/110] Loss_D: 0.3910 Loss_G: 3.1813\n",
      "[23/25][53/110] Loss_D: 1.1732 Loss_G: 6.8770\n",
      "[23/25][54/110] Loss_D: 0.2798 Loss_G: 6.3100\n",
      "[23/25][55/110] Loss_D: 0.5505 Loss_G: 3.6924\n",
      "[23/25][56/110] Loss_D: 0.6944 Loss_G: 3.7707\n",
      "[23/25][57/110] Loss_D: 0.3822 Loss_G: 5.7201\n",
      "[23/25][58/110] Loss_D: 0.4808 Loss_G: 4.5007\n",
      "[23/25][59/110] Loss_D: 0.4218 Loss_G: 4.7116\n",
      "[23/25][60/110] Loss_D: 0.9518 Loss_G: 3.6519\n",
      "[23/25][61/110] Loss_D: 1.5066 Loss_G: 2.5946\n",
      "[23/25][62/110] Loss_D: 1.2062 Loss_G: 6.3920\n",
      "[23/25][63/110] Loss_D: 0.2988 Loss_G: 5.1480\n",
      "[23/25][64/110] Loss_D: 0.4865 Loss_G: 5.0685\n",
      "[23/25][65/110] Loss_D: 0.9106 Loss_G: 2.5288\n",
      "[23/25][66/110] Loss_D: 1.3380 Loss_G: 7.7463\n",
      "[23/25][67/110] Loss_D: 1.0445 Loss_G: 4.8768\n",
      "[23/25][68/110] Loss_D: 1.4765 Loss_G: 2.9155\n",
      "[23/25][69/110] Loss_D: 1.0609 Loss_G: 5.7935\n",
      "[23/25][70/110] Loss_D: 0.3092 Loss_G: 4.7630\n",
      "[23/25][71/110] Loss_D: 0.5140 Loss_G: 3.5360\n",
      "[23/25][72/110] Loss_D: 0.6291 Loss_G: 6.6515\n",
      "[23/25][73/110] Loss_D: 0.6677 Loss_G: 4.4484\n",
      "[23/25][74/110] Loss_D: 1.1285 Loss_G: 3.9466\n",
      "[23/25][75/110] Loss_D: 0.3387 Loss_G: 4.9153\n",
      "[23/25][76/110] Loss_D: 0.7460 Loss_G: 3.0312\n",
      "[23/25][77/110] Loss_D: 1.4054 Loss_G: 10.4067\n",
      "[23/25][78/110] Loss_D: 1.0534 Loss_G: 8.6710\n",
      "[23/25][79/110] Loss_D: 0.6830 Loss_G: 3.0284\n",
      "[23/25][80/110] Loss_D: 1.2131 Loss_G: 6.5646\n",
      "[23/25][81/110] Loss_D: 0.0836 Loss_G: 6.7999\n",
      "[23/25][82/110] Loss_D: 0.3250 Loss_G: 4.9712\n",
      "[23/25][83/110] Loss_D: 0.9801 Loss_G: 4.0997\n",
      "[23/25][84/110] Loss_D: 0.8478 Loss_G: 3.6168\n",
      "[23/25][85/110] Loss_D: 0.3943 Loss_G: 5.5237\n",
      "[23/25][86/110] Loss_D: 0.6272 Loss_G: 3.4204\n",
      "[23/25][87/110] Loss_D: 1.1290 Loss_G: 2.6691\n",
      "[23/25][88/110] Loss_D: 0.7018 Loss_G: 7.0185\n",
      "[23/25][89/110] Loss_D: 0.8669 Loss_G: 4.5521\n",
      "[23/25][90/110] Loss_D: 0.6338 Loss_G: 5.2635\n",
      "[23/25][91/110] Loss_D: 0.5571 Loss_G: 4.7894\n",
      "[23/25][92/110] Loss_D: 0.6867 Loss_G: 2.6596\n",
      "[23/25][93/110] Loss_D: 0.6903 Loss_G: 3.2193\n",
      "[23/25][94/110] Loss_D: 1.4677 Loss_G: 9.0152\n",
      "[23/25][95/110] Loss_D: 0.6598 Loss_G: 6.4056\n",
      "[23/25][96/110] Loss_D: 1.4318 Loss_G: 1.8667\n",
      "[23/25][97/110] Loss_D: 0.6225 Loss_G: 5.8304\n",
      "[23/25][98/110] Loss_D: 0.4051 Loss_G: 6.5046\n",
      "[23/25][99/110] Loss_D: 0.3674 Loss_G: 4.8528\n",
      "[23/25][100/110] Loss_D: 0.5650 Loss_G: 5.6835\n",
      "[23/25][101/110] Loss_D: 0.5784 Loss_G: 3.1098\n",
      "[23/25][102/110] Loss_D: 1.6218 Loss_G: 8.0295\n",
      "[23/25][103/110] Loss_D: 2.7534 Loss_G: 1.7453\n",
      "[23/25][104/110] Loss_D: 1.4431 Loss_G: 6.4622\n",
      "[23/25][105/110] Loss_D: 0.4409 Loss_G: 6.9916\n",
      "[23/25][106/110] Loss_D: 0.9624 Loss_G: 3.0040\n",
      "[23/25][107/110] Loss_D: 2.0402 Loss_G: 7.8868\n",
      "[23/25][108/110] Loss_D: 0.9016 Loss_G: 3.7254\n",
      "[23/25][109/110] Loss_D: 0.2777 Loss_G: 3.0534\n",
      "[24/25][0/110] Loss_D: 3.1039 Loss_G: 8.8278\n",
      "[24/25][1/110] Loss_D: 1.8624 Loss_G: 4.3425\n",
      "[24/25][2/110] Loss_D: 0.2534 Loss_G: 2.1578\n",
      "[24/25][3/110] Loss_D: 0.8100 Loss_G: 4.1218\n",
      "[24/25][4/110] Loss_D: 0.3972 Loss_G: 4.5026\n",
      "[24/25][5/110] Loss_D: 0.6539 Loss_G: 3.6847\n",
      "[24/25][6/110] Loss_D: 0.5398 Loss_G: 4.4668\n",
      "[24/25][7/110] Loss_D: 0.4649 Loss_G: 4.6133\n",
      "[24/25][8/110] Loss_D: 0.6044 Loss_G: 2.8803\n",
      "[24/25][9/110] Loss_D: 1.6047 Loss_G: 7.1934\n",
      "[24/25][10/110] Loss_D: 1.1623 Loss_G: 4.1071\n",
      "[24/25][11/110] Loss_D: 0.5847 Loss_G: 1.7758\n",
      "[24/25][12/110] Loss_D: 2.4682 Loss_G: 6.4291\n",
      "[24/25][13/110] Loss_D: 0.2944 Loss_G: 7.4282\n",
      "[24/25][14/110] Loss_D: 0.9380 Loss_G: 3.1945\n",
      "[24/25][15/110] Loss_D: 0.5862 Loss_G: 3.2305\n",
      "[24/25][16/110] Loss_D: 0.3950 Loss_G: 5.2471\n",
      "[24/25][17/110] Loss_D: 0.6686 Loss_G: 3.0404\n",
      "[24/25][18/110] Loss_D: 1.1165 Loss_G: 6.1890\n",
      "[24/25][19/110] Loss_D: 0.4887 Loss_G: 4.4158\n",
      "[24/25][20/110] Loss_D: 0.8989 Loss_G: 2.2581\n",
      "[24/25][21/110] Loss_D: 0.9362 Loss_G: 5.0101\n",
      "[24/25][22/110] Loss_D: 0.6594 Loss_G: 5.5993\n",
      "[24/25][23/110] Loss_D: 0.6341 Loss_G: 4.7538\n",
      "[24/25][24/110] Loss_D: 0.5264 Loss_G: 3.3110\n",
      "[24/25][25/110] Loss_D: 0.5737 Loss_G: 4.4131\n",
      "[24/25][26/110] Loss_D: 0.5223 Loss_G: 5.6893\n",
      "[24/25][27/110] Loss_D: 0.7881 Loss_G: 3.1024\n",
      "[24/25][28/110] Loss_D: 1.1122 Loss_G: 5.0214\n",
      "[24/25][29/110] Loss_D: 0.7644 Loss_G: 3.5812\n",
      "[24/25][30/110] Loss_D: 0.7826 Loss_G: 3.8098\n",
      "[24/25][31/110] Loss_D: 0.3866 Loss_G: 4.6408\n",
      "[24/25][32/110] Loss_D: 0.4069 Loss_G: 4.2418\n",
      "[24/25][33/110] Loss_D: 0.5773 Loss_G: 4.8661\n",
      "[24/25][34/110] Loss_D: 0.8948 Loss_G: 2.7388\n",
      "[24/25][35/110] Loss_D: 0.7880 Loss_G: 6.0369\n",
      "[24/25][36/110] Loss_D: 1.0889 Loss_G: 3.6669\n",
      "[24/25][37/110] Loss_D: 0.6360 Loss_G: 2.7048\n",
      "[24/25][38/110] Loss_D: 0.6165 Loss_G: 4.4904\n",
      "[24/25][39/110] Loss_D: 0.3165 Loss_G: 4.7943\n",
      "[24/25][40/110] Loss_D: 0.2372 Loss_G: 4.3765\n",
      "[24/25][41/110] Loss_D: 0.2072 Loss_G: 4.2312\n",
      "[24/25][42/110] Loss_D: 0.8192 Loss_G: 2.0602\n",
      "[24/25][43/110] Loss_D: 1.3327 Loss_G: 8.8491\n",
      "[24/25][44/110] Loss_D: 1.4414 Loss_G: 6.2554\n",
      "[24/25][45/110] Loss_D: 0.2723 Loss_G: 3.4092\n",
      "[24/25][46/110] Loss_D: 0.4999 Loss_G: 3.8190\n",
      "[24/25][47/110] Loss_D: 0.3079 Loss_G: 5.2954\n",
      "[24/25][48/110] Loss_D: 0.1367 Loss_G: 4.8757\n",
      "[24/25][49/110] Loss_D: 0.3939 Loss_G: 5.3523\n",
      "[24/25][50/110] Loss_D: 0.9916 Loss_G: 5.4146\n",
      "[24/25][51/110] Loss_D: 1.1960 Loss_G: 2.4093\n",
      "[24/25][52/110] Loss_D: 1.4431 Loss_G: 10.0116\n",
      "[24/25][53/110] Loss_D: 1.9071 Loss_G: 5.9203\n",
      "[24/25][54/110] Loss_D: 0.2560 Loss_G: 2.8554\n",
      "[24/25][55/110] Loss_D: 0.9834 Loss_G: 6.3772\n",
      "[24/25][56/110] Loss_D: 0.2661 Loss_G: 5.9088\n",
      "[24/25][57/110] Loss_D: 0.1916 Loss_G: 3.8404\n",
      "[24/25][58/110] Loss_D: 0.4346 Loss_G: 4.4311\n",
      "[24/25][59/110] Loss_D: 0.4766 Loss_G: 4.4886\n",
      "[24/25][60/110] Loss_D: 0.9303 Loss_G: 5.3747\n",
      "[24/25][61/110] Loss_D: 0.6344 Loss_G: 4.1191\n",
      "[24/25][62/110] Loss_D: 0.8795 Loss_G: 2.8086\n",
      "[24/25][63/110] Loss_D: 0.7420 Loss_G: 4.5103\n",
      "[24/25][64/110] Loss_D: 1.2915 Loss_G: 2.6069\n",
      "[24/25][65/110] Loss_D: 1.4284 Loss_G: 5.6165\n",
      "[24/25][66/110] Loss_D: 0.5408 Loss_G: 5.0584\n",
      "[24/25][67/110] Loss_D: 0.6617 Loss_G: 3.8657\n",
      "[24/25][68/110] Loss_D: 0.6449 Loss_G: 4.6865\n",
      "[24/25][69/110] Loss_D: 0.5243 Loss_G: 5.3591\n",
      "[24/25][70/110] Loss_D: 0.9057 Loss_G: 3.1900\n",
      "[24/25][71/110] Loss_D: 1.0788 Loss_G: 5.6049\n",
      "[24/25][72/110] Loss_D: 0.5903 Loss_G: 4.4318\n",
      "[24/25][73/110] Loss_D: 1.3120 Loss_G: 1.8029\n",
      "[24/25][74/110] Loss_D: 0.5596 Loss_G: 2.6475\n",
      "[24/25][75/110] Loss_D: 0.6755 Loss_G: 5.0623\n",
      "[24/25][76/110] Loss_D: 0.2009 Loss_G: 5.8073\n",
      "[24/25][77/110] Loss_D: 0.3304 Loss_G: 4.2299\n",
      "[24/25][78/110] Loss_D: 0.4752 Loss_G: 4.9440\n",
      "[24/25][79/110] Loss_D: 1.5206 Loss_G: 1.6313\n",
      "[24/25][80/110] Loss_D: 1.8367 Loss_G: 5.2149\n",
      "[24/25][81/110] Loss_D: 0.6644 Loss_G: 4.7008\n",
      "[24/25][82/110] Loss_D: 0.8062 Loss_G: 3.2002\n",
      "[24/25][83/110] Loss_D: 0.7894 Loss_G: 7.0557\n",
      "[24/25][84/110] Loss_D: 0.2593 Loss_G: 6.2189\n",
      "[24/25][85/110] Loss_D: 0.9920 Loss_G: 2.0924\n",
      "[24/25][86/110] Loss_D: 1.8996 Loss_G: 8.4992\n",
      "[24/25][87/110] Loss_D: 0.2821 Loss_G: 8.2708\n",
      "[24/25][88/110] Loss_D: 1.9237 Loss_G: 1.7866\n",
      "[24/25][89/110] Loss_D: 1.6644 Loss_G: 6.0153\n",
      "[24/25][90/110] Loss_D: 0.3825 Loss_G: 5.9489\n",
      "[24/25][91/110] Loss_D: 0.1534 Loss_G: 5.7991\n",
      "[24/25][92/110] Loss_D: 0.8183 Loss_G: 4.1937\n",
      "[24/25][93/110] Loss_D: 0.8423 Loss_G: 4.2537\n",
      "[24/25][94/110] Loss_D: 0.6484 Loss_G: 6.6295\n",
      "[24/25][95/110] Loss_D: 0.6792 Loss_G: 4.2066\n",
      "[24/25][96/110] Loss_D: 0.3211 Loss_G: 2.5095\n",
      "[24/25][97/110] Loss_D: 0.8155 Loss_G: 6.7855\n",
      "[24/25][98/110] Loss_D: 0.4879 Loss_G: 4.3163\n",
      "[24/25][99/110] Loss_D: 0.8106 Loss_G: 1.4543\n",
      "[24/25][100/110] Loss_D: 1.0968 Loss_G: 6.2745\n",
      "[24/25][101/110] Loss_D: 0.5548 Loss_G: 4.5777\n",
      "[24/25][102/110] Loss_D: 0.8502 Loss_G: 3.6503\n",
      "[24/25][103/110] Loss_D: 1.2346 Loss_G: 4.8749\n",
      "[24/25][104/110] Loss_D: 0.9145 Loss_G: 3.7077\n",
      "[24/25][105/110] Loss_D: 0.6611 Loss_G: 5.1557\n",
      "[24/25][106/110] Loss_D: 0.1853 Loss_G: 6.2320\n",
      "[24/25][107/110] Loss_D: 0.1666 Loss_G: 5.0880\n",
      "[24/25][108/110] Loss_D: 0.2624 Loss_G: 4.7408\n",
      "[24/25][109/110] Loss_D: 0.2673 Loss_G: 4.7974\n"
     ]
    }
   ],
   "source": [
    "#defining the generator\n",
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G,self).__init__()\n",
    "        self.main=nn.Sequential(\n",
    "            nn.ConvTranspose2d(100,512,4,1,0,bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512,256,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256,128,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128,64,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64,3,4,2,1,bias=False),\n",
    "            nn.Tanh()\n",
    "            )\n",
    "    def forward(self,input):\n",
    "        output=self.main(input)\n",
    "        return output\n",
    "#creating the generator\n",
    "Generator=G()\n",
    "Generator.apply(weights_init)\n",
    "#defining the discriminator\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D,self).__init__()\n",
    "        self.main=nn.Sequential(\n",
    "            nn.Conv2d(3,64,4,2,1,bias=False),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64,128,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(128,256,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(256,512,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(512,1,4,1,0,bias=False),\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.main(input)\n",
    "        return output.view(-1) #flattening the ouput\n",
    "#creating the discriminator\n",
    "Discriminator=D()\n",
    "Discriminator.apply(weights_init)\n",
    "\n",
    "\n",
    "#Training the DCGANs\n",
    "criterion=nn.BCELoss()\n",
    "#criterion=torch.nn.BCEWithLogitsLoss\n",
    "#criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "optmizerD=optim.Adam(Discriminator.parameters(),lr=0.0005,betas=(0.5,0.999))\n",
    "optimizerG=optim.Adam(Generator.parameters(),lr=0.0005,betas=(0.5,0.999))\n",
    "\n",
    "for epoch in range(25):\n",
    "    for i, data in enumerate(dataloader,0):\n",
    "        #1st step: updating the weights of the discriminator\n",
    "        Discriminator.zero_grad() #initialize gradients to zero\n",
    "        #training the discriminator with real images\n",
    "        real,_=data\n",
    "        input=Variable(real) #converting the real image into a torch variable\n",
    "        target=Variable(torch.ones(input.size()[0])) #creating a tensor of ones with the size of the minibatch\n",
    "        output=Discriminator(input) #feeding the real image into the discriminator network\n",
    "        error_real=criterion(output,target)\n",
    "\n",
    "        #training the discriminator with fake images\n",
    "        noise=Variable(torch.randn(input.size()[0],100,1,1)) #making the noise tensor\n",
    "        fake=Generator(noise) # output of the generator taking the noise to generate \n",
    "        target=Variable(torch.zeros(input.size()[0])) \n",
    "        output=Discriminator(fake.detach()) #feeding the fake image into the discriminator, but detaching the gradeints\n",
    "        error_false=criterion(output,target)\n",
    "\n",
    "        #backpropagation of the total error\n",
    "        total_error=error_real+error_false #total error\n",
    "        total_error.backward() #backpropagating the error\n",
    "        optmizerD.step() #for weight updation\n",
    "\n",
    "        #updating weights of the generator\n",
    "        Generator.zero_grad() #initialize gradients of generator to zero\n",
    "        target=Variable(torch.ones(input.size()[0])) #creating a tensor of ones with the size of the minibatch\n",
    "        output=Discriminator(fake) #feeding the fake into discriminator\n",
    "        error_G=criterion(output,target)\n",
    "        error_G.backward() #backpropagating the gradients in the generator\n",
    "        optimizerG.step() #updating the weights of the generator\n",
    "\n",
    "        #printing losses and saving real and generated images\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch,25,i,len(dataloader),total_error.data,error_G.data))\n",
    "        #save images every 100 steps or iternations\n",
    "        if i%100==0:\n",
    "            vutils.save_image(real,'%s/real_samples.png' %\"C:/Users/satya/Downloads/Azure/Azure_cp/samples/results3\")\n",
    "            fake=Generator(noise)\n",
    "            vutils.save_image(fake.data,'%s/fake_samples_epoch_%03d.png'% (\"C:/Users/satya/Downloads/Azure/Azure_cp/samples/results3\",epoch))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (13): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001EDE35ED228>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Discriminator.parameters()\n",
    "Generator.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-81a6622bbc66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptmizerD\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0002\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moptimizerG\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0002\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\satya\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[0;32m     72\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[0;32m     73\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\satya\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[0/25][0/782] Loss_D: 1.9885 Loss_G: 6.5058\n",
      "[0/25][1/782] Loss_D: 1.2612 Loss_G: 4.3858\n",
      "[0/25][2/782] Loss_D: 1.3176 Loss_G: 5.8346\n",
      "[0/25][3/782] Loss_D: 1.0160 Loss_G: 6.3122\n",
      "[0/25][4/782] Loss_D: 0.8345 Loss_G: 6.1848\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4b0c0d0b5470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0merrG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We compute the loss between the prediction (output between 0 and 1) and the target (equal to 1).\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0merrG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[0moptimizerG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We apply the optimizer to update the weights according to how much they are responsible for the loss error of the generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\satya\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\satya\\anaconda3\\envs\\tf-gpu-cuda8\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('tf-gpu-cuda8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03f4ac3119e2450266d45ef487def59165e9739e84fca1acfa1dc135d1f5e0c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
